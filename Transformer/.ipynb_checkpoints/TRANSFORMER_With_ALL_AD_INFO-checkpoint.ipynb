{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter\n",
    "###################\n",
    "SAVE_PATH = '/home/huangzc/competition/tencent/model_ckpt/TRANSFORMER/model_all.ckpt'\n",
    "EMB_SIZE = 50\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "TARGET = 'age'\n",
    "COLS_NAME = ['creative_id', 'advertiser_id', 'ad_id', 'product_id', 'product_category', 'industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data\n",
    "### TARGET DF\n",
    "tr_user_df = pd.read_pickle(TRAIN_DIR+USER_PATH)\n",
    "tr_user_df = tr_user_df.groupby(['user_id']).agg({'age': 'first', 'gender': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ad_df = pd.read_pickle(TRAIN_DIR+AD_PATH)\n",
    "ts_ad_df = pd.read_pickle(TEST_DIR+AD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:02<00:00, 30.42s/it]\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "for col in tqdm(COLS_NAME):\n",
    "    train_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    test_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    \n",
    "train_df.append(tr_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_df(creative_df, advertiser_df, ad_df, product_id_df, product_cat_df, industry_df, user_df=None):\n",
    "    if user_df is  None:\n",
    "        user_df = creative_df[['user_id']]\n",
    "        user_df[TARGET] = np.nan\n",
    "    assert user_df['user_id'].values.tolist() == creative_df['user_id'].values.tolist() \\\n",
    "    == ad_df['user_id'].values.tolist() == product_id_df['user_id'].values.tolist() \\\n",
    "    == product_cat_df['user_id'].values.tolist() == industry_df['user_id'].values.tolist() \\\n",
    "    == advertiser_df['user_id'].values.tolist()\n",
    "\n",
    "    del advertiser_df['user_id'], ad_df['user_id'], product_id_df['user_id'], product_cat_df['user_id'], industry_df['user_id']\n",
    "    \n",
    "    grid_df = pd.concat([creative_df, advertiser_df, \n",
    "                         ad_df, product_id_df, \n",
    "                         product_cat_df, industry_df,\n",
    "                         user_df[[TARGET]]], axis=1)\n",
    "\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = get_grid_df(*train_df)\n",
    "grid_df_test = get_grid_df(*test_df)\n",
    "grid_df[TARGET] = grid_df[TARGET] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(grid_df, test_size=0.2, random_state=2020)\n",
    "test = grid_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose sentences max len: 29\n",
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "sentence_size = max(int(grid_df[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.6)), \n",
    "                    int(grid_df_test[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.6)))\n",
    "print('choose sentences max len: %d' % (sentence_size))\n",
    "print(\"Pad sequences (samples x time)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_feature():  \n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    x_test = []\n",
    "     \n",
    "    for col in tqdm(COLS_NAME):\n",
    "        x_train.append(sequence.pad_sequences(train[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_val.append(sequence.pad_sequences(val[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_test.append(sequence.pad_sequences(test[col], \n",
    "                                            maxlen=sentence_size, \n",
    "                                            padding='post',\n",
    "                                            truncating='post',\n",
    "                                            dtype='int64',\n",
    "                                            value=0\n",
    "                                           ))\n",
    "    print('feature count: train->%d, valid->%d, test->%d' %(len(x_train), len(x_val), len(x_test)))\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:34<00:00, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature count: train->6, valid->6, test->6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, x_test = pad_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get tf dataset\n",
    "def get_train_ds(x, y): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.repeat(EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "def get_test_ds(x, ): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_train_ds(tuple(x_train), train[TARGET].values)\n",
    "valid_ds = get_train_ds(tuple(x_val), val[TARGET].values)\n",
    "test_ds = get_test_ds(tuple(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_id\n",
      "advertiser_id\n",
      "ad_id\n",
      "product_id\n",
      "product_category\n",
      "industry\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = []\n",
    "temp = pd.concat([tr_ad_df, ts_ad_df], axis=0)\n",
    "for col in COLS_NAME:\n",
    "    print(col)\n",
    "    vocab_sizes.append(max(temp[col].unique().tolist()) + 1) ### padding 0 need add 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.load('/home/huangzc/competition/tencent/data/train_preliminary/gensim_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, ):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct Model\n",
    "#################################\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, EMB_SIZE, weights=[weights])\n",
    "        self.embeddings = []\n",
    "        for s in vocab_sizes:\n",
    "            self.embeddings.append(TokenAndPositionEmbedding(sentence_size, s, EMB_SIZE))\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        \n",
    "        ### Transformer\n",
    "        self.transformer_block = TransformerBlock(EMB_SIZE*len(vocab_sizes), 5, 64)\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embs = []\n",
    "        for emb, inp in zip(self.embeddings, inputs):\n",
    "            x = emb(inp)            \n",
    "            embs.append(x)\n",
    "        x = self.concat(embs)\n",
    "        \n",
    "        x = self.transformer_block(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restore the weights\n",
    "# model.load_weights(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss & Metric\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(features, labels):\n",
    "    predictions = model(features)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(v_loss)\n",
    "    valid_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "batch: 50, batch train loss: 1.93, train acc: 24.73%, consuming tine: 9.14\n",
      "batch: 50, batch valid loss: 1.72, valid acc: 30.48%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 100, batch train loss: 1.79, train acc: 28.73%, consuming tine: 8.31\n",
      "batch: 100, batch valid loss: 1.66, valid acc: 32.62%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 150, batch train loss: 1.72, train acc: 31.06%, consuming tine: 8.23\n",
      "batch: 150, batch valid loss: 1.63, valid acc: 33.98%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 200, batch train loss: 1.67, train acc: 32.72%, consuming tine: 8.14\n",
      "batch: 200, batch valid loss: 1.60, valid acc: 35.04%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 250, batch train loss: 1.64, train acc: 33.87%, consuming tine: 7.69\n",
      "batch: 250, batch valid loss: 1.58, valid acc: 35.76%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 300, batch train loss: 1.61, train acc: 34.71%, consuming tine: 8.17\n",
      "batch: 300, batch valid loss: 1.56, valid acc: 36.46%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 350, batch train loss: 1.59, train acc: 35.48%, consuming tine: 7.51\n",
      "batch: 350, batch valid loss: 1.54, valid acc: 37.07%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 400, batch train loss: 1.58, train acc: 36.14%, consuming tine: 7.84\n",
      "batch: 400, batch valid loss: 1.53, valid acc: 37.56%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 450, batch train loss: 1.56, train acc: 36.69%, consuming tine: 7.38\n",
      "batch: 450, batch valid loss: 1.52, valid acc: 37.96%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 500, batch train loss: 1.55, train acc: 37.20%, consuming tine: 8.16\n",
      "batch: 500, batch valid loss: 1.51, valid acc: 38.35%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 550, batch train loss: 1.54, train acc: 37.64%, consuming tine: 7.20\n",
      "batch: 550, batch valid loss: 1.50, valid acc: 38.72%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 600, batch train loss: 1.53, train acc: 38.04%, consuming tine: 7.51\n",
      "batch: 600, batch valid loss: 1.49, valid acc: 39.02%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 650, batch train loss: 1.52, train acc: 38.36%, consuming tine: 7.74\n",
      "batch: 650, batch valid loss: 1.49, valid acc: 39.30%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 700, batch train loss: 1.51, train acc: 38.72%, consuming tine: 8.62\n",
      "batch: 700, batch valid loss: 1.48, valid acc: 39.54%\n",
      "##################################################\n",
      "Epoch 1, Loss: 1.51, Accuracy: 38.74%, Valid Loss: 1.48, Valid Accuracy: 39.54%\n",
      "##################################################\n",
      "batch: 50, batch train loss: 1.27, train acc: 49.43%, consuming tine: 7.75\n",
      "batch: 50, batch valid loss: 1.47, valid acc: 41.69%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 100, batch train loss: 1.17, train acc: 53.61%, consuming tine: 7.94\n",
      "batch: 100, batch valid loss: 1.53, valid acc: 40.80%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 150, batch train loss: 1.10, train acc: 56.78%, consuming tine: 8.86\n",
      "batch: 150, batch valid loss: 1.60, valid acc: 40.03%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 200, batch train loss: 1.05, train acc: 59.01%, consuming tine: 8.57\n",
      "batch: 200, batch valid loss: 1.63, valid acc: 39.62%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 250, batch train loss: 1.01, train acc: 60.73%, consuming tine: 8.17\n",
      "batch: 250, batch valid loss: 1.67, valid acc: 39.42%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 300, batch train loss: 0.98, train acc: 61.81%, consuming tine: 8.03\n",
      "batch: 300, batch valid loss: 1.70, valid acc: 39.18%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 350, batch train loss: 0.96, train acc: 62.92%, consuming tine: 8.06\n",
      "batch: 350, batch valid loss: 1.71, valid acc: 39.06%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 400, batch train loss: 0.93, train acc: 63.96%, consuming tine: 8.23\n",
      "batch: 400, batch valid loss: 1.74, valid acc: 38.93%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 450, batch train loss: 0.91, train acc: 64.91%, consuming tine: 7.76\n",
      "batch: 450, batch valid loss: 1.76, valid acc: 38.79%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 500, batch train loss: 0.89, train acc: 65.65%, consuming tine: 7.77\n",
      "batch: 500, batch valid loss: 1.77, valid acc: 38.65%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 550, batch train loss: 0.88, train acc: 66.23%, consuming tine: 7.09\n",
      "batch: 550, batch valid loss: 1.78, valid acc: 38.59%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 600, batch train loss: 0.87, train acc: 66.81%, consuming tine: 7.95\n",
      "batch: 600, batch valid loss: 1.79, valid acc: 38.53%\n",
      "##################################################\n",
      "##################################################\n",
      "batch: 650, batch train loss: 0.86, train acc: 67.26%, consuming tine: 8.00\n",
      "batch: 650, batch valid loss: 1.80, valid acc: 38.53%\n",
      "##################################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8d62ce229923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mCNT\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "\n",
    "    CNT = 0\n",
    "    for features, labels in train_ds:\n",
    "        begin = time.time()\n",
    "        train_step(features, labels)\n",
    "        CNT += 1\n",
    "\n",
    "\n",
    "        if CNT % 50 == 0: \n",
    "            for val_features, val_labels in valid_ds:\n",
    "                valid_step(val_features, val_labels)\n",
    "            print('##################################################')\n",
    "            print('batch: {:d}, batch train loss: {:.2f}, train acc: {:.2f}%, consuming tine: {:.2f}'.\n",
    "                  format(CNT, train_loss.result(), train_accuracy.result()*100, time.time()-begin))            \n",
    "            print('batch: {:d}, batch valid loss: {:.2f}, valid acc: {:.2f}%'.\n",
    "                  format(CNT, valid_loss.result(), valid_accuracy.result()*100)) \n",
    "            print('##################################################')\n",
    "            \n",
    "    template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}%, Valid Loss: {:.2f}, Valid Accuracy: {:.2f}%'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         valid_loss.result(),\n",
    "                         valid_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition-py36",
   "language": "python",
   "name": "competition-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
