{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from config import *\n",
    "import pandas as pd\n",
    "from tools import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target #########\n",
    "TARGET = 'gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### target\n",
    "target = pd.read_pickle(TRAIN_DIR+USER_LOG_PATH)\n",
    "target = target.groupby(['user_id']).agg('first').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Per Day Click Times\n",
    "train_df = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['per_day_click'])\n",
    "test_df = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['per_day_click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df.shape[0] == 900000 and test_df.shape[0] == 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 点击 List CounterVec 特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creative_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['creative_id'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['creative_id'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['creative_id'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "creative_id_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "creative_id_cntv_user = creative_id_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'creative_id_cntv_user.npz', creative_id_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['ad_id'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['ad_id'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['ad_id'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "ad_id_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "ad_id_cntv_user = ad_id_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'ad_id_cntv_user.npz', ad_id_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### advertiser_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['advertiser_id'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['advertiser_id'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['advertiser_id'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "advertiser_id_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "advertiser_id_cntv_user = advertiser_id_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'advertiser_id_cntv_user.npz', advertiser_id_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['product_id'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['product_id'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['product_id'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "product_id_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "product_id_cntv_user = product_id_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'product_id_cntv_user.npz', product_id_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['product_category'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['product_category'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['product_category'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "product_category_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "product_category_cntv_user = product_category_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'product_category_cntv_user.npz', product_category_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT['industry'])\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+CLK_PATH_DICT['industry'])\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['industry'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "industry_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "industry_cntv_user = industry_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'industry_cntv_user.npz', industry_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_click_seq = pd.read_pickle(TRAIN_DIR+'clk_list_time.pkl')\n",
    "ts_click_seq = pd.read_pickle(TEST_DIR+'clk_list_time.pkl')\n",
    "\n",
    "assert tr_click_seq['user_id'].values.tolist() == train_df.index.values.tolist()\n",
    "assert ts_click_seq['user_id'].values.tolist() == test_df.index.values.tolist()\n",
    "\n",
    "click_seq = pd.concat([tr_click_seq, ts_click_seq], axis=0)['time'].values.tolist()\n",
    "\n",
    "cntv = CountVectorizer(tokenizer=lambda x: x, max_features=100000, max_df=0.9, min_df=30, lowercase=False)\n",
    "\n",
    "time_cntv_user = cntv.fit_transform(click_seq)\n",
    "\n",
    "time_cntv_user = time_cntv_user.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(TRAIN_DIR+'time_cntv_user.npz', time_cntv_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = sparse.csr_matrix(train_df.values)\n",
    "test_csr = sparse.csr_matrix(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = sparse.hstack((train_csr, cntv_user[:900000])).tocsr()\n",
    "test_csr  = sparse.hstack((test_csr , cntv_user[900000:])).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_csr, target['age']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred), \"length of y_true and y_pred not equal\"\n",
    "    total_example = len(y_true)\n",
    "    right_cnt = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if t == p:\n",
    "            right_cnt += 1\n",
    "    return right_cnt / total_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { \n",
    "    'boosting_type': 'gbdt',  \n",
    "    'objective': 'multiclass',  \n",
    "    'num_class': 10,  \n",
    "    'metric': ['multi_error'],  \n",
    "    'num_leaves': 2**9,  \n",
    "    'min_data_in_leaf': 500,  \n",
    "    'learning_rate': 0.1,  \n",
    "    'feature_fraction': 0.8,  \n",
    "    'bagging_fraction': 0.8,  \n",
    "    'bagging_freq': 5,  \n",
    "    'lambda_l1': 0.4,  \n",
    "    'lambda_l2': 0.5,  \n",
    "    'min_gain_to_split': 0.2,  \n",
    "    'verbose': -1,\n",
    "    'num_threads':6,\n",
    "    'n_estimators': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "folds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=np.random.randint(2020))\n",
    "# 五折交叉验证\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    clf = lgb.train(param, \n",
    "                    trn_data, \n",
    "                    valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval = 100, \n",
    "                    early_stopping_rounds = 200)\n",
    "    \n",
    "    y_val_pred = clf.predict(X_train[val_idx])\n",
    "    y_val_pred = np.argmax(y_val_pred,axis=-1).tolist()\n",
    "    acc = accuracy(y_train[val_idx].values.tolist(), y_val_pred)\n",
    "    print(\"kfold: {:d}, accuracy: {:.4f}\".format(fold_+1, acc))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model('/home/huangzc/competition/tencent/model_ckpt/lgb/model-1.35.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
