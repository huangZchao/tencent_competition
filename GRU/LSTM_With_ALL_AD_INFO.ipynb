{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "tf.enable_eager_execution()\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter\n",
    "###################\n",
    "SAVE_PATH = '/home/huangzc/competition/tencent/model_ckpt/LSTM/model_all.ckpt'\n",
    "EMB_SIZE = 50\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "TARGET = 'age'\n",
    "COLS_NAME = ['creative_id', 'advertiser_id', 'ad_id', 'product_id', 'product_category', 'industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data\n",
    "### TARGET DF\n",
    "tr_user_df = pd.read_pickle(TRAIN_DIR+USER_PATH)\n",
    "tr_user_df = tr_user_df.groupby(['user_id']).agg({'age': 'first', 'gender': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ad_df = pd.read_pickle(TRAIN_DIR+AD_PATH)\n",
    "ts_ad_df = pd.read_pickle(TEST_DIR+AD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:12<00:00, 32.16s/it]\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "for col in tqdm(COLS_NAME):\n",
    "    train_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    test_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    \n",
    "train_df.append(tr_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_df(creative_df, advertiser_df, ad_df, product_id_df, product_cat_df, industry_df, user_df=None):\n",
    "    if user_df is  None:\n",
    "        user_df = creative_df[['user_id']]\n",
    "        user_df[TARGET] = np.nan\n",
    "    assert user_df['user_id'].values.tolist() == creative_df['user_id'].values.tolist() \\\n",
    "    == ad_df['user_id'].values.tolist() == product_id_df['user_id'].values.tolist() \\\n",
    "    == product_cat_df['user_id'].values.tolist() == industry_df['user_id'].values.tolist() \\\n",
    "    == advertiser_df['user_id'].values.tolist()\n",
    "\n",
    "    del advertiser_df['user_id'], ad_df['user_id'], product_id_df['user_id'], product_cat_df['user_id'], industry_df['user_id']\n",
    "    \n",
    "    grid_df = pd.concat([creative_df, advertiser_df, \n",
    "                         ad_df, product_id_df, \n",
    "                         product_cat_df, industry_df,\n",
    "                         user_df[[TARGET]]], axis=1)\n",
    "\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = get_grid_df(*train_df)\n",
    "grid_df_test = get_grid_df(*test_df)\n",
    "grid_df[TARGET] = grid_df[TARGET] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(grid_df, test_size=0.2, random_state=2020)\n",
    "test = grid_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_feature():  \n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    x_test = []\n",
    "    \n",
    "    sentence_size = max(int(grid_df[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.6)), \n",
    "                        int(grid_df_test[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.6)))\n",
    "    print('choose sentences max len: %d' % (sentence_size))\n",
    "    print(\"Pad sequences (samples x time)\")      \n",
    "    for col in tqdm(COLS_NAME):\n",
    "        x_train.append(sequence.pad_sequences(train[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_val.append(sequence.pad_sequences(val[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_test.append(sequence.pad_sequences(test[col], \n",
    "                                            maxlen=sentence_size, \n",
    "                                            padding='post',\n",
    "                                            truncating='post',\n",
    "                                            dtype='int64',\n",
    "                                            value=0\n",
    "                                           ))\n",
    "    print('feature count: train->%d, valid->%d, test->%d' %(len(x_train), len(x_val), len(x_test)))\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose sentences max len: 29\n",
      "Pad sequences (samples x time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:34<00:00, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature count: train->6, valid->6, test->6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, x_test = pad_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get tf dataset\n",
    "def get_train_ds(x, y): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.repeat(EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "def get_test_ds(x, ): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_train_ds(tuple(x_train), train[TARGET].values)\n",
    "valid_ds = get_train_ds(tuple(x_val), val[TARGET].values)\n",
    "test_ds = get_test_ds(tuple(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_id\n",
      "advertiser_id\n",
      "ad_id\n",
      "product_id\n",
      "product_category\n",
      "industry\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = []\n",
    "temp = pd.concat([tr_ad_df, ts_ad_df], axis=0)\n",
    "for col in COLS_NAME:\n",
    "    print(col)\n",
    "    vocab_sizes.append(max(temp[col].unique().tolist()) + 1) ### padding 0 need add 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.load('/home/huangzc/competition/tencent/data/train_preliminary/gensim_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct Model\n",
    "#################################\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, EMB_SIZE, weights=[weights])\n",
    "        self.embeddings = []\n",
    "        for s in vocab_sizes:\n",
    "            self.embeddings.append(tf.keras.layers.Embedding(s, EMB_SIZE))\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=-1)\n",
    "        \n",
    "        ### LSTM\n",
    "        self.lstms = []\n",
    "        for i in range(1):\n",
    "            self.lstms.append(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))\n",
    "        self.lstms.append(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embs = []\n",
    "        for emb, inp in zip(self.embeddings, inputs):\n",
    "            x = emb(inp)            \n",
    "            embs.append(x)\n",
    "        x = self.concat(embs)\n",
    "        \n",
    "        for lstm in self.lstms:\n",
    "            x = lstm(x)\n",
    " \n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restore the weights\n",
    "# model.load_weights(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss & Metric\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(features, labels):\n",
    "    predictions = model(features)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(v_loss)\n",
    "    valid_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangzc/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "batch: 1, batch train loss: 2.30, train acc: 6.54%, consuming tine: 19.06\n",
      "batch: 2, batch train loss: 2.30, train acc: 9.13%, consuming tine: 7.68\n",
      "batch: 3, batch train loss: 2.30, train acc: 11.43%, consuming tine: 7.68\n",
      "batch: 4, batch train loss: 2.29, train acc: 13.45%, consuming tine: 5.72\n",
      "batch: 5, batch train loss: 2.29, train acc: 14.57%, consuming tine: 5.31\n",
      "batch: 6, batch train loss: 2.28, train acc: 15.59%, consuming tine: 5.88\n",
      "batch: 7, batch train loss: 2.27, train acc: 16.43%, consuming tine: 5.07\n",
      "batch: 8, batch train loss: 2.26, train acc: 17.18%, consuming tine: 4.93\n",
      "batch: 9, batch train loss: 2.25, train acc: 17.78%, consuming tine: 5.27\n",
      "batch: 10, batch train loss: 2.24, train acc: 18.22%, consuming tine: 4.89\n",
      "batch: 11, batch train loss: 2.23, train acc: 18.76%, consuming tine: 5.19\n",
      "batch: 12, batch train loss: 2.22, train acc: 19.12%, consuming tine: 4.99\n",
      "batch: 13, batch train loss: 2.21, train acc: 19.22%, consuming tine: 5.14\n",
      "batch: 14, batch train loss: 2.20, train acc: 19.19%, consuming tine: 5.15\n",
      "batch: 15, batch train loss: 2.20, train acc: 19.47%, consuming tine: 4.99\n",
      "batch: 16, batch train loss: 2.18, train acc: 19.77%, consuming tine: 5.41\n",
      "batch: 17, batch train loss: 2.17, train acc: 19.89%, consuming tine: 5.10\n",
      "batch: 18, batch train loss: 2.17, train acc: 20.14%, consuming tine: 5.19\n",
      "batch: 19, batch train loss: 2.16, train acc: 20.29%, consuming tine: 5.13\n",
      "batch: 20, batch train loss: 2.15, train acc: 20.32%, consuming tine: 4.95\n",
      "batch: 21, batch train loss: 2.15, train acc: 20.38%, consuming tine: 5.40\n",
      "batch: 22, batch train loss: 2.14, train acc: 20.53%, consuming tine: 5.09\n",
      "batch: 23, batch train loss: 2.14, train acc: 20.55%, consuming tine: 4.89\n",
      "batch: 24, batch train loss: 2.14, train acc: 20.62%, consuming tine: 5.20\n",
      "batch: 25, batch train loss: 2.13, train acc: 20.61%, consuming tine: 5.39\n",
      "batch: 26, batch train loss: 2.13, train acc: 20.63%, consuming tine: 4.88\n",
      "batch: 27, batch train loss: 2.12, train acc: 20.69%, consuming tine: 5.20\n",
      "batch: 28, batch train loss: 2.12, train acc: 20.81%, consuming tine: 5.09\n",
      "batch: 29, batch train loss: 2.12, train acc: 20.91%, consuming tine: 4.90\n",
      "batch: 30, batch train loss: 2.11, train acc: 21.01%, consuming tine: 4.48\n",
      "batch: 31, batch train loss: 2.11, train acc: 21.04%, consuming tine: 5.20\n",
      "batch: 32, batch train loss: 2.11, train acc: 21.14%, consuming tine: 5.00\n",
      "batch: 33, batch train loss: 2.11, train acc: 21.24%, consuming tine: 5.00\n",
      "batch: 34, batch train loss: 2.10, train acc: 21.27%, consuming tine: 4.79\n",
      "batch: 35, batch train loss: 2.10, train acc: 21.24%, consuming tine: 5.09\n",
      "batch: 36, batch train loss: 2.10, train acc: 21.30%, consuming tine: 5.09\n",
      "batch: 37, batch train loss: 2.10, train acc: 21.32%, consuming tine: 5.27\n",
      "batch: 38, batch train loss: 2.09, train acc: 21.39%, consuming tine: 4.60\n",
      "batch: 39, batch train loss: 2.09, train acc: 21.40%, consuming tine: 5.31\n",
      "batch: 40, batch train loss: 2.09, train acc: 21.41%, consuming tine: 5.48\n",
      "batch: 41, batch train loss: 2.09, train acc: 21.45%, consuming tine: 5.51\n",
      "batch: 42, batch train loss: 2.08, train acc: 21.51%, consuming tine: 5.81\n",
      "batch: 43, batch train loss: 2.08, train acc: 21.53%, consuming tine: 5.39\n",
      "batch: 44, batch train loss: 2.08, train acc: 21.60%, consuming tine: 5.38\n",
      "batch: 45, batch train loss: 2.08, train acc: 21.60%, consuming tine: 5.52\n",
      "batch: 46, batch train loss: 2.08, train acc: 21.67%, consuming tine: 5.04\n",
      "batch: 47, batch train loss: 2.07, train acc: 21.67%, consuming tine: 5.71\n",
      "batch: 48, batch train loss: 2.07, train acc: 21.67%, consuming tine: 5.19\n",
      "batch: 49, batch train loss: 2.07, train acc: 21.70%, consuming tine: 5.28\n",
      "batch: 50, batch train loss: 2.07, train acc: 21.73%, consuming tine: 5.40\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 1.94, valid acc: 23.14%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 2.06, train acc: 21.69%, consuming tine: 5.32\n",
      "batch: 52, batch train loss: 2.06, train acc: 21.73%, consuming tine: 5.27\n",
      "batch: 53, batch train loss: 2.06, train acc: 21.82%, consuming tine: 5.27\n",
      "batch: 54, batch train loss: 2.06, train acc: 21.84%, consuming tine: 4.78\n",
      "batch: 55, batch train loss: 2.05, train acc: 21.89%, consuming tine: 5.62\n",
      "batch: 56, batch train loss: 2.05, train acc: 21.95%, consuming tine: 5.37\n",
      "batch: 57, batch train loss: 2.05, train acc: 21.99%, consuming tine: 5.39\n",
      "batch: 58, batch train loss: 2.05, train acc: 22.02%, consuming tine: 5.12\n",
      "batch: 59, batch train loss: 2.04, train acc: 22.07%, consuming tine: 5.06\n",
      "batch: 60, batch train loss: 2.04, train acc: 22.15%, consuming tine: 5.09\n",
      "batch: 61, batch train loss: 2.04, train acc: 22.19%, consuming tine: 5.39\n",
      "batch: 62, batch train loss: 2.03, train acc: 22.29%, consuming tine: 5.29\n",
      "batch: 63, batch train loss: 2.03, train acc: 22.35%, consuming tine: 5.60\n",
      "batch: 64, batch train loss: 2.03, train acc: 22.42%, consuming tine: 5.29\n",
      "batch: 65, batch train loss: 2.03, train acc: 22.47%, consuming tine: 5.40\n",
      "batch: 66, batch train loss: 2.02, train acc: 22.53%, consuming tine: 5.08\n",
      "batch: 67, batch train loss: 2.02, train acc: 22.59%, consuming tine: 4.88\n",
      "batch: 68, batch train loss: 2.02, train acc: 22.65%, consuming tine: 5.59\n",
      "batch: 69, batch train loss: 2.02, train acc: 22.71%, consuming tine: 5.31\n",
      "batch: 70, batch train loss: 2.01, train acc: 22.81%, consuming tine: 5.70\n",
      "batch: 71, batch train loss: 2.01, train acc: 22.88%, consuming tine: 5.41\n",
      "batch: 72, batch train loss: 2.01, train acc: 22.94%, consuming tine: 5.33\n",
      "batch: 73, batch train loss: 2.00, train acc: 23.01%, consuming tine: 5.52\n",
      "batch: 74, batch train loss: 2.00, train acc: 23.06%, consuming tine: 5.07\n",
      "batch: 75, batch train loss: 2.00, train acc: 23.10%, consuming tine: 4.70\n",
      "batch: 76, batch train loss: 2.00, train acc: 23.16%, consuming tine: 5.73\n",
      "batch: 77, batch train loss: 1.99, train acc: 23.26%, consuming tine: 5.25\n",
      "batch: 78, batch train loss: 1.99, train acc: 23.31%, consuming tine: 5.31\n",
      "batch: 79, batch train loss: 1.99, train acc: 23.39%, consuming tine: 5.55\n",
      "batch: 80, batch train loss: 1.98, train acc: 23.48%, consuming tine: 5.53\n",
      "batch: 81, batch train loss: 1.98, train acc: 23.55%, consuming tine: 5.18\n",
      "batch: 82, batch train loss: 1.98, train acc: 23.61%, consuming tine: 5.70\n",
      "batch: 83, batch train loss: 1.98, train acc: 23.71%, consuming tine: 5.30\n",
      "batch: 84, batch train loss: 1.97, train acc: 23.78%, consuming tine: 5.50\n",
      "batch: 85, batch train loss: 1.97, train acc: 23.85%, consuming tine: 5.29\n",
      "batch: 86, batch train loss: 1.97, train acc: 23.93%, consuming tine: 5.59\n",
      "batch: 87, batch train loss: 1.97, train acc: 23.96%, consuming tine: 5.49\n",
      "batch: 88, batch train loss: 1.96, train acc: 24.04%, consuming tine: 4.81\n",
      "batch: 89, batch train loss: 1.96, train acc: 24.14%, consuming tine: 5.08\n",
      "batch: 90, batch train loss: 1.96, train acc: 24.18%, consuming tine: 5.10\n",
      "batch: 91, batch train loss: 1.96, train acc: 24.23%, consuming tine: 5.38\n",
      "batch: 92, batch train loss: 1.95, train acc: 24.28%, consuming tine: 4.91\n",
      "batch: 93, batch train loss: 1.95, train acc: 24.35%, consuming tine: 5.28\n",
      "batch: 94, batch train loss: 1.95, train acc: 24.39%, consuming tine: 5.38\n",
      "batch: 95, batch train loss: 1.95, train acc: 24.44%, consuming tine: 5.42\n",
      "batch: 96, batch train loss: 1.94, train acc: 24.48%, consuming tine: 5.09\n",
      "batch: 97, batch train loss: 1.94, train acc: 24.54%, consuming tine: 5.21\n",
      "batch: 98, batch train loss: 1.94, train acc: 24.62%, consuming tine: 5.37\n",
      "batch: 99, batch train loss: 1.94, train acc: 24.66%, consuming tine: 5.21\n",
      "batch: 100, batch train loss: 1.93, train acc: 24.72%, consuming tine: 5.39\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 1.82, valid acc: 26.82%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 1.93, train acc: 24.76%, consuming tine: 5.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 102, batch train loss: 1.93, train acc: 24.83%, consuming tine: 5.52\n",
      "batch: 103, batch train loss: 1.93, train acc: 24.90%, consuming tine: 5.08\n",
      "batch: 104, batch train loss: 1.92, train acc: 24.97%, consuming tine: 5.28\n",
      "batch: 105, batch train loss: 1.92, train acc: 25.03%, consuming tine: 5.27\n",
      "batch: 106, batch train loss: 1.92, train acc: 25.09%, consuming tine: 5.30\n",
      "batch: 107, batch train loss: 1.92, train acc: 25.18%, consuming tine: 5.69\n",
      "batch: 108, batch train loss: 1.92, train acc: 25.24%, consuming tine: 5.09\n",
      "batch: 109, batch train loss: 1.91, train acc: 25.29%, consuming tine: 5.25\n",
      "batch: 110, batch train loss: 1.91, train acc: 25.33%, consuming tine: 5.65\n",
      "batch: 111, batch train loss: 1.91, train acc: 25.40%, consuming tine: 5.07\n",
      "batch: 112, batch train loss: 1.91, train acc: 25.44%, consuming tine: 5.38\n",
      "batch: 113, batch train loss: 1.91, train acc: 25.49%, consuming tine: 4.95\n",
      "batch: 114, batch train loss: 1.90, train acc: 25.54%, consuming tine: 5.24\n",
      "batch: 115, batch train loss: 1.90, train acc: 25.59%, consuming tine: 5.09\n",
      "batch: 116, batch train loss: 1.90, train acc: 25.65%, consuming tine: 5.05\n",
      "batch: 117, batch train loss: 1.90, train acc: 25.72%, consuming tine: 5.05\n",
      "batch: 118, batch train loss: 1.89, train acc: 25.78%, consuming tine: 4.99\n",
      "batch: 119, batch train loss: 1.89, train acc: 25.83%, consuming tine: 5.29\n",
      "batch: 120, batch train loss: 1.89, train acc: 25.88%, consuming tine: 5.19\n",
      "batch: 121, batch train loss: 1.89, train acc: 25.94%, consuming tine: 4.79\n",
      "batch: 122, batch train loss: 1.89, train acc: 25.99%, consuming tine: 5.19\n",
      "batch: 123, batch train loss: 1.88, train acc: 26.04%, consuming tine: 5.02\n",
      "batch: 124, batch train loss: 1.88, train acc: 26.08%, consuming tine: 5.15\n",
      "batch: 125, batch train loss: 1.88, train acc: 26.13%, consuming tine: 5.22\n",
      "batch: 126, batch train loss: 1.88, train acc: 26.16%, consuming tine: 5.28\n",
      "batch: 127, batch train loss: 1.88, train acc: 26.23%, consuming tine: 4.98\n",
      "batch: 128, batch train loss: 1.88, train acc: 26.26%, consuming tine: 4.67\n",
      "batch: 129, batch train loss: 1.87, train acc: 26.31%, consuming tine: 5.17\n",
      "batch: 130, batch train loss: 1.87, train acc: 26.38%, consuming tine: 5.14\n",
      "batch: 131, batch train loss: 1.87, train acc: 26.44%, consuming tine: 5.02\n",
      "batch: 132, batch train loss: 1.87, train acc: 26.50%, consuming tine: 4.97\n",
      "batch: 133, batch train loss: 1.87, train acc: 26.55%, consuming tine: 5.10\n",
      "batch: 134, batch train loss: 1.86, train acc: 26.60%, consuming tine: 5.26\n",
      "batch: 135, batch train loss: 1.86, train acc: 26.67%, consuming tine: 5.16\n",
      "batch: 136, batch train loss: 1.86, train acc: 26.72%, consuming tine: 5.36\n",
      "batch: 137, batch train loss: 1.86, train acc: 26.76%, consuming tine: 5.17\n",
      "batch: 138, batch train loss: 1.86, train acc: 26.79%, consuming tine: 4.53\n",
      "batch: 139, batch train loss: 1.86, train acc: 26.85%, consuming tine: 4.86\n",
      "batch: 140, batch train loss: 1.85, train acc: 26.89%, consuming tine: 5.14\n",
      "batch: 141, batch train loss: 1.85, train acc: 26.94%, consuming tine: 4.75\n",
      "batch: 142, batch train loss: 1.85, train acc: 27.00%, consuming tine: 5.31\n",
      "batch: 143, batch train loss: 1.85, train acc: 27.03%, consuming tine: 4.98\n",
      "batch: 144, batch train loss: 1.85, train acc: 27.08%, consuming tine: 4.76\n",
      "batch: 145, batch train loss: 1.85, train acc: 27.14%, consuming tine: 5.00\n",
      "batch: 146, batch train loss: 1.84, train acc: 27.18%, consuming tine: 4.70\n",
      "batch: 147, batch train loss: 1.84, train acc: 27.22%, consuming tine: 4.80\n",
      "batch: 148, batch train loss: 1.84, train acc: 27.28%, consuming tine: 4.58\n",
      "batch: 149, batch train loss: 1.84, train acc: 27.31%, consuming tine: 4.49\n",
      "batch: 150, batch train loss: 1.84, train acc: 27.37%, consuming tine: 4.60\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 1.75, valid acc: 29.37%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 1.84, train acc: 27.41%, consuming tine: 5.68\n",
      "batch: 152, batch train loss: 1.83, train acc: 27.44%, consuming tine: 4.80\n",
      "batch: 153, batch train loss: 1.83, train acc: 27.50%, consuming tine: 4.69\n",
      "batch: 154, batch train loss: 1.83, train acc: 27.54%, consuming tine: 4.73\n",
      "batch: 155, batch train loss: 1.83, train acc: 27.61%, consuming tine: 4.71\n",
      "batch: 156, batch train loss: 1.83, train acc: 27.65%, consuming tine: 4.43\n",
      "batch: 157, batch train loss: 1.83, train acc: 27.69%, consuming tine: 4.69\n",
      "batch: 158, batch train loss: 1.83, train acc: 27.75%, consuming tine: 4.60\n",
      "batch: 159, batch train loss: 1.82, train acc: 27.79%, consuming tine: 4.29\n",
      "batch: 160, batch train loss: 1.82, train acc: 27.84%, consuming tine: 4.72\n",
      "batch: 161, batch train loss: 1.82, train acc: 27.88%, consuming tine: 4.78\n",
      "batch: 162, batch train loss: 1.82, train acc: 27.94%, consuming tine: 4.55\n",
      "batch: 163, batch train loss: 1.82, train acc: 27.98%, consuming tine: 4.93\n",
      "batch: 164, batch train loss: 1.82, train acc: 28.01%, consuming tine: 5.88\n",
      "batch: 165, batch train loss: 1.82, train acc: 28.05%, consuming tine: 5.79\n",
      "batch: 166, batch train loss: 1.81, train acc: 28.08%, consuming tine: 5.27\n",
      "batch: 167, batch train loss: 1.81, train acc: 28.12%, consuming tine: 5.89\n",
      "batch: 168, batch train loss: 1.81, train acc: 28.16%, consuming tine: 5.75\n",
      "batch: 169, batch train loss: 1.81, train acc: 28.20%, consuming tine: 5.63\n",
      "batch: 170, batch train loss: 1.81, train acc: 28.24%, consuming tine: 5.51\n",
      "batch: 171, batch train loss: 1.81, train acc: 28.29%, consuming tine: 5.69\n",
      "batch: 172, batch train loss: 1.81, train acc: 28.34%, consuming tine: 5.79\n",
      "batch: 173, batch train loss: 1.80, train acc: 28.39%, consuming tine: 5.41\n",
      "batch: 174, batch train loss: 1.80, train acc: 28.42%, consuming tine: 5.80\n",
      "batch: 175, batch train loss: 1.80, train acc: 28.45%, consuming tine: 5.46\n",
      "batch: 176, batch train loss: 1.80, train acc: 28.51%, consuming tine: 5.51\n",
      "batch: 177, batch train loss: 1.80, train acc: 28.54%, consuming tine: 5.87\n",
      "batch: 178, batch train loss: 1.80, train acc: 28.58%, consuming tine: 5.85\n",
      "batch: 179, batch train loss: 1.80, train acc: 28.63%, consuming tine: 5.95\n",
      "batch: 180, batch train loss: 1.79, train acc: 28.67%, consuming tine: 6.09\n",
      "batch: 181, batch train loss: 1.79, train acc: 28.72%, consuming tine: 5.78\n",
      "batch: 182, batch train loss: 1.79, train acc: 28.75%, consuming tine: 5.80\n",
      "batch: 183, batch train loss: 1.79, train acc: 28.81%, consuming tine: 6.30\n",
      "batch: 184, batch train loss: 1.79, train acc: 28.85%, consuming tine: 5.38\n",
      "batch: 185, batch train loss: 1.79, train acc: 28.90%, consuming tine: 5.99\n",
      "batch: 186, batch train loss: 1.79, train acc: 28.94%, consuming tine: 5.59\n",
      "batch: 187, batch train loss: 1.79, train acc: 28.97%, consuming tine: 6.10\n",
      "batch: 188, batch train loss: 1.78, train acc: 29.01%, consuming tine: 5.68\n",
      "batch: 189, batch train loss: 1.78, train acc: 29.04%, consuming tine: 5.71\n",
      "batch: 190, batch train loss: 1.78, train acc: 29.06%, consuming tine: 5.67\n",
      "batch: 191, batch train loss: 1.78, train acc: 29.08%, consuming tine: 5.40\n",
      "batch: 192, batch train loss: 1.78, train acc: 29.11%, consuming tine: 5.99\n",
      "batch: 193, batch train loss: 1.78, train acc: 29.15%, consuming tine: 5.60\n",
      "batch: 194, batch train loss: 1.78, train acc: 29.19%, consuming tine: 5.78\n",
      "batch: 195, batch train loss: 1.78, train acc: 29.23%, consuming tine: 5.59\n",
      "batch: 196, batch train loss: 1.78, train acc: 29.26%, consuming tine: 5.79\n",
      "batch: 197, batch train loss: 1.77, train acc: 29.30%, consuming tine: 5.69\n",
      "batch: 198, batch train loss: 1.77, train acc: 29.35%, consuming tine: 5.70\n",
      "batch: 199, batch train loss: 1.77, train acc: 29.38%, consuming tine: 5.68\n",
      "batch: 200, batch train loss: 1.77, train acc: 29.43%, consuming tine: 5.39\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 1.70, valid acc: 31.15%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 1.77, train acc: 29.47%, consuming tine: 5.92\n",
      "batch: 202, batch train loss: 1.77, train acc: 29.50%, consuming tine: 5.52\n",
      "batch: 203, batch train loss: 1.77, train acc: 29.54%, consuming tine: 5.89\n",
      "batch: 204, batch train loss: 1.77, train acc: 29.59%, consuming tine: 5.49\n",
      "batch: 205, batch train loss: 1.77, train acc: 29.63%, consuming tine: 5.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 206, batch train loss: 1.76, train acc: 29.66%, consuming tine: 5.98\n",
      "batch: 207, batch train loss: 1.76, train acc: 29.69%, consuming tine: 5.49\n",
      "batch: 208, batch train loss: 1.76, train acc: 29.72%, consuming tine: 5.79\n",
      "batch: 209, batch train loss: 1.76, train acc: 29.74%, consuming tine: 5.73\n",
      "batch: 210, batch train loss: 1.76, train acc: 29.77%, consuming tine: 5.75\n",
      "batch: 211, batch train loss: 1.76, train acc: 29.81%, consuming tine: 5.70\n",
      "batch: 212, batch train loss: 1.76, train acc: 29.84%, consuming tine: 5.89\n",
      "batch: 213, batch train loss: 1.76, train acc: 29.87%, consuming tine: 5.88\n",
      "batch: 214, batch train loss: 1.76, train acc: 29.89%, consuming tine: 5.90\n",
      "batch: 215, batch train loss: 1.76, train acc: 29.93%, consuming tine: 5.40\n",
      "batch: 216, batch train loss: 1.76, train acc: 29.96%, consuming tine: 5.58\n",
      "batch: 217, batch train loss: 1.75, train acc: 29.99%, consuming tine: 5.68\n",
      "batch: 218, batch train loss: 1.75, train acc: 30.03%, consuming tine: 5.79\n",
      "batch: 219, batch train loss: 1.75, train acc: 30.08%, consuming tine: 5.69\n",
      "batch: 220, batch train loss: 1.75, train acc: 30.10%, consuming tine: 5.51\n",
      "batch: 221, batch train loss: 1.75, train acc: 30.13%, consuming tine: 5.68\n",
      "batch: 222, batch train loss: 1.75, train acc: 30.16%, consuming tine: 5.59\n",
      "batch: 223, batch train loss: 1.75, train acc: 30.19%, consuming tine: 6.06\n",
      "batch: 224, batch train loss: 1.75, train acc: 30.22%, consuming tine: 5.63\n",
      "batch: 225, batch train loss: 1.75, train acc: 30.24%, consuming tine: 5.91\n",
      "batch: 226, batch train loss: 1.75, train acc: 30.27%, consuming tine: 5.57\n",
      "batch: 227, batch train loss: 1.74, train acc: 30.30%, consuming tine: 6.18\n",
      "batch: 228, batch train loss: 1.74, train acc: 30.32%, consuming tine: 5.69\n",
      "batch: 229, batch train loss: 1.74, train acc: 30.33%, consuming tine: 5.41\n",
      "batch: 230, batch train loss: 1.74, train acc: 30.36%, consuming tine: 5.70\n",
      "batch: 231, batch train loss: 1.74, train acc: 30.39%, consuming tine: 5.47\n",
      "batch: 232, batch train loss: 1.74, train acc: 30.41%, consuming tine: 5.91\n",
      "batch: 233, batch train loss: 1.74, train acc: 30.44%, consuming tine: 5.79\n",
      "batch: 234, batch train loss: 1.74, train acc: 30.46%, consuming tine: 5.90\n",
      "batch: 235, batch train loss: 1.74, train acc: 30.49%, consuming tine: 5.88\n",
      "batch: 236, batch train loss: 1.74, train acc: 30.51%, consuming tine: 5.60\n",
      "batch: 237, batch train loss: 1.74, train acc: 30.53%, consuming tine: 5.85\n",
      "batch: 238, batch train loss: 1.74, train acc: 30.55%, consuming tine: 5.51\n",
      "batch: 239, batch train loss: 1.73, train acc: 30.57%, consuming tine: 5.52\n",
      "batch: 240, batch train loss: 1.73, train acc: 30.61%, consuming tine: 5.76\n",
      "batch: 241, batch train loss: 1.73, train acc: 30.63%, consuming tine: 5.40\n",
      "batch: 242, batch train loss: 1.73, train acc: 30.66%, consuming tine: 5.89\n",
      "batch: 243, batch train loss: 1.73, train acc: 30.68%, consuming tine: 4.88\n",
      "batch: 244, batch train loss: 1.73, train acc: 30.72%, consuming tine: 5.20\n",
      "batch: 245, batch train loss: 1.73, train acc: 30.73%, consuming tine: 5.12\n",
      "batch: 246, batch train loss: 1.73, train acc: 30.74%, consuming tine: 5.19\n",
      "batch: 247, batch train loss: 1.73, train acc: 30.77%, consuming tine: 5.31\n",
      "batch: 248, batch train loss: 1.73, train acc: 30.80%, consuming tine: 5.25\n",
      "batch: 249, batch train loss: 1.73, train acc: 30.81%, consuming tine: 5.67\n",
      "batch: 250, batch train loss: 1.73, train acc: 30.83%, consuming tine: 5.61\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 1.67, valid acc: 32.36%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 1.73, train acc: 30.85%, consuming tine: 5.78\n",
      "batch: 252, batch train loss: 1.72, train acc: 30.88%, consuming tine: 5.78\n",
      "batch: 253, batch train loss: 1.72, train acc: 30.90%, consuming tine: 5.41\n",
      "batch: 254, batch train loss: 1.72, train acc: 30.91%, consuming tine: 5.78\n",
      "batch: 255, batch train loss: 1.72, train acc: 30.93%, consuming tine: 5.59\n",
      "batch: 256, batch train loss: 1.72, train acc: 30.95%, consuming tine: 5.58\n",
      "batch: 257, batch train loss: 1.72, train acc: 30.98%, consuming tine: 5.40\n",
      "batch: 258, batch train loss: 1.72, train acc: 31.02%, consuming tine: 5.60\n",
      "batch: 259, batch train loss: 1.72, train acc: 31.04%, consuming tine: 5.88\n",
      "batch: 260, batch train loss: 1.72, train acc: 31.06%, consuming tine: 5.60\n",
      "batch: 261, batch train loss: 1.72, train acc: 31.08%, consuming tine: 5.69\n",
      "batch: 262, batch train loss: 1.72, train acc: 31.10%, consuming tine: 5.58\n",
      "batch: 263, batch train loss: 1.72, train acc: 31.11%, consuming tine: 5.81\n",
      "batch: 264, batch train loss: 1.72, train acc: 31.13%, consuming tine: 5.71\n",
      "batch: 265, batch train loss: 1.72, train acc: 31.15%, consuming tine: 5.48\n",
      "batch: 266, batch train loss: 1.72, train acc: 31.19%, consuming tine: 5.88\n",
      "batch: 267, batch train loss: 1.71, train acc: 31.21%, consuming tine: 5.39\n",
      "batch: 268, batch train loss: 1.71, train acc: 31.23%, consuming tine: 5.68\n",
      "batch: 269, batch train loss: 1.71, train acc: 31.26%, consuming tine: 5.70\n",
      "batch: 270, batch train loss: 1.71, train acc: 31.29%, consuming tine: 5.80\n",
      "batch: 271, batch train loss: 1.71, train acc: 31.32%, consuming tine: 5.38\n",
      "batch: 272, batch train loss: 1.71, train acc: 31.33%, consuming tine: 5.60\n",
      "batch: 273, batch train loss: 1.71, train acc: 31.35%, consuming tine: 5.68\n",
      "batch: 274, batch train loss: 1.71, train acc: 31.37%, consuming tine: 5.61\n",
      "batch: 275, batch train loss: 1.71, train acc: 31.39%, consuming tine: 5.89\n",
      "batch: 276, batch train loss: 1.71, train acc: 31.42%, consuming tine: 5.39\n",
      "batch: 277, batch train loss: 1.71, train acc: 31.43%, consuming tine: 5.92\n",
      "batch: 278, batch train loss: 1.71, train acc: 31.46%, consuming tine: 5.65\n",
      "batch: 279, batch train loss: 1.71, train acc: 31.48%, consuming tine: 5.78\n",
      "batch: 280, batch train loss: 1.71, train acc: 31.51%, consuming tine: 5.80\n",
      "batch: 281, batch train loss: 1.71, train acc: 31.53%, consuming tine: 5.80\n",
      "batch: 282, batch train loss: 1.70, train acc: 31.55%, consuming tine: 5.79\n",
      "batch: 283, batch train loss: 1.70, train acc: 31.57%, consuming tine: 5.80\n",
      "batch: 284, batch train loss: 1.70, train acc: 31.59%, consuming tine: 5.88\n",
      "batch: 285, batch train loss: 1.70, train acc: 31.61%, consuming tine: 5.50\n",
      "batch: 286, batch train loss: 1.70, train acc: 31.62%, consuming tine: 5.81\n",
      "batch: 287, batch train loss: 1.70, train acc: 31.64%, consuming tine: 5.57\n",
      "batch: 288, batch train loss: 1.70, train acc: 31.66%, consuming tine: 5.58\n",
      "batch: 289, batch train loss: 1.70, train acc: 31.68%, consuming tine: 5.70\n",
      "batch: 290, batch train loss: 1.70, train acc: 31.71%, consuming tine: 5.80\n",
      "batch: 291, batch train loss: 1.70, train acc: 31.72%, consuming tine: 5.70\n",
      "batch: 292, batch train loss: 1.70, train acc: 31.74%, consuming tine: 5.90\n",
      "batch: 293, batch train loss: 1.70, train acc: 31.76%, consuming tine: 5.57\n",
      "batch: 294, batch train loss: 1.70, train acc: 31.77%, consuming tine: 5.64\n",
      "batch: 295, batch train loss: 1.70, train acc: 31.79%, consuming tine: 5.53\n",
      "batch: 296, batch train loss: 1.70, train acc: 31.81%, consuming tine: 6.02\n",
      "batch: 297, batch train loss: 1.70, train acc: 31.83%, consuming tine: 5.67\n",
      "batch: 298, batch train loss: 1.70, train acc: 31.85%, consuming tine: 5.70\n",
      "batch: 299, batch train loss: 1.69, train acc: 31.88%, consuming tine: 5.83\n",
      "batch: 300, batch train loss: 1.69, train acc: 31.89%, consuming tine: 5.43\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 1.64, valid acc: 33.26%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 1.69, train acc: 31.91%, consuming tine: 5.56\n",
      "batch: 302, batch train loss: 1.69, train acc: 31.92%, consuming tine: 5.78\n",
      "batch: 303, batch train loss: 1.69, train acc: 31.94%, consuming tine: 5.61\n",
      "batch: 304, batch train loss: 1.69, train acc: 31.95%, consuming tine: 5.70\n",
      "batch: 305, batch train loss: 1.69, train acc: 31.97%, consuming tine: 5.69\n",
      "batch: 306, batch train loss: 1.69, train acc: 32.00%, consuming tine: 5.48\n",
      "batch: 307, batch train loss: 1.69, train acc: 32.01%, consuming tine: 5.71\n",
      "batch: 308, batch train loss: 1.69, train acc: 32.04%, consuming tine: 5.68\n",
      "batch: 309, batch train loss: 1.69, train acc: 32.06%, consuming tine: 5.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 310, batch train loss: 1.69, train acc: 32.08%, consuming tine: 5.87\n",
      "batch: 311, batch train loss: 1.69, train acc: 32.09%, consuming tine: 5.59\n",
      "batch: 312, batch train loss: 1.69, train acc: 32.11%, consuming tine: 6.05\n",
      "batch: 313, batch train loss: 1.69, train acc: 32.13%, consuming tine: 5.45\n",
      "batch: 314, batch train loss: 1.69, train acc: 32.15%, consuming tine: 6.09\n",
      "batch: 315, batch train loss: 1.69, train acc: 32.17%, consuming tine: 5.59\n",
      "batch: 316, batch train loss: 1.69, train acc: 32.19%, consuming tine: 5.82\n",
      "batch: 317, batch train loss: 1.68, train acc: 32.22%, consuming tine: 5.57\n",
      "batch: 318, batch train loss: 1.68, train acc: 32.23%, consuming tine: 5.68\n",
      "batch: 319, batch train loss: 1.68, train acc: 32.24%, consuming tine: 5.90\n",
      "batch: 320, batch train loss: 1.68, train acc: 32.26%, consuming tine: 5.59\n",
      "batch: 321, batch train loss: 1.68, train acc: 32.28%, consuming tine: 5.88\n",
      "batch: 322, batch train loss: 1.68, train acc: 32.31%, consuming tine: 5.59\n",
      "batch: 323, batch train loss: 1.68, train acc: 32.34%, consuming tine: 5.68\n",
      "batch: 324, batch train loss: 1.68, train acc: 32.35%, consuming tine: 5.71\n",
      "batch: 325, batch train loss: 1.68, train acc: 32.37%, consuming tine: 5.79\n",
      "batch: 326, batch train loss: 1.68, train acc: 32.39%, consuming tine: 5.80\n",
      "batch: 327, batch train loss: 1.68, train acc: 32.41%, consuming tine: 5.69\n",
      "batch: 328, batch train loss: 1.68, train acc: 32.43%, consuming tine: 5.57\n",
      "batch: 329, batch train loss: 1.68, train acc: 32.44%, consuming tine: 5.79\n",
      "batch: 330, batch train loss: 1.68, train acc: 32.46%, consuming tine: 5.41\n",
      "batch: 331, batch train loss: 1.68, train acc: 32.47%, consuming tine: 5.88\n",
      "batch: 332, batch train loss: 1.68, train acc: 32.48%, consuming tine: 5.58\n",
      "batch: 333, batch train loss: 1.68, train acc: 32.50%, consuming tine: 5.79\n",
      "batch: 334, batch train loss: 1.68, train acc: 32.52%, consuming tine: 5.81\n",
      "batch: 335, batch train loss: 1.68, train acc: 32.53%, consuming tine: 5.57\n",
      "batch: 336, batch train loss: 1.67, train acc: 32.54%, consuming tine: 5.88\n",
      "batch: 337, batch train loss: 1.67, train acc: 32.57%, consuming tine: 5.43\n",
      "batch: 338, batch train loss: 1.67, train acc: 32.58%, consuming tine: 5.68\n",
      "batch: 339, batch train loss: 1.67, train acc: 32.60%, consuming tine: 5.78\n",
      "batch: 340, batch train loss: 1.67, train acc: 32.62%, consuming tine: 5.49\n",
      "batch: 341, batch train loss: 1.67, train acc: 32.64%, consuming tine: 5.49\n",
      "batch: 342, batch train loss: 1.67, train acc: 32.65%, consuming tine: 5.71\n",
      "batch: 343, batch train loss: 1.67, train acc: 32.67%, consuming tine: 5.67\n",
      "batch: 344, batch train loss: 1.67, train acc: 32.68%, consuming tine: 5.88\n",
      "batch: 345, batch train loss: 1.67, train acc: 32.69%, consuming tine: 5.60\n",
      "batch: 346, batch train loss: 1.67, train acc: 32.71%, consuming tine: 5.90\n",
      "batch: 347, batch train loss: 1.67, train acc: 32.73%, consuming tine: 5.59\n",
      "batch: 348, batch train loss: 1.67, train acc: 32.75%, consuming tine: 5.79\n",
      "batch: 349, batch train loss: 1.67, train acc: 32.76%, consuming tine: 5.50\n",
      "batch: 350, batch train loss: 1.67, train acc: 32.78%, consuming tine: 5.50\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 1.62, valid acc: 34.00%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 1.67, train acc: 32.80%, consuming tine: 5.70\n",
      "batch: 352, batch train loss: 1.67, train acc: 32.82%, consuming tine: 5.67\n",
      "batch: 353, batch train loss: 1.67, train acc: 32.85%, consuming tine: 6.11\n",
      "batch: 354, batch train loss: 1.67, train acc: 32.87%, consuming tine: 5.48\n",
      "batch: 355, batch train loss: 1.67, train acc: 32.88%, consuming tine: 5.98\n",
      "batch: 356, batch train loss: 1.66, train acc: 32.89%, consuming tine: 5.48\n",
      "batch: 357, batch train loss: 1.66, train acc: 32.90%, consuming tine: 5.91\n",
      "batch: 358, batch train loss: 1.66, train acc: 32.92%, consuming tine: 5.60\n",
      "batch: 359, batch train loss: 1.66, train acc: 32.93%, consuming tine: 5.88\n",
      "batch: 360, batch train loss: 1.66, train acc: 32.95%, consuming tine: 5.78\n",
      "batch: 361, batch train loss: 1.66, train acc: 32.97%, consuming tine: 5.71\n",
      "batch: 362, batch train loss: 1.66, train acc: 32.99%, consuming tine: 5.91\n",
      "batch: 363, batch train loss: 1.66, train acc: 33.00%, consuming tine: 5.67\n",
      "batch: 364, batch train loss: 1.66, train acc: 33.02%, consuming tine: 5.69\n",
      "batch: 365, batch train loss: 1.66, train acc: 33.03%, consuming tine: 5.61\n",
      "batch: 366, batch train loss: 1.66, train acc: 33.05%, consuming tine: 5.78\n",
      "batch: 367, batch train loss: 1.66, train acc: 33.07%, consuming tine: 5.79\n",
      "batch: 368, batch train loss: 1.66, train acc: 33.08%, consuming tine: 5.75\n",
      "batch: 369, batch train loss: 1.66, train acc: 33.10%, consuming tine: 5.54\n",
      "batch: 370, batch train loss: 1.66, train acc: 33.11%, consuming tine: 5.70\n",
      "batch: 371, batch train loss: 1.66, train acc: 33.13%, consuming tine: 5.69\n",
      "batch: 372, batch train loss: 1.66, train acc: 33.14%, consuming tine: 5.50\n",
      "batch: 373, batch train loss: 1.66, train acc: 33.16%, consuming tine: 5.77\n",
      "batch: 374, batch train loss: 1.66, train acc: 33.18%, consuming tine: 5.69\n",
      "batch: 375, batch train loss: 1.66, train acc: 33.19%, consuming tine: 5.59\n",
      "batch: 376, batch train loss: 1.66, train acc: 33.20%, consuming tine: 5.88\n",
      "batch: 377, batch train loss: 1.66, train acc: 33.21%, consuming tine: 5.40\n",
      "batch: 378, batch train loss: 1.65, train acc: 33.23%, consuming tine: 5.90\n",
      "batch: 379, batch train loss: 1.65, train acc: 33.24%, consuming tine: 5.72\n",
      "batch: 380, batch train loss: 1.65, train acc: 33.25%, consuming tine: 5.60\n",
      "batch: 381, batch train loss: 1.65, train acc: 33.27%, consuming tine: 5.65\n",
      "batch: 382, batch train loss: 1.65, train acc: 33.29%, consuming tine: 5.61\n",
      "batch: 383, batch train loss: 1.65, train acc: 33.30%, consuming tine: 5.61\n",
      "batch: 384, batch train loss: 1.65, train acc: 33.31%, consuming tine: 5.48\n",
      "batch: 385, batch train loss: 1.65, train acc: 33.32%, consuming tine: 5.68\n",
      "batch: 386, batch train loss: 1.65, train acc: 33.34%, consuming tine: 5.51\n",
      "batch: 387, batch train loss: 1.65, train acc: 33.35%, consuming tine: 5.58\n",
      "batch: 388, batch train loss: 1.65, train acc: 33.37%, consuming tine: 5.63\n",
      "batch: 389, batch train loss: 1.65, train acc: 33.38%, consuming tine: 5.45\n",
      "batch: 390, batch train loss: 1.65, train acc: 33.38%, consuming tine: 5.59\n",
      "batch: 391, batch train loss: 1.65, train acc: 33.40%, consuming tine: 5.50\n",
      "batch: 392, batch train loss: 1.65, train acc: 33.41%, consuming tine: 5.58\n",
      "batch: 393, batch train loss: 1.65, train acc: 33.42%, consuming tine: 5.75\n",
      "batch: 394, batch train loss: 1.65, train acc: 33.43%, consuming tine: 5.53\n",
      "batch: 395, batch train loss: 1.65, train acc: 33.44%, consuming tine: 5.48\n",
      "batch: 396, batch train loss: 1.65, train acc: 33.46%, consuming tine: 5.99\n",
      "batch: 397, batch train loss: 1.65, train acc: 33.47%, consuming tine: 5.59\n",
      "batch: 398, batch train loss: 1.65, train acc: 33.49%, consuming tine: 5.50\n",
      "batch: 399, batch train loss: 1.65, train acc: 33.51%, consuming tine: 5.59\n",
      "batch: 400, batch train loss: 1.65, train acc: 33.52%, consuming tine: 5.40\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 1.60, valid acc: 34.60%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 1.65, train acc: 33.54%, consuming tine: 5.51\n",
      "batch: 402, batch train loss: 1.65, train acc: 33.55%, consuming tine: 5.69\n",
      "batch: 403, batch train loss: 1.65, train acc: 33.56%, consuming tine: 5.59\n",
      "batch: 404, batch train loss: 1.65, train acc: 33.57%, consuming tine: 5.40\n",
      "batch: 405, batch train loss: 1.64, train acc: 33.59%, consuming tine: 6.02\n",
      "batch: 406, batch train loss: 1.64, train acc: 33.60%, consuming tine: 5.56\n",
      "batch: 407, batch train loss: 1.64, train acc: 33.61%, consuming tine: 5.70\n",
      "batch: 408, batch train loss: 1.64, train acc: 33.62%, consuming tine: 5.81\n",
      "batch: 409, batch train loss: 1.64, train acc: 33.63%, consuming tine: 5.88\n",
      "batch: 410, batch train loss: 1.64, train acc: 33.65%, consuming tine: 5.69\n",
      "batch: 411, batch train loss: 1.64, train acc: 33.66%, consuming tine: 5.58\n",
      "batch: 412, batch train loss: 1.64, train acc: 33.67%, consuming tine: 5.58\n",
      "batch: 413, batch train loss: 1.64, train acc: 33.68%, consuming tine: 5.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 414, batch train loss: 1.64, train acc: 33.69%, consuming tine: 5.77\n",
      "batch: 415, batch train loss: 1.64, train acc: 33.71%, consuming tine: 5.40\n",
      "batch: 416, batch train loss: 1.64, train acc: 33.72%, consuming tine: 5.83\n",
      "batch: 417, batch train loss: 1.64, train acc: 33.73%, consuming tine: 5.57\n",
      "batch: 418, batch train loss: 1.64, train acc: 33.74%, consuming tine: 5.77\n",
      "batch: 419, batch train loss: 1.64, train acc: 33.75%, consuming tine: 5.70\n",
      "batch: 420, batch train loss: 1.64, train acc: 33.76%, consuming tine: 5.68\n",
      "batch: 421, batch train loss: 1.64, train acc: 33.77%, consuming tine: 5.64\n",
      "batch: 422, batch train loss: 1.64, train acc: 33.78%, consuming tine: 5.54\n",
      "batch: 423, batch train loss: 1.64, train acc: 33.79%, consuming tine: 5.60\n",
      "batch: 424, batch train loss: 1.64, train acc: 33.81%, consuming tine: 5.42\n",
      "batch: 425, batch train loss: 1.64, train acc: 33.82%, consuming tine: 5.56\n",
      "batch: 426, batch train loss: 1.64, train acc: 33.83%, consuming tine: 5.68\n",
      "batch: 427, batch train loss: 1.64, train acc: 33.84%, consuming tine: 5.49\n",
      "batch: 428, batch train loss: 1.64, train acc: 33.85%, consuming tine: 5.69\n",
      "batch: 429, batch train loss: 1.64, train acc: 33.86%, consuming tine: 5.39\n",
      "batch: 430, batch train loss: 1.64, train acc: 33.87%, consuming tine: 5.69\n",
      "batch: 431, batch train loss: 1.64, train acc: 33.88%, consuming tine: 5.59\n",
      "batch: 432, batch train loss: 1.63, train acc: 33.89%, consuming tine: 5.59\n",
      "batch: 433, batch train loss: 1.63, train acc: 33.90%, consuming tine: 5.59\n",
      "batch: 434, batch train loss: 1.63, train acc: 33.92%, consuming tine: 5.49\n",
      "batch: 435, batch train loss: 1.63, train acc: 33.93%, consuming tine: 5.59\n",
      "batch: 436, batch train loss: 1.63, train acc: 33.94%, consuming tine: 5.31\n",
      "batch: 437, batch train loss: 1.63, train acc: 33.95%, consuming tine: 5.38\n",
      "batch: 438, batch train loss: 1.63, train acc: 33.97%, consuming tine: 5.69\n",
      "batch: 439, batch train loss: 1.63, train acc: 33.98%, consuming tine: 5.49\n",
      "batch: 440, batch train loss: 1.63, train acc: 33.99%, consuming tine: 5.72\n",
      "batch: 441, batch train loss: 1.63, train acc: 34.00%, consuming tine: 5.56\n",
      "batch: 442, batch train loss: 1.63, train acc: 34.00%, consuming tine: 5.60\n",
      "batch: 443, batch train loss: 1.63, train acc: 34.01%, consuming tine: 5.78\n",
      "batch: 444, batch train loss: 1.63, train acc: 34.02%, consuming tine: 5.50\n",
      "batch: 445, batch train loss: 1.63, train acc: 34.04%, consuming tine: 5.89\n",
      "batch: 446, batch train loss: 1.63, train acc: 34.05%, consuming tine: 5.70\n",
      "batch: 447, batch train loss: 1.63, train acc: 34.06%, consuming tine: 5.68\n",
      "batch: 448, batch train loss: 1.63, train acc: 34.08%, consuming tine: 5.69\n",
      "batch: 449, batch train loss: 1.63, train acc: 34.09%, consuming tine: 5.39\n",
      "batch: 450, batch train loss: 1.63, train acc: 34.10%, consuming tine: 5.81\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 1.59, valid acc: 35.10%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 1.63, train acc: 34.11%, consuming tine: 5.51\n",
      "batch: 452, batch train loss: 1.63, train acc: 34.13%, consuming tine: 5.69\n",
      "batch: 453, batch train loss: 1.63, train acc: 34.14%, consuming tine: 5.49\n",
      "batch: 454, batch train loss: 1.63, train acc: 34.14%, consuming tine: 5.61\n",
      "batch: 455, batch train loss: 1.63, train acc: 34.16%, consuming tine: 5.77\n",
      "batch: 456, batch train loss: 1.63, train acc: 34.17%, consuming tine: 5.39\n",
      "batch: 457, batch train loss: 1.63, train acc: 34.18%, consuming tine: 5.71\n",
      "batch: 458, batch train loss: 1.63, train acc: 34.18%, consuming tine: 5.48\n",
      "batch: 459, batch train loss: 1.63, train acc: 34.19%, consuming tine: 5.71\n",
      "batch: 460, batch train loss: 1.63, train acc: 34.20%, consuming tine: 5.58\n",
      "batch: 461, batch train loss: 1.63, train acc: 34.21%, consuming tine: 5.59\n",
      "batch: 462, batch train loss: 1.62, train acc: 34.22%, consuming tine: 5.51\n",
      "batch: 463, batch train loss: 1.62, train acc: 34.24%, consuming tine: 5.49\n",
      "batch: 464, batch train loss: 1.62, train acc: 34.24%, consuming tine: 5.58\n",
      "batch: 465, batch train loss: 1.62, train acc: 34.26%, consuming tine: 5.77\n",
      "batch: 466, batch train loss: 1.62, train acc: 34.27%, consuming tine: 5.69\n",
      "batch: 467, batch train loss: 1.62, train acc: 34.28%, consuming tine: 5.71\n",
      "batch: 468, batch train loss: 1.62, train acc: 34.29%, consuming tine: 5.69\n",
      "batch: 469, batch train loss: 1.62, train acc: 34.30%, consuming tine: 5.59\n",
      "batch: 470, batch train loss: 1.62, train acc: 34.31%, consuming tine: 5.50\n",
      "batch: 471, batch train loss: 1.62, train acc: 34.33%, consuming tine: 5.68\n",
      "batch: 472, batch train loss: 1.62, train acc: 34.34%, consuming tine: 5.48\n",
      "batch: 473, batch train loss: 1.62, train acc: 34.35%, consuming tine: 5.50\n",
      "batch: 474, batch train loss: 1.62, train acc: 34.36%, consuming tine: 5.78\n",
      "batch: 475, batch train loss: 1.62, train acc: 34.37%, consuming tine: 5.69\n",
      "batch: 476, batch train loss: 1.62, train acc: 34.38%, consuming tine: 5.92\n",
      "batch: 477, batch train loss: 1.62, train acc: 34.38%, consuming tine: 5.67\n",
      "batch: 478, batch train loss: 1.62, train acc: 34.39%, consuming tine: 5.58\n",
      "batch: 479, batch train loss: 1.62, train acc: 34.41%, consuming tine: 5.69\n",
      "batch: 480, batch train loss: 1.62, train acc: 34.42%, consuming tine: 5.60\n",
      "batch: 481, batch train loss: 1.62, train acc: 34.43%, consuming tine: 5.73\n",
      "batch: 482, batch train loss: 1.62, train acc: 34.44%, consuming tine: 5.54\n",
      "batch: 483, batch train loss: 1.62, train acc: 34.45%, consuming tine: 5.58\n",
      "batch: 484, batch train loss: 1.62, train acc: 34.47%, consuming tine: 5.74\n",
      "batch: 485, batch train loss: 1.62, train acc: 34.48%, consuming tine: 5.65\n",
      "batch: 486, batch train loss: 1.62, train acc: 34.49%, consuming tine: 5.99\n",
      "batch: 487, batch train loss: 1.62, train acc: 34.50%, consuming tine: 5.49\n",
      "batch: 488, batch train loss: 1.62, train acc: 34.51%, consuming tine: 5.50\n",
      "batch: 489, batch train loss: 1.62, train acc: 34.52%, consuming tine: 5.69\n",
      "batch: 490, batch train loss: 1.62, train acc: 34.53%, consuming tine: 5.72\n",
      "batch: 491, batch train loss: 1.62, train acc: 34.54%, consuming tine: 5.78\n",
      "batch: 492, batch train loss: 1.62, train acc: 34.55%, consuming tine: 5.67\n",
      "batch: 493, batch train loss: 1.62, train acc: 34.57%, consuming tine: 5.80\n",
      "batch: 494, batch train loss: 1.61, train acc: 34.58%, consuming tine: 5.50\n",
      "batch: 495, batch train loss: 1.61, train acc: 34.60%, consuming tine: 5.69\n",
      "batch: 496, batch train loss: 1.61, train acc: 34.60%, consuming tine: 5.48\n",
      "batch: 497, batch train loss: 1.61, train acc: 34.61%, consuming tine: 5.79\n",
      "batch: 498, batch train loss: 1.61, train acc: 34.62%, consuming tine: 5.78\n",
      "batch: 499, batch train loss: 1.61, train acc: 34.63%, consuming tine: 5.41\n",
      "batch: 500, batch train loss: 1.61, train acc: 34.64%, consuming tine: 5.88\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 1.58, valid acc: 35.51%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 1.61, train acc: 34.65%, consuming tine: 5.51\n",
      "batch: 502, batch train loss: 1.61, train acc: 34.66%, consuming tine: 5.75\n",
      "batch: 503, batch train loss: 1.61, train acc: 34.66%, consuming tine: 5.61\n",
      "batch: 504, batch train loss: 1.61, train acc: 34.66%, consuming tine: 5.66\n",
      "batch: 505, batch train loss: 1.61, train acc: 34.67%, consuming tine: 5.69\n",
      "batch: 506, batch train loss: 1.61, train acc: 34.69%, consuming tine: 6.09\n",
      "batch: 507, batch train loss: 1.61, train acc: 34.70%, consuming tine: 5.59\n",
      "batch: 508, batch train loss: 1.61, train acc: 34.71%, consuming tine: 5.89\n",
      "batch: 509, batch train loss: 1.61, train acc: 34.72%, consuming tine: 5.79\n",
      "batch: 510, batch train loss: 1.61, train acc: 34.73%, consuming tine: 5.60\n",
      "batch: 511, batch train loss: 1.61, train acc: 34.73%, consuming tine: 5.87\n",
      "batch: 512, batch train loss: 1.61, train acc: 34.74%, consuming tine: 5.51\n",
      "batch: 513, batch train loss: 1.61, train acc: 34.76%, consuming tine: 5.88\n",
      "batch: 514, batch train loss: 1.61, train acc: 34.77%, consuming tine: 5.70\n",
      "batch: 515, batch train loss: 1.61, train acc: 34.78%, consuming tine: 5.59\n",
      "batch: 516, batch train loss: 1.61, train acc: 34.79%, consuming tine: 5.50\n",
      "batch: 517, batch train loss: 1.61, train acc: 34.80%, consuming tine: 5.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 518, batch train loss: 1.61, train acc: 34.81%, consuming tine: 5.60\n",
      "batch: 519, batch train loss: 1.61, train acc: 34.81%, consuming tine: 5.58\n",
      "batch: 520, batch train loss: 1.61, train acc: 34.82%, consuming tine: 5.59\n",
      "batch: 521, batch train loss: 1.61, train acc: 34.83%, consuming tine: 5.91\n",
      "batch: 522, batch train loss: 1.61, train acc: 34.84%, consuming tine: 5.67\n",
      "batch: 523, batch train loss: 1.61, train acc: 34.84%, consuming tine: 5.80\n",
      "batch: 524, batch train loss: 1.61, train acc: 34.86%, consuming tine: 5.49\n",
      "batch: 525, batch train loss: 1.61, train acc: 34.86%, consuming tine: 5.79\n",
      "batch: 526, batch train loss: 1.61, train acc: 34.87%, consuming tine: 5.79\n",
      "batch: 527, batch train loss: 1.61, train acc: 34.88%, consuming tine: 5.69\n",
      "batch: 528, batch train loss: 1.61, train acc: 34.89%, consuming tine: 5.56\n",
      "batch: 529, batch train loss: 1.61, train acc: 34.90%, consuming tine: 5.62\n",
      "batch: 530, batch train loss: 1.61, train acc: 34.91%, consuming tine: 5.64\n",
      "batch: 531, batch train loss: 1.61, train acc: 34.93%, consuming tine: 5.54\n",
      "batch: 532, batch train loss: 1.61, train acc: 34.94%, consuming tine: 5.89\n",
      "batch: 533, batch train loss: 1.61, train acc: 34.95%, consuming tine: 5.59\n",
      "batch: 534, batch train loss: 1.61, train acc: 34.96%, consuming tine: 5.80\n",
      "batch: 535, batch train loss: 1.60, train acc: 34.96%, consuming tine: 5.70\n",
      "batch: 536, batch train loss: 1.60, train acc: 34.97%, consuming tine: 5.59\n",
      "batch: 537, batch train loss: 1.60, train acc: 34.99%, consuming tine: 5.89\n",
      "batch: 538, batch train loss: 1.60, train acc: 34.99%, consuming tine: 5.67\n",
      "batch: 539, batch train loss: 1.60, train acc: 35.00%, consuming tine: 6.00\n",
      "batch: 540, batch train loss: 1.60, train acc: 35.01%, consuming tine: 5.59\n",
      "batch: 541, batch train loss: 1.60, train acc: 35.01%, consuming tine: 5.90\n",
      "batch: 542, batch train loss: 1.60, train acc: 35.03%, consuming tine: 5.72\n",
      "batch: 543, batch train loss: 1.60, train acc: 35.04%, consuming tine: 5.66\n",
      "batch: 544, batch train loss: 1.60, train acc: 35.05%, consuming tine: 5.90\n",
      "batch: 545, batch train loss: 1.60, train acc: 35.06%, consuming tine: 5.69\n",
      "batch: 546, batch train loss: 1.60, train acc: 35.07%, consuming tine: 4.98\n",
      "batch: 547, batch train loss: 1.60, train acc: 35.08%, consuming tine: 4.93\n",
      "batch: 548, batch train loss: 1.60, train acc: 35.09%, consuming tine: 4.68\n",
      "batch: 549, batch train loss: 1.60, train acc: 35.10%, consuming tine: 4.58\n",
      "batch: 550, batch train loss: 1.60, train acc: 35.10%, consuming tine: 4.87\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 1.57, valid acc: 35.90%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 1.60, train acc: 35.11%, consuming tine: 4.60\n",
      "batch: 552, batch train loss: 1.60, train acc: 35.12%, consuming tine: 4.49\n",
      "batch: 553, batch train loss: 1.60, train acc: 35.13%, consuming tine: 4.68\n",
      "batch: 554, batch train loss: 1.60, train acc: 35.13%, consuming tine: 4.59\n",
      "batch: 555, batch train loss: 1.60, train acc: 35.15%, consuming tine: 4.40\n",
      "batch: 556, batch train loss: 1.60, train acc: 35.15%, consuming tine: 4.30\n",
      "batch: 557, batch train loss: 1.60, train acc: 35.16%, consuming tine: 4.57\n",
      "batch: 558, batch train loss: 1.60, train acc: 35.17%, consuming tine: 4.50\n",
      "batch: 559, batch train loss: 1.60, train acc: 35.18%, consuming tine: 4.70\n",
      "batch: 560, batch train loss: 1.60, train acc: 35.19%, consuming tine: 4.81\n",
      "batch: 561, batch train loss: 1.60, train acc: 35.20%, consuming tine: 4.57\n",
      "batch: 562, batch train loss: 1.60, train acc: 35.21%, consuming tine: 4.81\n",
      "batch: 563, batch train loss: 1.60, train acc: 35.22%, consuming tine: 4.57\n",
      "batch: 564, batch train loss: 1.60, train acc: 35.23%, consuming tine: 4.49\n",
      "batch: 565, batch train loss: 1.60, train acc: 35.23%, consuming tine: 4.70\n",
      "batch: 566, batch train loss: 1.60, train acc: 35.24%, consuming tine: 4.49\n",
      "batch: 567, batch train loss: 1.60, train acc: 35.25%, consuming tine: 4.59\n",
      "batch: 568, batch train loss: 1.60, train acc: 35.26%, consuming tine: 4.69\n",
      "batch: 569, batch train loss: 1.60, train acc: 35.27%, consuming tine: 4.59\n",
      "batch: 570, batch train loss: 1.60, train acc: 35.28%, consuming tine: 4.19\n",
      "batch: 571, batch train loss: 1.60, train acc: 35.29%, consuming tine: 4.69\n",
      "batch: 572, batch train loss: 1.60, train acc: 35.30%, consuming tine: 4.81\n",
      "batch: 573, batch train loss: 1.60, train acc: 35.31%, consuming tine: 4.59\n",
      "batch: 574, batch train loss: 1.60, train acc: 35.32%, consuming tine: 4.73\n",
      "batch: 575, batch train loss: 1.60, train acc: 35.33%, consuming tine: 4.66\n",
      "batch: 576, batch train loss: 1.60, train acc: 35.34%, consuming tine: 4.58\n",
      "batch: 577, batch train loss: 1.59, train acc: 35.35%, consuming tine: 4.98\n",
      "batch: 578, batch train loss: 1.59, train acc: 35.35%, consuming tine: 4.60\n",
      "batch: 579, batch train loss: 1.59, train acc: 35.36%, consuming tine: 4.59\n",
      "batch: 580, batch train loss: 1.59, train acc: 35.37%, consuming tine: 4.79\n",
      "batch: 581, batch train loss: 1.59, train acc: 35.38%, consuming tine: 4.70\n",
      "batch: 582, batch train loss: 1.59, train acc: 35.39%, consuming tine: 4.58\n",
      "batch: 583, batch train loss: 1.59, train acc: 35.39%, consuming tine: 4.61\n",
      "batch: 584, batch train loss: 1.59, train acc: 35.40%, consuming tine: 4.57\n",
      "batch: 585, batch train loss: 1.59, train acc: 35.41%, consuming tine: 4.90\n",
      "batch: 586, batch train loss: 1.59, train acc: 35.41%, consuming tine: 4.49\n",
      "batch: 587, batch train loss: 1.59, train acc: 35.41%, consuming tine: 4.68\n",
      "batch: 588, batch train loss: 1.59, train acc: 35.42%, consuming tine: 4.50\n",
      "batch: 589, batch train loss: 1.59, train acc: 35.43%, consuming tine: 4.59\n",
      "batch: 590, batch train loss: 1.59, train acc: 35.44%, consuming tine: 4.50\n",
      "batch: 591, batch train loss: 1.59, train acc: 35.44%, consuming tine: 4.69\n",
      "batch: 592, batch train loss: 1.59, train acc: 35.45%, consuming tine: 4.52\n",
      "batch: 593, batch train loss: 1.59, train acc: 35.46%, consuming tine: 4.58\n",
      "batch: 594, batch train loss: 1.59, train acc: 35.46%, consuming tine: 4.68\n",
      "batch: 595, batch train loss: 1.59, train acc: 35.47%, consuming tine: 4.39\n",
      "batch: 596, batch train loss: 1.59, train acc: 35.48%, consuming tine: 4.40\n",
      "batch: 597, batch train loss: 1.59, train acc: 35.48%, consuming tine: 4.59\n",
      "batch: 598, batch train loss: 1.59, train acc: 35.49%, consuming tine: 4.61\n",
      "batch: 599, batch train loss: 1.59, train acc: 35.50%, consuming tine: 4.59\n",
      "batch: 600, batch train loss: 1.59, train acc: 35.50%, consuming tine: 4.59\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 1.56, valid acc: 36.26%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 1.59, train acc: 35.51%, consuming tine: 4.44\n",
      "batch: 602, batch train loss: 1.59, train acc: 35.52%, consuming tine: 4.69\n",
      "batch: 603, batch train loss: 1.59, train acc: 35.52%, consuming tine: 4.79\n",
      "batch: 604, batch train loss: 1.59, train acc: 35.53%, consuming tine: 4.59\n",
      "batch: 605, batch train loss: 1.59, train acc: 35.53%, consuming tine: 5.29\n",
      "batch: 606, batch train loss: 1.59, train acc: 35.54%, consuming tine: 4.79\n",
      "batch: 607, batch train loss: 1.59, train acc: 35.55%, consuming tine: 4.39\n",
      "batch: 608, batch train loss: 1.59, train acc: 35.56%, consuming tine: 4.70\n",
      "batch: 609, batch train loss: 1.59, train acc: 35.56%, consuming tine: 4.58\n",
      "batch: 610, batch train loss: 1.59, train acc: 35.57%, consuming tine: 4.40\n",
      "batch: 611, batch train loss: 1.59, train acc: 35.58%, consuming tine: 4.51\n",
      "batch: 612, batch train loss: 1.59, train acc: 35.59%, consuming tine: 4.47\n",
      "batch: 613, batch train loss: 1.59, train acc: 35.61%, consuming tine: 4.89\n",
      "batch: 614, batch train loss: 1.59, train acc: 35.61%, consuming tine: 4.49\n",
      "batch: 615, batch train loss: 1.59, train acc: 35.61%, consuming tine: 4.70\n",
      "batch: 616, batch train loss: 1.59, train acc: 35.62%, consuming tine: 4.38\n",
      "batch: 617, batch train loss: 1.59, train acc: 35.63%, consuming tine: 4.89\n",
      "batch: 618, batch train loss: 1.59, train acc: 35.64%, consuming tine: 4.69\n",
      "batch: 619, batch train loss: 1.59, train acc: 35.65%, consuming tine: 4.49\n",
      "batch: 620, batch train loss: 1.59, train acc: 35.66%, consuming tine: 4.49\n",
      "batch: 621, batch train loss: 1.59, train acc: 35.67%, consuming tine: 4.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 622, batch train loss: 1.59, train acc: 35.68%, consuming tine: 4.48\n",
      "batch: 623, batch train loss: 1.58, train acc: 35.69%, consuming tine: 5.19\n",
      "batch: 624, batch train loss: 1.58, train acc: 35.70%, consuming tine: 4.49\n",
      "batch: 625, batch train loss: 1.58, train acc: 35.70%, consuming tine: 4.26\n",
      "batch: 626, batch train loss: 1.58, train acc: 35.71%, consuming tine: 5.22\n",
      "batch: 627, batch train loss: 1.58, train acc: 35.71%, consuming tine: 4.59\n",
      "batch: 628, batch train loss: 1.58, train acc: 35.71%, consuming tine: 4.68\n",
      "batch: 629, batch train loss: 1.58, train acc: 35.72%, consuming tine: 4.50\n",
      "batch: 630, batch train loss: 1.58, train acc: 35.72%, consuming tine: 4.60\n",
      "batch: 631, batch train loss: 1.58, train acc: 35.73%, consuming tine: 4.82\n",
      "batch: 632, batch train loss: 1.58, train acc: 35.74%, consuming tine: 4.46\n",
      "batch: 633, batch train loss: 1.58, train acc: 35.74%, consuming tine: 4.39\n",
      "batch: 634, batch train loss: 1.58, train acc: 35.75%, consuming tine: 4.61\n",
      "batch: 635, batch train loss: 1.58, train acc: 35.75%, consuming tine: 4.78\n",
      "batch: 636, batch train loss: 1.58, train acc: 35.76%, consuming tine: 4.40\n",
      "batch: 637, batch train loss: 1.58, train acc: 35.77%, consuming tine: 4.38\n",
      "batch: 638, batch train loss: 1.58, train acc: 35.77%, consuming tine: 4.62\n",
      "batch: 639, batch train loss: 1.58, train acc: 35.78%, consuming tine: 4.47\n",
      "batch: 640, batch train loss: 1.58, train acc: 35.79%, consuming tine: 4.58\n",
      "batch: 641, batch train loss: 1.58, train acc: 35.80%, consuming tine: 4.60\n",
      "batch: 642, batch train loss: 1.58, train acc: 35.80%, consuming tine: 4.69\n",
      "batch: 643, batch train loss: 1.58, train acc: 35.81%, consuming tine: 4.69\n",
      "batch: 644, batch train loss: 1.58, train acc: 35.82%, consuming tine: 4.70\n",
      "batch: 645, batch train loss: 1.58, train acc: 35.83%, consuming tine: 4.19\n",
      "batch: 646, batch train loss: 1.58, train acc: 35.84%, consuming tine: 4.20\n",
      "batch: 647, batch train loss: 1.58, train acc: 35.84%, consuming tine: 4.59\n",
      "batch: 648, batch train loss: 1.58, train acc: 35.85%, consuming tine: 4.59\n",
      "batch: 649, batch train loss: 1.58, train acc: 35.86%, consuming tine: 4.38\n",
      "batch: 650, batch train loss: 1.58, train acc: 35.86%, consuming tine: 5.01\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 1.55, valid acc: 36.59%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 1.58, train acc: 35.87%, consuming tine: 4.41\n",
      "batch: 652, batch train loss: 1.58, train acc: 35.88%, consuming tine: 4.59\n",
      "batch: 653, batch train loss: 1.58, train acc: 35.89%, consuming tine: 4.48\n",
      "batch: 654, batch train loss: 1.58, train acc: 35.89%, consuming tine: 4.80\n",
      "batch: 655, batch train loss: 1.58, train acc: 35.90%, consuming tine: 4.89\n",
      "batch: 656, batch train loss: 1.58, train acc: 35.91%, consuming tine: 4.59\n",
      "batch: 657, batch train loss: 1.58, train acc: 35.92%, consuming tine: 4.50\n",
      "batch: 658, batch train loss: 1.58, train acc: 35.93%, consuming tine: 4.69\n",
      "batch: 659, batch train loss: 1.58, train acc: 35.94%, consuming tine: 4.39\n",
      "batch: 660, batch train loss: 1.58, train acc: 35.94%, consuming tine: 4.59\n",
      "batch: 661, batch train loss: 1.58, train acc: 35.95%, consuming tine: 4.80\n",
      "batch: 662, batch train loss: 1.58, train acc: 35.95%, consuming tine: 4.38\n",
      "batch: 663, batch train loss: 1.58, train acc: 35.96%, consuming tine: 4.80\n",
      "batch: 664, batch train loss: 1.58, train acc: 35.97%, consuming tine: 4.68\n",
      "batch: 665, batch train loss: 1.58, train acc: 35.98%, consuming tine: 4.89\n",
      "batch: 666, batch train loss: 1.58, train acc: 35.99%, consuming tine: 5.00\n",
      "batch: 667, batch train loss: 1.58, train acc: 36.00%, consuming tine: 4.91\n",
      "batch: 668, batch train loss: 1.58, train acc: 36.00%, consuming tine: 4.77\n",
      "batch: 669, batch train loss: 1.58, train acc: 36.01%, consuming tine: 4.70\n",
      "batch: 670, batch train loss: 1.58, train acc: 36.02%, consuming tine: 4.69\n",
      "batch: 671, batch train loss: 1.58, train acc: 36.03%, consuming tine: 4.50\n",
      "batch: 672, batch train loss: 1.58, train acc: 36.04%, consuming tine: 4.50\n",
      "batch: 673, batch train loss: 1.58, train acc: 36.04%, consuming tine: 4.78\n",
      "batch: 674, batch train loss: 1.58, train acc: 36.05%, consuming tine: 4.71\n",
      "batch: 675, batch train loss: 1.57, train acc: 36.06%, consuming tine: 4.59\n",
      "batch: 676, batch train loss: 1.57, train acc: 36.07%, consuming tine: 4.58\n",
      "batch: 677, batch train loss: 1.57, train acc: 36.08%, consuming tine: 4.30\n",
      "batch: 678, batch train loss: 1.57, train acc: 36.09%, consuming tine: 4.71\n",
      "batch: 679, batch train loss: 1.57, train acc: 36.10%, consuming tine: 4.68\n",
      "batch: 680, batch train loss: 1.57, train acc: 36.10%, consuming tine: 4.79\n",
      "batch: 681, batch train loss: 1.57, train acc: 36.12%, consuming tine: 4.53\n",
      "batch: 682, batch train loss: 1.57, train acc: 36.12%, consuming tine: 4.67\n",
      "batch: 683, batch train loss: 1.57, train acc: 36.13%, consuming tine: 4.68\n",
      "batch: 684, batch train loss: 1.57, train acc: 36.14%, consuming tine: 4.80\n",
      "batch: 685, batch train loss: 1.57, train acc: 36.15%, consuming tine: 4.59\n",
      "batch: 686, batch train loss: 1.57, train acc: 36.15%, consuming tine: 4.49\n",
      "batch: 687, batch train loss: 1.57, train acc: 36.16%, consuming tine: 4.79\n",
      "batch: 688, batch train loss: 1.57, train acc: 36.16%, consuming tine: 4.60\n",
      "batch: 689, batch train loss: 1.57, train acc: 36.17%, consuming tine: 4.78\n",
      "batch: 690, batch train loss: 1.57, train acc: 36.18%, consuming tine: 5.31\n",
      "batch: 691, batch train loss: 1.57, train acc: 36.19%, consuming tine: 5.50\n",
      "batch: 692, batch train loss: 1.57, train acc: 36.19%, consuming tine: 5.38\n",
      "batch: 693, batch train loss: 1.57, train acc: 36.20%, consuming tine: 5.52\n",
      "batch: 694, batch train loss: 1.57, train acc: 36.21%, consuming tine: 5.28\n",
      "batch: 695, batch train loss: 1.57, train acc: 36.21%, consuming tine: 5.38\n",
      "batch: 696, batch train loss: 1.57, train acc: 36.22%, consuming tine: 5.60\n",
      "batch: 697, batch train loss: 1.57, train acc: 36.23%, consuming tine: 5.40\n",
      "batch: 698, batch train loss: 1.57, train acc: 36.24%, consuming tine: 5.87\n",
      "batch: 699, batch train loss: 1.57, train acc: 36.24%, consuming tine: 5.40\n",
      "batch: 700, batch train loss: 1.57, train acc: 36.25%, consuming tine: 5.39\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 1.54, valid acc: 36.87%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 1.57, train acc: 36.25%, consuming tine: 5.84\n",
      "batch: 702, batch train loss: 1.57, train acc: 36.26%, consuming tine: 5.77\n",
      "batch: 703, batch train loss: 1.57, train acc: 36.27%, consuming tine: 5.80\n",
      "batch: 704, batch train loss: 1.57, train acc: 36.27%, consuming tine: 13.38\n",
      "Epoch 1, Loss: 1.57, Accuracy: 36.27%, Valid Loss: 1.54, Valid Accuracy: 36.87%\n",
      "batch: 1, batch train loss: 1.46, train acc: 38.96%, consuming tine: 5.79\n",
      "batch: 2, batch train loss: 1.46, train acc: 40.53%, consuming tine: 5.61\n",
      "batch: 3, batch train loss: 1.45, train acc: 41.18%, consuming tine: 5.76\n",
      "batch: 4, batch train loss: 1.45, train acc: 40.87%, consuming tine: 5.70\n",
      "batch: 5, batch train loss: 1.44, train acc: 40.90%, consuming tine: 5.59\n",
      "batch: 6, batch train loss: 1.44, train acc: 40.82%, consuming tine: 5.47\n",
      "batch: 7, batch train loss: 1.43, train acc: 41.14%, consuming tine: 5.81\n",
      "batch: 8, batch train loss: 1.43, train acc: 41.33%, consuming tine: 5.46\n",
      "batch: 9, batch train loss: 1.43, train acc: 41.18%, consuming tine: 5.38\n",
      "batch: 10, batch train loss: 1.43, train acc: 41.35%, consuming tine: 5.39\n",
      "batch: 11, batch train loss: 1.43, train acc: 41.43%, consuming tine: 5.40\n",
      "batch: 12, batch train loss: 1.43, train acc: 41.39%, consuming tine: 8.07\n",
      "batch: 13, batch train loss: 1.42, train acc: 41.53%, consuming tine: 5.40\n",
      "batch: 14, batch train loss: 1.43, train acc: 41.48%, consuming tine: 5.49\n",
      "batch: 15, batch train loss: 1.43, train acc: 41.50%, consuming tine: 5.58\n",
      "batch: 16, batch train loss: 1.43, train acc: 41.50%, consuming tine: 5.23\n",
      "batch: 17, batch train loss: 1.43, train acc: 41.47%, consuming tine: 5.67\n",
      "batch: 18, batch train loss: 1.43, train acc: 41.54%, consuming tine: 5.67\n",
      "batch: 19, batch train loss: 1.42, train acc: 41.74%, consuming tine: 5.39\n",
      "batch: 20, batch train loss: 1.42, train acc: 41.73%, consuming tine: 5.78\n",
      "batch: 21, batch train loss: 1.43, train acc: 41.63%, consuming tine: 5.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 22, batch train loss: 1.42, train acc: 41.80%, consuming tine: 5.69\n",
      "batch: 23, batch train loss: 1.42, train acc: 41.74%, consuming tine: 5.58\n",
      "batch: 24, batch train loss: 1.42, train acc: 41.72%, consuming tine: 5.60\n",
      "batch: 25, batch train loss: 1.42, train acc: 41.66%, consuming tine: 5.78\n",
      "batch: 26, batch train loss: 1.42, train acc: 41.67%, consuming tine: 5.79\n",
      "batch: 27, batch train loss: 1.42, train acc: 41.67%, consuming tine: 5.61\n",
      "batch: 28, batch train loss: 1.42, train acc: 41.70%, consuming tine: 5.54\n",
      "batch: 29, batch train loss: 1.42, train acc: 41.66%, consuming tine: 5.74\n",
      "batch: 30, batch train loss: 1.42, train acc: 41.67%, consuming tine: 5.58\n",
      "batch: 31, batch train loss: 1.42, train acc: 41.68%, consuming tine: 5.57\n",
      "batch: 32, batch train loss: 1.42, train acc: 41.79%, consuming tine: 5.39\n",
      "batch: 33, batch train loss: 1.42, train acc: 41.85%, consuming tine: 5.49\n",
      "batch: 34, batch train loss: 1.42, train acc: 41.87%, consuming tine: 5.38\n",
      "batch: 35, batch train loss: 1.42, train acc: 41.93%, consuming tine: 5.58\n",
      "batch: 36, batch train loss: 1.42, train acc: 41.93%, consuming tine: 5.09\n",
      "batch: 37, batch train loss: 1.42, train acc: 41.91%, consuming tine: 5.59\n",
      "batch: 38, batch train loss: 1.42, train acc: 41.88%, consuming tine: 5.51\n",
      "batch: 39, batch train loss: 1.42, train acc: 41.92%, consuming tine: 5.37\n",
      "batch: 40, batch train loss: 1.42, train acc: 42.02%, consuming tine: 5.28\n",
      "batch: 41, batch train loss: 1.42, train acc: 42.09%, consuming tine: 5.60\n",
      "batch: 42, batch train loss: 1.41, train acc: 42.11%, consuming tine: 5.70\n",
      "batch: 43, batch train loss: 1.41, train acc: 42.10%, consuming tine: 5.37\n",
      "batch: 44, batch train loss: 1.41, train acc: 42.06%, consuming tine: 5.49\n",
      "batch: 45, batch train loss: 1.41, train acc: 42.06%, consuming tine: 5.67\n",
      "batch: 46, batch train loss: 1.41, train acc: 42.09%, consuming tine: 5.71\n",
      "batch: 47, batch train loss: 1.41, train acc: 42.11%, consuming tine: 5.77\n",
      "batch: 48, batch train loss: 1.41, train acc: 42.14%, consuming tine: 5.50\n",
      "batch: 49, batch train loss: 1.41, train acc: 42.19%, consuming tine: 5.80\n",
      "batch: 50, batch train loss: 1.41, train acc: 42.26%, consuming tine: 5.46\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 1.45, valid acc: 40.66%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 1.41, train acc: 42.28%, consuming tine: 5.71\n",
      "batch: 52, batch train loss: 1.41, train acc: 42.32%, consuming tine: 5.89\n",
      "batch: 53, batch train loss: 1.41, train acc: 42.32%, consuming tine: 5.70\n",
      "batch: 54, batch train loss: 1.40, train acc: 42.38%, consuming tine: 5.78\n",
      "batch: 55, batch train loss: 1.40, train acc: 42.38%, consuming tine: 5.48\n",
      "batch: 56, batch train loss: 1.40, train acc: 42.37%, consuming tine: 5.89\n",
      "batch: 57, batch train loss: 1.40, train acc: 42.42%, consuming tine: 5.69\n",
      "batch: 58, batch train loss: 1.40, train acc: 42.45%, consuming tine: 5.70\n",
      "batch: 59, batch train loss: 1.40, train acc: 42.50%, consuming tine: 5.77\n",
      "batch: 60, batch train loss: 1.40, train acc: 42.56%, consuming tine: 5.58\n",
      "batch: 61, batch train loss: 1.40, train acc: 42.64%, consuming tine: 5.61\n",
      "batch: 62, batch train loss: 1.40, train acc: 42.72%, consuming tine: 5.57\n",
      "batch: 63, batch train loss: 1.39, train acc: 42.77%, consuming tine: 5.68\n",
      "batch: 64, batch train loss: 1.39, train acc: 42.86%, consuming tine: 5.59\n",
      "batch: 65, batch train loss: 1.39, train acc: 42.88%, consuming tine: 5.70\n",
      "batch: 66, batch train loss: 1.39, train acc: 42.94%, consuming tine: 5.78\n",
      "batch: 67, batch train loss: 1.39, train acc: 43.00%, consuming tine: 5.70\n",
      "batch: 68, batch train loss: 1.39, train acc: 43.07%, consuming tine: 5.78\n",
      "batch: 69, batch train loss: 1.39, train acc: 43.10%, consuming tine: 5.63\n",
      "batch: 70, batch train loss: 1.39, train acc: 43.19%, consuming tine: 5.64\n",
      "batch: 71, batch train loss: 1.38, train acc: 43.28%, consuming tine: 6.00\n",
      "batch: 72, batch train loss: 1.38, train acc: 43.33%, consuming tine: 5.79\n",
      "batch: 73, batch train loss: 1.38, train acc: 43.40%, consuming tine: 5.59\n",
      "batch: 74, batch train loss: 1.38, train acc: 43.45%, consuming tine: 5.79\n",
      "batch: 75, batch train loss: 1.38, train acc: 43.51%, consuming tine: 5.66\n",
      "batch: 76, batch train loss: 1.38, train acc: 43.51%, consuming tine: 5.80\n",
      "batch: 77, batch train loss: 1.37, train acc: 43.58%, consuming tine: 5.59\n",
      "batch: 78, batch train loss: 1.37, train acc: 43.61%, consuming tine: 5.58\n",
      "batch: 79, batch train loss: 1.37, train acc: 43.65%, consuming tine: 5.60\n",
      "batch: 80, batch train loss: 1.37, train acc: 43.66%, consuming tine: 5.49\n",
      "batch: 81, batch train loss: 1.37, train acc: 43.71%, consuming tine: 5.98\n",
      "batch: 82, batch train loss: 1.37, train acc: 43.76%, consuming tine: 5.59\n",
      "batch: 83, batch train loss: 1.37, train acc: 43.79%, consuming tine: 5.59\n",
      "batch: 84, batch train loss: 1.37, train acc: 43.85%, consuming tine: 5.78\n",
      "batch: 85, batch train loss: 1.37, train acc: 43.89%, consuming tine: 5.49\n",
      "batch: 86, batch train loss: 1.36, train acc: 43.93%, consuming tine: 5.80\n",
      "batch: 87, batch train loss: 1.36, train acc: 43.95%, consuming tine: 5.58\n",
      "batch: 88, batch train loss: 1.36, train acc: 44.00%, consuming tine: 5.48\n",
      "batch: 89, batch train loss: 1.36, train acc: 44.04%, consuming tine: 5.69\n",
      "batch: 90, batch train loss: 1.36, train acc: 44.09%, consuming tine: 5.49\n",
      "batch: 91, batch train loss: 1.36, train acc: 44.14%, consuming tine: 5.69\n",
      "batch: 92, batch train loss: 1.36, train acc: 44.20%, consuming tine: 5.58\n",
      "batch: 93, batch train loss: 1.36, train acc: 44.27%, consuming tine: 5.52\n",
      "batch: 94, batch train loss: 1.35, train acc: 44.33%, consuming tine: 5.54\n",
      "batch: 95, batch train loss: 1.35, train acc: 44.40%, consuming tine: 5.80\n",
      "batch: 96, batch train loss: 1.35, train acc: 44.46%, consuming tine: 5.87\n",
      "batch: 97, batch train loss: 1.35, train acc: 44.49%, consuming tine: 5.49\n",
      "batch: 98, batch train loss: 1.35, train acc: 44.54%, consuming tine: 5.59\n",
      "batch: 99, batch train loss: 1.35, train acc: 44.59%, consuming tine: 5.58\n",
      "batch: 100, batch train loss: 1.35, train acc: 44.65%, consuming tine: 5.60\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 1.48, valid acc: 40.22%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 1.35, train acc: 44.69%, consuming tine: 5.87\n",
      "batch: 102, batch train loss: 1.35, train acc: 44.73%, consuming tine: 5.88\n",
      "batch: 103, batch train loss: 1.34, train acc: 44.77%, consuming tine: 5.38\n",
      "batch: 104, batch train loss: 1.34, train acc: 44.80%, consuming tine: 5.73\n",
      "batch: 105, batch train loss: 1.34, train acc: 44.88%, consuming tine: 5.55\n",
      "batch: 106, batch train loss: 1.34, train acc: 44.91%, consuming tine: 5.74\n",
      "batch: 107, batch train loss: 1.34, train acc: 44.94%, consuming tine: 5.75\n",
      "batch: 108, batch train loss: 1.34, train acc: 44.98%, consuming tine: 5.77\n",
      "batch: 109, batch train loss: 1.34, train acc: 45.03%, consuming tine: 5.78\n",
      "batch: 110, batch train loss: 1.34, train acc: 45.08%, consuming tine: 5.78\n",
      "batch: 111, batch train loss: 1.34, train acc: 45.14%, consuming tine: 5.59\n",
      "batch: 112, batch train loss: 1.33, train acc: 45.20%, consuming tine: 5.59\n",
      "batch: 113, batch train loss: 1.33, train acc: 45.24%, consuming tine: 5.79\n",
      "batch: 114, batch train loss: 1.33, train acc: 45.25%, consuming tine: 5.51\n",
      "batch: 115, batch train loss: 1.33, train acc: 45.30%, consuming tine: 5.58\n",
      "batch: 116, batch train loss: 1.33, train acc: 45.36%, consuming tine: 5.65\n",
      "batch: 117, batch train loss: 1.33, train acc: 45.40%, consuming tine: 5.49\n",
      "batch: 118, batch train loss: 1.33, train acc: 45.45%, consuming tine: 5.79\n",
      "batch: 119, batch train loss: 1.33, train acc: 45.50%, consuming tine: 5.48\n",
      "batch: 120, batch train loss: 1.33, train acc: 45.55%, consuming tine: 5.59\n",
      "batch: 121, batch train loss: 1.32, train acc: 45.62%, consuming tine: 5.87\n",
      "batch: 122, batch train loss: 1.32, train acc: 45.66%, consuming tine: 5.30\n",
      "batch: 123, batch train loss: 1.32, train acc: 45.70%, consuming tine: 5.78\n",
      "batch: 124, batch train loss: 1.32, train acc: 45.75%, consuming tine: 5.69\n",
      "batch: 125, batch train loss: 1.32, train acc: 45.79%, consuming tine: 5.69\n",
      "batch: 126, batch train loss: 1.32, train acc: 45.84%, consuming tine: 5.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 127, batch train loss: 1.32, train acc: 45.90%, consuming tine: 5.50\n",
      "batch: 128, batch train loss: 1.32, train acc: 45.94%, consuming tine: 5.89\n",
      "batch: 129, batch train loss: 1.32, train acc: 45.97%, consuming tine: 5.68\n",
      "batch: 130, batch train loss: 1.32, train acc: 46.02%, consuming tine: 5.79\n",
      "batch: 131, batch train loss: 1.32, train acc: 46.07%, consuming tine: 5.68\n",
      "batch: 132, batch train loss: 1.31, train acc: 46.12%, consuming tine: 5.77\n",
      "batch: 133, batch train loss: 1.31, train acc: 46.16%, consuming tine: 6.09\n",
      "batch: 134, batch train loss: 1.31, train acc: 46.20%, consuming tine: 5.59\n",
      "batch: 135, batch train loss: 1.31, train acc: 46.25%, consuming tine: 5.68\n",
      "batch: 136, batch train loss: 1.31, train acc: 46.29%, consuming tine: 5.60\n",
      "batch: 137, batch train loss: 1.31, train acc: 46.33%, consuming tine: 5.78\n",
      "batch: 138, batch train loss: 1.31, train acc: 46.38%, consuming tine: 5.72\n",
      "batch: 139, batch train loss: 1.31, train acc: 46.44%, consuming tine: 5.77\n",
      "batch: 140, batch train loss: 1.31, train acc: 46.47%, consuming tine: 5.49\n",
      "batch: 141, batch train loss: 1.31, train acc: 46.52%, consuming tine: 5.58\n",
      "batch: 142, batch train loss: 1.30, train acc: 46.57%, consuming tine: 5.80\n",
      "batch: 143, batch train loss: 1.30, train acc: 46.60%, consuming tine: 5.77\n",
      "batch: 144, batch train loss: 1.30, train acc: 46.64%, consuming tine: 5.39\n",
      "batch: 145, batch train loss: 1.30, train acc: 46.69%, consuming tine: 5.79\n",
      "batch: 146, batch train loss: 1.30, train acc: 46.72%, consuming tine: 5.71\n",
      "batch: 147, batch train loss: 1.30, train acc: 46.75%, consuming tine: 5.58\n",
      "batch: 148, batch train loss: 1.30, train acc: 46.80%, consuming tine: 5.39\n",
      "batch: 149, batch train loss: 1.30, train acc: 46.84%, consuming tine: 5.18\n",
      "batch: 150, batch train loss: 1.30, train acc: 46.87%, consuming tine: 5.59\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 1.51, valid acc: 39.98%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 1.29, train acc: 46.91%, consuming tine: 5.21\n",
      "batch: 152, batch train loss: 1.29, train acc: 46.95%, consuming tine: 5.06\n",
      "batch: 153, batch train loss: 1.29, train acc: 46.99%, consuming tine: 5.38\n",
      "batch: 154, batch train loss: 1.29, train acc: 47.03%, consuming tine: 5.01\n",
      "batch: 155, batch train loss: 1.29, train acc: 47.09%, consuming tine: 4.97\n",
      "batch: 156, batch train loss: 1.29, train acc: 47.14%, consuming tine: 5.09\n",
      "batch: 157, batch train loss: 1.29, train acc: 47.18%, consuming tine: 4.78\n",
      "batch: 158, batch train loss: 1.29, train acc: 47.24%, consuming tine: 4.79\n",
      "batch: 159, batch train loss: 1.29, train acc: 47.27%, consuming tine: 4.87\n",
      "batch: 160, batch train loss: 1.29, train acc: 47.32%, consuming tine: 4.60\n",
      "batch: 161, batch train loss: 1.29, train acc: 47.37%, consuming tine: 4.50\n",
      "batch: 162, batch train loss: 1.28, train acc: 47.43%, consuming tine: 4.99\n",
      "batch: 163, batch train loss: 1.28, train acc: 47.46%, consuming tine: 4.27\n",
      "batch: 164, batch train loss: 1.28, train acc: 47.52%, consuming tine: 4.56\n",
      "batch: 165, batch train loss: 1.28, train acc: 47.57%, consuming tine: 5.19\n",
      "batch: 166, batch train loss: 1.28, train acc: 47.61%, consuming tine: 4.47\n",
      "batch: 167, batch train loss: 1.28, train acc: 47.64%, consuming tine: 4.48\n",
      "batch: 168, batch train loss: 1.28, train acc: 47.68%, consuming tine: 4.79\n",
      "batch: 169, batch train loss: 1.28, train acc: 47.72%, consuming tine: 4.40\n",
      "batch: 170, batch train loss: 1.28, train acc: 47.75%, consuming tine: 4.58\n",
      "batch: 171, batch train loss: 1.28, train acc: 47.81%, consuming tine: 4.90\n",
      "batch: 172, batch train loss: 1.27, train acc: 47.85%, consuming tine: 4.37\n",
      "batch: 173, batch train loss: 1.27, train acc: 47.89%, consuming tine: 4.40\n",
      "batch: 174, batch train loss: 1.27, train acc: 47.93%, consuming tine: 4.49\n",
      "batch: 175, batch train loss: 1.27, train acc: 47.97%, consuming tine: 4.59\n",
      "batch: 176, batch train loss: 1.27, train acc: 48.02%, consuming tine: 4.38\n",
      "batch: 177, batch train loss: 1.27, train acc: 48.05%, consuming tine: 4.71\n",
      "batch: 178, batch train loss: 1.27, train acc: 48.10%, consuming tine: 4.66\n",
      "batch: 179, batch train loss: 1.27, train acc: 48.14%, consuming tine: 4.60\n",
      "batch: 180, batch train loss: 1.27, train acc: 48.20%, consuming tine: 4.49\n",
      "batch: 181, batch train loss: 1.27, train acc: 48.24%, consuming tine: 4.57\n",
      "batch: 182, batch train loss: 1.27, train acc: 48.29%, consuming tine: 4.39\n",
      "batch: 183, batch train loss: 1.26, train acc: 48.34%, consuming tine: 4.39\n",
      "batch: 184, batch train loss: 1.26, train acc: 48.40%, consuming tine: 4.69\n",
      "batch: 185, batch train loss: 1.26, train acc: 48.44%, consuming tine: 4.58\n",
      "batch: 186, batch train loss: 1.26, train acc: 48.48%, consuming tine: 4.70\n",
      "batch: 187, batch train loss: 1.26, train acc: 48.52%, consuming tine: 4.48\n",
      "batch: 188, batch train loss: 1.26, train acc: 48.58%, consuming tine: 4.59\n",
      "batch: 189, batch train loss: 1.26, train acc: 48.60%, consuming tine: 5.00\n",
      "batch: 190, batch train loss: 1.26, train acc: 48.63%, consuming tine: 4.58\n",
      "batch: 191, batch train loss: 1.26, train acc: 48.66%, consuming tine: 4.49\n",
      "batch: 192, batch train loss: 1.26, train acc: 48.69%, consuming tine: 4.78\n",
      "batch: 193, batch train loss: 1.26, train acc: 48.73%, consuming tine: 4.38\n",
      "batch: 194, batch train loss: 1.26, train acc: 48.76%, consuming tine: 4.49\n",
      "batch: 195, batch train loss: 1.25, train acc: 48.79%, consuming tine: 4.60\n",
      "batch: 196, batch train loss: 1.25, train acc: 48.82%, consuming tine: 4.50\n",
      "batch: 197, batch train loss: 1.25, train acc: 48.86%, consuming tine: 4.46\n",
      "batch: 198, batch train loss: 1.25, train acc: 48.89%, consuming tine: 4.40\n",
      "batch: 199, batch train loss: 1.25, train acc: 48.91%, consuming tine: 4.39\n",
      "batch: 200, batch train loss: 1.25, train acc: 48.94%, consuming tine: 4.58\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 1.53, valid acc: 39.64%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 1.25, train acc: 48.98%, consuming tine: 4.46\n",
      "batch: 202, batch train loss: 1.25, train acc: 49.01%, consuming tine: 4.68\n",
      "batch: 203, batch train loss: 1.25, train acc: 49.03%, consuming tine: 4.41\n",
      "batch: 204, batch train loss: 1.25, train acc: 49.08%, consuming tine: 4.48\n",
      "batch: 205, batch train loss: 1.25, train acc: 49.11%, consuming tine: 4.54\n",
      "batch: 206, batch train loss: 1.25, train acc: 49.14%, consuming tine: 4.52\n",
      "batch: 207, batch train loss: 1.25, train acc: 49.16%, consuming tine: 4.29\n",
      "batch: 208, batch train loss: 1.25, train acc: 49.19%, consuming tine: 4.70\n",
      "batch: 209, batch train loss: 1.25, train acc: 49.21%, consuming tine: 4.38\n",
      "batch: 210, batch train loss: 1.24, train acc: 49.24%, consuming tine: 4.48\n",
      "batch: 211, batch train loss: 1.24, train acc: 49.27%, consuming tine: 4.75\n",
      "batch: 212, batch train loss: 1.24, train acc: 49.29%, consuming tine: 4.53\n",
      "batch: 213, batch train loss: 1.24, train acc: 49.33%, consuming tine: 4.48\n",
      "batch: 214, batch train loss: 1.24, train acc: 49.36%, consuming tine: 4.73\n",
      "batch: 215, batch train loss: 1.24, train acc: 49.40%, consuming tine: 4.54\n",
      "batch: 216, batch train loss: 1.24, train acc: 49.44%, consuming tine: 4.69\n",
      "batch: 217, batch train loss: 1.24, train acc: 49.45%, consuming tine: 4.88\n",
      "batch: 218, batch train loss: 1.24, train acc: 49.47%, consuming tine: 4.49\n",
      "batch: 219, batch train loss: 1.24, train acc: 49.49%, consuming tine: 4.59\n",
      "batch: 220, batch train loss: 1.24, train acc: 49.52%, consuming tine: 4.69\n",
      "batch: 221, batch train loss: 1.24, train acc: 49.55%, consuming tine: 4.49\n",
      "batch: 222, batch train loss: 1.24, train acc: 49.57%, consuming tine: 4.48\n",
      "batch: 223, batch train loss: 1.24, train acc: 49.58%, consuming tine: 4.70\n",
      "batch: 224, batch train loss: 1.24, train acc: 49.59%, consuming tine: 4.48\n",
      "batch: 225, batch train loss: 1.24, train acc: 49.62%, consuming tine: 4.29\n",
      "batch: 226, batch train loss: 1.24, train acc: 49.65%, consuming tine: 4.78\n",
      "batch: 227, batch train loss: 1.24, train acc: 49.65%, consuming tine: 4.50\n",
      "batch: 228, batch train loss: 1.23, train acc: 49.66%, consuming tine: 4.39\n",
      "batch: 229, batch train loss: 1.23, train acc: 49.68%, consuming tine: 4.68\n",
      "batch: 230, batch train loss: 1.23, train acc: 49.70%, consuming tine: 4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 231, batch train loss: 1.23, train acc: 49.71%, consuming tine: 4.50\n",
      "batch: 232, batch train loss: 1.23, train acc: 49.73%, consuming tine: 4.58\n",
      "batch: 233, batch train loss: 1.23, train acc: 49.75%, consuming tine: 4.39\n",
      "batch: 234, batch train loss: 1.23, train acc: 49.76%, consuming tine: 4.39\n",
      "batch: 235, batch train loss: 1.23, train acc: 49.78%, consuming tine: 4.50\n",
      "batch: 236, batch train loss: 1.23, train acc: 49.81%, consuming tine: 4.47\n",
      "batch: 237, batch train loss: 1.23, train acc: 49.85%, consuming tine: 4.40\n",
      "batch: 238, batch train loss: 1.23, train acc: 49.87%, consuming tine: 4.79\n",
      "batch: 239, batch train loss: 1.23, train acc: 49.90%, consuming tine: 4.43\n",
      "batch: 240, batch train loss: 1.23, train acc: 49.93%, consuming tine: 4.54\n",
      "batch: 241, batch train loss: 1.23, train acc: 49.96%, consuming tine: 4.51\n",
      "batch: 242, batch train loss: 1.23, train acc: 49.99%, consuming tine: 4.36\n",
      "batch: 243, batch train loss: 1.23, train acc: 50.02%, consuming tine: 4.60\n",
      "batch: 244, batch train loss: 1.23, train acc: 50.04%, consuming tine: 4.58\n",
      "batch: 245, batch train loss: 1.23, train acc: 50.06%, consuming tine: 4.38\n",
      "batch: 246, batch train loss: 1.23, train acc: 50.08%, consuming tine: 4.50\n",
      "batch: 247, batch train loss: 1.22, train acc: 50.10%, consuming tine: 4.78\n",
      "batch: 248, batch train loss: 1.22, train acc: 50.13%, consuming tine: 4.28\n",
      "batch: 249, batch train loss: 1.22, train acc: 50.15%, consuming tine: 4.49\n",
      "batch: 250, batch train loss: 1.22, train acc: 50.19%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 1.55, valid acc: 39.53%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 1.22, train acc: 50.21%, consuming tine: 4.61\n",
      "batch: 252, batch train loss: 1.22, train acc: 50.22%, consuming tine: 4.66\n",
      "batch: 253, batch train loss: 1.22, train acc: 50.24%, consuming tine: 4.48\n",
      "batch: 254, batch train loss: 1.22, train acc: 50.26%, consuming tine: 4.50\n",
      "batch: 255, batch train loss: 1.22, train acc: 50.28%, consuming tine: 4.79\n",
      "batch: 256, batch train loss: 1.22, train acc: 50.31%, consuming tine: 4.62\n",
      "batch: 257, batch train loss: 1.22, train acc: 50.34%, consuming tine: 4.64\n",
      "batch: 258, batch train loss: 1.22, train acc: 50.37%, consuming tine: 4.59\n",
      "batch: 259, batch train loss: 1.22, train acc: 50.40%, consuming tine: 4.48\n",
      "batch: 260, batch train loss: 1.22, train acc: 50.43%, consuming tine: 4.49\n",
      "batch: 261, batch train loss: 1.22, train acc: 50.45%, consuming tine: 4.82\n",
      "batch: 262, batch train loss: 1.22, train acc: 50.46%, consuming tine: 4.47\n",
      "batch: 263, batch train loss: 1.22, train acc: 50.46%, consuming tine: 4.67\n",
      "batch: 264, batch train loss: 1.22, train acc: 50.47%, consuming tine: 4.68\n",
      "batch: 265, batch train loss: 1.22, train acc: 50.49%, consuming tine: 4.59\n",
      "batch: 266, batch train loss: 1.22, train acc: 50.51%, consuming tine: 4.62\n",
      "batch: 267, batch train loss: 1.22, train acc: 50.51%, consuming tine: 4.58\n",
      "batch: 268, batch train loss: 1.21, train acc: 50.53%, consuming tine: 4.28\n",
      "batch: 269, batch train loss: 1.21, train acc: 50.56%, consuming tine: 4.50\n",
      "batch: 270, batch train loss: 1.21, train acc: 50.57%, consuming tine: 4.68\n",
      "batch: 271, batch train loss: 1.21, train acc: 50.59%, consuming tine: 4.51\n",
      "batch: 272, batch train loss: 1.21, train acc: 50.61%, consuming tine: 4.37\n",
      "batch: 273, batch train loss: 1.21, train acc: 50.62%, consuming tine: 4.70\n",
      "batch: 274, batch train loss: 1.21, train acc: 50.65%, consuming tine: 4.88\n",
      "batch: 275, batch train loss: 1.21, train acc: 50.67%, consuming tine: 4.52\n",
      "batch: 276, batch train loss: 1.21, train acc: 50.70%, consuming tine: 4.66\n",
      "batch: 277, batch train loss: 1.21, train acc: 50.71%, consuming tine: 4.52\n",
      "batch: 278, batch train loss: 1.21, train acc: 50.73%, consuming tine: 4.47\n",
      "batch: 279, batch train loss: 1.21, train acc: 50.75%, consuming tine: 4.57\n",
      "batch: 280, batch train loss: 1.21, train acc: 50.78%, consuming tine: 4.63\n",
      "batch: 281, batch train loss: 1.21, train acc: 50.81%, consuming tine: 4.58\n",
      "batch: 282, batch train loss: 1.21, train acc: 50.83%, consuming tine: 4.46\n",
      "batch: 283, batch train loss: 1.21, train acc: 50.85%, consuming tine: 4.82\n",
      "batch: 284, batch train loss: 1.21, train acc: 50.88%, consuming tine: 4.64\n",
      "batch: 285, batch train loss: 1.21, train acc: 50.90%, consuming tine: 4.60\n",
      "batch: 286, batch train loss: 1.21, train acc: 50.93%, consuming tine: 4.67\n",
      "batch: 287, batch train loss: 1.21, train acc: 50.95%, consuming tine: 4.50\n",
      "batch: 288, batch train loss: 1.20, train acc: 50.97%, consuming tine: 4.50\n",
      "batch: 289, batch train loss: 1.20, train acc: 50.99%, consuming tine: 4.55\n",
      "batch: 290, batch train loss: 1.20, train acc: 51.01%, consuming tine: 4.41\n",
      "batch: 291, batch train loss: 1.20, train acc: 51.02%, consuming tine: 4.40\n",
      "batch: 292, batch train loss: 1.20, train acc: 51.05%, consuming tine: 4.79\n",
      "batch: 293, batch train loss: 1.20, train acc: 51.07%, consuming tine: 4.59\n",
      "batch: 294, batch train loss: 1.20, train acc: 51.09%, consuming tine: 4.59\n",
      "batch: 295, batch train loss: 1.20, train acc: 51.11%, consuming tine: 4.92\n",
      "batch: 296, batch train loss: 1.20, train acc: 51.13%, consuming tine: 4.76\n",
      "batch: 297, batch train loss: 1.20, train acc: 51.15%, consuming tine: 4.49\n",
      "batch: 298, batch train loss: 1.20, train acc: 51.18%, consuming tine: 4.99\n",
      "batch: 299, batch train loss: 1.20, train acc: 51.20%, consuming tine: 4.47\n",
      "batch: 300, batch train loss: 1.20, train acc: 51.22%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 1.57, valid acc: 39.44%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 1.20, train acc: 51.24%, consuming tine: 4.73\n",
      "batch: 302, batch train loss: 1.20, train acc: 51.26%, consuming tine: 4.58\n",
      "batch: 303, batch train loss: 1.20, train acc: 51.27%, consuming tine: 4.29\n",
      "batch: 304, batch train loss: 1.20, train acc: 51.28%, consuming tine: 4.60\n",
      "batch: 305, batch train loss: 1.20, train acc: 51.29%, consuming tine: 4.58\n",
      "batch: 306, batch train loss: 1.20, train acc: 51.31%, consuming tine: 4.49\n",
      "batch: 307, batch train loss: 1.20, train acc: 51.33%, consuming tine: 4.54\n",
      "batch: 308, batch train loss: 1.20, train acc: 51.35%, consuming tine: 4.54\n",
      "batch: 309, batch train loss: 1.20, train acc: 51.35%, consuming tine: 4.49\n",
      "batch: 310, batch train loss: 1.20, train acc: 51.37%, consuming tine: 4.39\n",
      "batch: 311, batch train loss: 1.20, train acc: 51.38%, consuming tine: 4.49\n",
      "batch: 312, batch train loss: 1.20, train acc: 51.40%, consuming tine: 4.55\n",
      "batch: 313, batch train loss: 1.20, train acc: 51.41%, consuming tine: 4.52\n",
      "batch: 314, batch train loss: 1.19, train acc: 51.44%, consuming tine: 4.69\n",
      "batch: 315, batch train loss: 1.19, train acc: 51.45%, consuming tine: 4.92\n",
      "batch: 316, batch train loss: 1.19, train acc: 51.47%, consuming tine: 4.65\n",
      "batch: 317, batch train loss: 1.19, train acc: 51.50%, consuming tine: 4.78\n",
      "batch: 318, batch train loss: 1.19, train acc: 51.51%, consuming tine: 4.52\n",
      "batch: 319, batch train loss: 1.19, train acc: 51.53%, consuming tine: 4.77\n",
      "batch: 320, batch train loss: 1.19, train acc: 51.56%, consuming tine: 4.60\n",
      "batch: 321, batch train loss: 1.19, train acc: 51.58%, consuming tine: 4.47\n",
      "batch: 322, batch train loss: 1.19, train acc: 51.61%, consuming tine: 4.66\n",
      "batch: 323, batch train loss: 1.19, train acc: 51.63%, consuming tine: 4.51\n",
      "batch: 324, batch train loss: 1.19, train acc: 51.65%, consuming tine: 4.39\n",
      "batch: 325, batch train loss: 1.19, train acc: 51.67%, consuming tine: 4.50\n",
      "batch: 326, batch train loss: 1.19, train acc: 51.69%, consuming tine: 4.57\n",
      "batch: 327, batch train loss: 1.19, train acc: 51.72%, consuming tine: 4.81\n",
      "batch: 328, batch train loss: 1.19, train acc: 51.75%, consuming tine: 4.96\n",
      "batch: 329, batch train loss: 1.19, train acc: 51.76%, consuming tine: 4.59\n",
      "batch: 330, batch train loss: 1.19, train acc: 51.78%, consuming tine: 4.92\n",
      "batch: 331, batch train loss: 1.19, train acc: 51.80%, consuming tine: 4.97\n",
      "batch: 332, batch train loss: 1.19, train acc: 51.82%, consuming tine: 4.60\n",
      "batch: 333, batch train loss: 1.19, train acc: 51.83%, consuming tine: 5.17\n",
      "batch: 334, batch train loss: 1.19, train acc: 51.85%, consuming tine: 5.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 335, batch train loss: 1.19, train acc: 51.87%, consuming tine: 4.98\n",
      "batch: 336, batch train loss: 1.19, train acc: 51.89%, consuming tine: 4.89\n",
      "batch: 337, batch train loss: 1.18, train acc: 51.91%, consuming tine: 4.99\n",
      "batch: 338, batch train loss: 1.18, train acc: 51.92%, consuming tine: 4.88\n",
      "batch: 339, batch train loss: 1.18, train acc: 51.95%, consuming tine: 4.48\n",
      "batch: 340, batch train loss: 1.18, train acc: 51.97%, consuming tine: 4.87\n",
      "batch: 341, batch train loss: 1.18, train acc: 51.98%, consuming tine: 5.01\n",
      "batch: 342, batch train loss: 1.18, train acc: 52.00%, consuming tine: 4.40\n",
      "batch: 343, batch train loss: 1.18, train acc: 52.02%, consuming tine: 4.81\n",
      "batch: 344, batch train loss: 1.18, train acc: 52.05%, consuming tine: 4.75\n",
      "batch: 345, batch train loss: 1.18, train acc: 52.07%, consuming tine: 4.80\n",
      "batch: 346, batch train loss: 1.18, train acc: 52.09%, consuming tine: 4.65\n",
      "batch: 347, batch train loss: 1.18, train acc: 52.11%, consuming tine: 4.60\n",
      "batch: 348, batch train loss: 1.18, train acc: 52.13%, consuming tine: 4.60\n",
      "batch: 349, batch train loss: 1.18, train acc: 52.15%, consuming tine: 4.77\n",
      "batch: 350, batch train loss: 1.18, train acc: 52.17%, consuming tine: 4.59\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 1.58, valid acc: 39.36%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 1.18, train acc: 52.20%, consuming tine: 4.37\n",
      "batch: 352, batch train loss: 1.18, train acc: 52.22%, consuming tine: 4.69\n",
      "batch: 353, batch train loss: 1.18, train acc: 52.24%, consuming tine: 4.59\n",
      "batch: 354, batch train loss: 1.18, train acc: 52.26%, consuming tine: 4.78\n",
      "batch: 355, batch train loss: 1.18, train acc: 52.28%, consuming tine: 4.78\n",
      "batch: 356, batch train loss: 1.18, train acc: 52.29%, consuming tine: 5.18\n",
      "batch: 357, batch train loss: 1.18, train acc: 52.31%, consuming tine: 4.81\n",
      "batch: 358, batch train loss: 1.18, train acc: 52.32%, consuming tine: 5.19\n",
      "batch: 359, batch train loss: 1.18, train acc: 52.34%, consuming tine: 4.87\n",
      "batch: 360, batch train loss: 1.18, train acc: 52.36%, consuming tine: 4.70\n",
      "batch: 361, batch train loss: 1.17, train acc: 52.38%, consuming tine: 4.87\n",
      "batch: 362, batch train loss: 1.17, train acc: 52.41%, consuming tine: 4.60\n",
      "batch: 363, batch train loss: 1.17, train acc: 52.42%, consuming tine: 4.70\n",
      "batch: 364, batch train loss: 1.17, train acc: 52.44%, consuming tine: 5.17\n",
      "batch: 365, batch train loss: 1.17, train acc: 52.45%, consuming tine: 4.81\n",
      "batch: 366, batch train loss: 1.17, train acc: 52.47%, consuming tine: 4.47\n",
      "batch: 367, batch train loss: 1.17, train acc: 52.49%, consuming tine: 4.70\n",
      "batch: 368, batch train loss: 1.17, train acc: 52.50%, consuming tine: 4.81\n",
      "batch: 369, batch train loss: 1.17, train acc: 52.52%, consuming tine: 4.56\n",
      "batch: 370, batch train loss: 1.17, train acc: 52.54%, consuming tine: 4.98\n",
      "batch: 371, batch train loss: 1.17, train acc: 52.56%, consuming tine: 4.49\n",
      "batch: 372, batch train loss: 1.17, train acc: 52.57%, consuming tine: 4.80\n",
      "batch: 373, batch train loss: 1.17, train acc: 52.60%, consuming tine: 4.49\n",
      "batch: 374, batch train loss: 1.17, train acc: 52.61%, consuming tine: 4.87\n",
      "batch: 375, batch train loss: 1.17, train acc: 52.63%, consuming tine: 4.70\n",
      "batch: 376, batch train loss: 1.17, train acc: 52.65%, consuming tine: 4.88\n",
      "batch: 377, batch train loss: 1.17, train acc: 52.67%, consuming tine: 4.70\n",
      "batch: 378, batch train loss: 1.17, train acc: 52.69%, consuming tine: 4.49\n",
      "batch: 379, batch train loss: 1.17, train acc: 52.71%, consuming tine: 4.77\n",
      "batch: 380, batch train loss: 1.17, train acc: 52.72%, consuming tine: 4.59\n",
      "batch: 381, batch train loss: 1.17, train acc: 52.74%, consuming tine: 4.89\n",
      "batch: 382, batch train loss: 1.17, train acc: 52.76%, consuming tine: 4.78\n",
      "batch: 383, batch train loss: 1.17, train acc: 52.78%, consuming tine: 4.60\n",
      "batch: 384, batch train loss: 1.17, train acc: 52.79%, consuming tine: 4.68\n",
      "batch: 385, batch train loss: 1.17, train acc: 52.80%, consuming tine: 4.79\n",
      "batch: 386, batch train loss: 1.17, train acc: 52.81%, consuming tine: 4.58\n",
      "batch: 387, batch train loss: 1.17, train acc: 52.82%, consuming tine: 4.50\n",
      "batch: 388, batch train loss: 1.17, train acc: 52.84%, consuming tine: 4.50\n",
      "batch: 389, batch train loss: 1.16, train acc: 52.86%, consuming tine: 4.37\n",
      "batch: 390, batch train loss: 1.16, train acc: 52.87%, consuming tine: 4.90\n",
      "batch: 391, batch train loss: 1.16, train acc: 52.89%, consuming tine: 5.07\n",
      "batch: 392, batch train loss: 1.16, train acc: 52.90%, consuming tine: 4.49\n",
      "batch: 393, batch train loss: 1.16, train acc: 52.92%, consuming tine: 4.59\n",
      "batch: 394, batch train loss: 1.16, train acc: 52.94%, consuming tine: 4.72\n",
      "batch: 395, batch train loss: 1.16, train acc: 52.95%, consuming tine: 4.47\n",
      "batch: 396, batch train loss: 1.16, train acc: 52.96%, consuming tine: 4.49\n",
      "batch: 397, batch train loss: 1.16, train acc: 52.99%, consuming tine: 4.58\n",
      "batch: 398, batch train loss: 1.16, train acc: 53.00%, consuming tine: 4.79\n",
      "batch: 399, batch train loss: 1.16, train acc: 53.03%, consuming tine: 4.39\n",
      "batch: 400, batch train loss: 1.16, train acc: 53.05%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 1.59, valid acc: 39.22%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 1.16, train acc: 53.06%, consuming tine: 4.97\n",
      "batch: 402, batch train loss: 1.16, train acc: 53.07%, consuming tine: 4.56\n",
      "batch: 403, batch train loss: 1.16, train acc: 53.09%, consuming tine: 4.50\n",
      "batch: 404, batch train loss: 1.16, train acc: 53.10%, consuming tine: 4.80\n",
      "batch: 405, batch train loss: 1.16, train acc: 53.12%, consuming tine: 4.79\n",
      "batch: 406, batch train loss: 1.16, train acc: 53.14%, consuming tine: 4.88\n",
      "batch: 407, batch train loss: 1.16, train acc: 53.16%, consuming tine: 4.50\n",
      "batch: 408, batch train loss: 1.16, train acc: 53.18%, consuming tine: 4.88\n",
      "batch: 409, batch train loss: 1.16, train acc: 53.20%, consuming tine: 4.58\n",
      "batch: 410, batch train loss: 1.16, train acc: 53.21%, consuming tine: 4.69\n",
      "batch: 411, batch train loss: 1.16, train acc: 53.22%, consuming tine: 4.68\n",
      "batch: 412, batch train loss: 1.16, train acc: 53.24%, consuming tine: 5.19\n",
      "batch: 413, batch train loss: 1.16, train acc: 53.26%, consuming tine: 4.76\n",
      "batch: 414, batch train loss: 1.16, train acc: 53.27%, consuming tine: 5.15\n",
      "batch: 415, batch train loss: 1.16, train acc: 53.29%, consuming tine: 4.88\n",
      "batch: 416, batch train loss: 1.16, train acc: 53.31%, consuming tine: 4.97\n",
      "batch: 417, batch train loss: 1.15, train acc: 53.32%, consuming tine: 4.89\n",
      "batch: 418, batch train loss: 1.15, train acc: 53.34%, consuming tine: 4.71\n",
      "batch: 419, batch train loss: 1.15, train acc: 53.35%, consuming tine: 4.79\n",
      "batch: 420, batch train loss: 1.15, train acc: 53.38%, consuming tine: 4.87\n",
      "batch: 421, batch train loss: 1.15, train acc: 53.39%, consuming tine: 5.30\n",
      "batch: 422, batch train loss: 1.15, train acc: 53.40%, consuming tine: 4.68\n",
      "batch: 423, batch train loss: 1.15, train acc: 53.42%, consuming tine: 4.98\n",
      "batch: 424, batch train loss: 1.15, train acc: 53.44%, consuming tine: 4.80\n",
      "batch: 425, batch train loss: 1.15, train acc: 53.46%, consuming tine: 4.98\n",
      "batch: 426, batch train loss: 1.15, train acc: 53.47%, consuming tine: 4.59\n",
      "batch: 427, batch train loss: 1.15, train acc: 53.48%, consuming tine: 4.65\n",
      "batch: 428, batch train loss: 1.15, train acc: 53.48%, consuming tine: 4.62\n",
      "batch: 429, batch train loss: 1.15, train acc: 53.50%, consuming tine: 4.69\n",
      "batch: 430, batch train loss: 1.15, train acc: 53.52%, consuming tine: 4.69\n",
      "batch: 431, batch train loss: 1.15, train acc: 53.53%, consuming tine: 4.60\n",
      "batch: 432, batch train loss: 1.15, train acc: 53.55%, consuming tine: 4.69\n",
      "batch: 433, batch train loss: 1.15, train acc: 53.56%, consuming tine: 4.98\n",
      "batch: 434, batch train loss: 1.15, train acc: 53.58%, consuming tine: 4.38\n",
      "batch: 435, batch train loss: 1.15, train acc: 53.59%, consuming tine: 4.72\n",
      "batch: 436, batch train loss: 1.15, train acc: 53.60%, consuming tine: 4.76\n",
      "batch: 437, batch train loss: 1.15, train acc: 53.62%, consuming tine: 4.58\n",
      "batch: 438, batch train loss: 1.15, train acc: 53.63%, consuming tine: 4.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 439, batch train loss: 1.15, train acc: 53.66%, consuming tine: 5.09\n",
      "batch: 440, batch train loss: 1.15, train acc: 53.68%, consuming tine: 4.57\n",
      "batch: 441, batch train loss: 1.15, train acc: 53.69%, consuming tine: 4.79\n",
      "batch: 442, batch train loss: 1.15, train acc: 53.71%, consuming tine: 4.79\n",
      "batch: 443, batch train loss: 1.15, train acc: 53.72%, consuming tine: 4.71\n",
      "batch: 444, batch train loss: 1.15, train acc: 53.74%, consuming tine: 4.67\n",
      "batch: 445, batch train loss: 1.15, train acc: 53.76%, consuming tine: 4.68\n",
      "batch: 446, batch train loss: 1.15, train acc: 53.77%, consuming tine: 4.72\n",
      "batch: 447, batch train loss: 1.14, train acc: 53.78%, consuming tine: 4.78\n",
      "batch: 448, batch train loss: 1.14, train acc: 53.79%, consuming tine: 4.50\n",
      "batch: 449, batch train loss: 1.14, train acc: 53.82%, consuming tine: 4.68\n",
      "batch: 450, batch train loss: 1.14, train acc: 53.82%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 1.60, valid acc: 39.13%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 1.14, train acc: 53.84%, consuming tine: 4.60\n",
      "batch: 452, batch train loss: 1.14, train acc: 53.86%, consuming tine: 4.60\n",
      "batch: 453, batch train loss: 1.14, train acc: 53.88%, consuming tine: 4.68\n",
      "batch: 454, batch train loss: 1.14, train acc: 53.89%, consuming tine: 4.49\n",
      "batch: 455, batch train loss: 1.14, train acc: 53.91%, consuming tine: 5.08\n",
      "batch: 456, batch train loss: 1.14, train acc: 53.93%, consuming tine: 4.70\n",
      "batch: 457, batch train loss: 1.14, train acc: 53.94%, consuming tine: 4.78\n",
      "batch: 458, batch train loss: 1.14, train acc: 53.95%, consuming tine: 5.11\n",
      "batch: 459, batch train loss: 1.14, train acc: 53.97%, consuming tine: 4.89\n",
      "batch: 460, batch train loss: 1.14, train acc: 53.99%, consuming tine: 4.87\n",
      "batch: 461, batch train loss: 1.14, train acc: 54.01%, consuming tine: 4.98\n",
      "batch: 462, batch train loss: 1.14, train acc: 54.02%, consuming tine: 5.00\n",
      "batch: 463, batch train loss: 1.14, train acc: 54.04%, consuming tine: 4.51\n",
      "batch: 464, batch train loss: 1.14, train acc: 54.06%, consuming tine: 4.68\n",
      "batch: 465, batch train loss: 1.14, train acc: 54.08%, consuming tine: 4.78\n",
      "batch: 466, batch train loss: 1.14, train acc: 54.10%, consuming tine: 4.78\n",
      "batch: 467, batch train loss: 1.14, train acc: 54.11%, consuming tine: 4.91\n",
      "batch: 468, batch train loss: 1.14, train acc: 54.12%, consuming tine: 5.08\n",
      "batch: 469, batch train loss: 1.14, train acc: 54.14%, consuming tine: 4.58\n",
      "batch: 470, batch train loss: 1.14, train acc: 54.16%, consuming tine: 4.88\n",
      "batch: 471, batch train loss: 1.14, train acc: 54.17%, consuming tine: 4.50\n",
      "batch: 472, batch train loss: 1.14, train acc: 54.17%, consuming tine: 4.78\n",
      "batch: 473, batch train loss: 1.14, train acc: 54.19%, consuming tine: 4.80\n",
      "batch: 474, batch train loss: 1.14, train acc: 54.20%, consuming tine: 4.58\n",
      "batch: 475, batch train loss: 1.14, train acc: 54.22%, consuming tine: 4.77\n",
      "batch: 476, batch train loss: 1.14, train acc: 54.24%, consuming tine: 4.58\n",
      "batch: 477, batch train loss: 1.14, train acc: 54.24%, consuming tine: 4.60\n",
      "batch: 478, batch train loss: 1.14, train acc: 54.26%, consuming tine: 4.49\n",
      "batch: 479, batch train loss: 1.13, train acc: 54.27%, consuming tine: 4.59\n",
      "batch: 480, batch train loss: 1.13, train acc: 54.28%, consuming tine: 4.89\n",
      "batch: 481, batch train loss: 1.13, train acc: 54.30%, consuming tine: 4.50\n",
      "batch: 482, batch train loss: 1.13, train acc: 54.31%, consuming tine: 5.19\n",
      "batch: 483, batch train loss: 1.13, train acc: 54.33%, consuming tine: 4.68\n",
      "batch: 484, batch train loss: 1.13, train acc: 54.35%, consuming tine: 4.58\n",
      "batch: 485, batch train loss: 1.13, train acc: 54.36%, consuming tine: 4.90\n",
      "batch: 486, batch train loss: 1.13, train acc: 54.37%, consuming tine: 4.48\n",
      "batch: 487, batch train loss: 1.13, train acc: 54.38%, consuming tine: 4.69\n",
      "batch: 488, batch train loss: 1.13, train acc: 54.39%, consuming tine: 4.90\n",
      "batch: 489, batch train loss: 1.13, train acc: 54.39%, consuming tine: 4.69\n",
      "batch: 490, batch train loss: 1.13, train acc: 54.41%, consuming tine: 4.88\n",
      "batch: 491, batch train loss: 1.13, train acc: 54.41%, consuming tine: 4.78\n",
      "batch: 492, batch train loss: 1.13, train acc: 54.42%, consuming tine: 4.80\n",
      "batch: 493, batch train loss: 1.13, train acc: 54.44%, consuming tine: 4.59\n",
      "batch: 494, batch train loss: 1.13, train acc: 54.45%, consuming tine: 4.96\n",
      "batch: 495, batch train loss: 1.13, train acc: 54.47%, consuming tine: 4.40\n",
      "batch: 496, batch train loss: 1.13, train acc: 54.48%, consuming tine: 4.78\n",
      "batch: 497, batch train loss: 1.13, train acc: 54.49%, consuming tine: 4.77\n",
      "batch: 498, batch train loss: 1.13, train acc: 54.50%, consuming tine: 4.99\n",
      "batch: 499, batch train loss: 1.13, train acc: 54.51%, consuming tine: 4.98\n",
      "batch: 500, batch train loss: 1.13, train acc: 54.53%, consuming tine: 4.73\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 1.61, valid acc: 39.02%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 1.13, train acc: 54.54%, consuming tine: 4.85\n",
      "batch: 502, batch train loss: 1.13, train acc: 54.55%, consuming tine: 4.98\n",
      "batch: 503, batch train loss: 1.13, train acc: 54.56%, consuming tine: 4.58\n",
      "batch: 504, batch train loss: 1.13, train acc: 54.57%, consuming tine: 4.90\n",
      "batch: 505, batch train loss: 1.13, train acc: 54.58%, consuming tine: 4.98\n",
      "batch: 506, batch train loss: 1.13, train acc: 54.59%, consuming tine: 4.78\n",
      "batch: 507, batch train loss: 1.13, train acc: 54.61%, consuming tine: 4.49\n",
      "batch: 508, batch train loss: 1.13, train acc: 54.62%, consuming tine: 5.40\n",
      "batch: 509, batch train loss: 1.13, train acc: 54.63%, consuming tine: 4.68\n",
      "batch: 510, batch train loss: 1.13, train acc: 54.64%, consuming tine: 4.39\n",
      "batch: 511, batch train loss: 1.13, train acc: 54.66%, consuming tine: 4.89\n",
      "batch: 512, batch train loss: 1.13, train acc: 54.66%, consuming tine: 4.69\n",
      "batch: 513, batch train loss: 1.13, train acc: 54.68%, consuming tine: 4.71\n",
      "batch: 514, batch train loss: 1.13, train acc: 54.69%, consuming tine: 4.76\n",
      "batch: 515, batch train loss: 1.13, train acc: 54.70%, consuming tine: 4.60\n",
      "batch: 516, batch train loss: 1.13, train acc: 54.71%, consuming tine: 4.67\n",
      "batch: 517, batch train loss: 1.13, train acc: 54.72%, consuming tine: 4.98\n",
      "batch: 518, batch train loss: 1.13, train acc: 54.73%, consuming tine: 4.59\n",
      "batch: 519, batch train loss: 1.13, train acc: 54.75%, consuming tine: 4.61\n",
      "batch: 520, batch train loss: 1.13, train acc: 54.76%, consuming tine: 4.96\n",
      "batch: 521, batch train loss: 1.13, train acc: 54.78%, consuming tine: 4.59\n",
      "batch: 522, batch train loss: 1.12, train acc: 54.79%, consuming tine: 4.71\n",
      "batch: 523, batch train loss: 1.12, train acc: 54.80%, consuming tine: 5.06\n",
      "batch: 524, batch train loss: 1.12, train acc: 54.81%, consuming tine: 5.11\n",
      "batch: 525, batch train loss: 1.12, train acc: 54.82%, consuming tine: 4.99\n",
      "batch: 526, batch train loss: 1.12, train acc: 54.83%, consuming tine: 4.68\n",
      "batch: 527, batch train loss: 1.12, train acc: 54.85%, consuming tine: 4.79\n",
      "batch: 528, batch train loss: 1.12, train acc: 54.86%, consuming tine: 4.49\n",
      "batch: 529, batch train loss: 1.12, train acc: 54.87%, consuming tine: 4.81\n",
      "batch: 530, batch train loss: 1.12, train acc: 54.88%, consuming tine: 4.47\n",
      "batch: 531, batch train loss: 1.12, train acc: 54.89%, consuming tine: 4.77\n",
      "batch: 532, batch train loss: 1.12, train acc: 54.90%, consuming tine: 4.80\n",
      "batch: 533, batch train loss: 1.12, train acc: 54.91%, consuming tine: 4.79\n",
      "batch: 534, batch train loss: 1.12, train acc: 54.92%, consuming tine: 5.17\n",
      "batch: 535, batch train loss: 1.12, train acc: 54.93%, consuming tine: 5.00\n",
      "batch: 536, batch train loss: 1.12, train acc: 54.95%, consuming tine: 4.59\n",
      "batch: 537, batch train loss: 1.12, train acc: 54.96%, consuming tine: 4.69\n",
      "batch: 538, batch train loss: 1.12, train acc: 54.97%, consuming tine: 4.89\n",
      "batch: 539, batch train loss: 1.12, train acc: 54.98%, consuming tine: 4.48\n",
      "batch: 540, batch train loss: 1.12, train acc: 54.99%, consuming tine: 4.90\n",
      "batch: 541, batch train loss: 1.12, train acc: 55.00%, consuming tine: 4.77\n",
      "batch: 542, batch train loss: 1.12, train acc: 55.02%, consuming tine: 4.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 543, batch train loss: 1.12, train acc: 55.03%, consuming tine: 4.71\n",
      "batch: 544, batch train loss: 1.12, train acc: 55.04%, consuming tine: 4.97\n",
      "batch: 545, batch train loss: 1.12, train acc: 55.05%, consuming tine: 4.48\n",
      "batch: 546, batch train loss: 1.12, train acc: 55.06%, consuming tine: 4.80\n",
      "batch: 547, batch train loss: 1.12, train acc: 55.07%, consuming tine: 4.89\n",
      "batch: 548, batch train loss: 1.12, train acc: 55.07%, consuming tine: 4.98\n",
      "batch: 549, batch train loss: 1.12, train acc: 55.09%, consuming tine: 5.10\n",
      "batch: 550, batch train loss: 1.12, train acc: 55.10%, consuming tine: 5.20\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 1.62, valid acc: 38.98%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 1.12, train acc: 55.11%, consuming tine: 4.67\n",
      "batch: 552, batch train loss: 1.12, train acc: 55.13%, consuming tine: 4.88\n",
      "batch: 553, batch train loss: 1.12, train acc: 55.14%, consuming tine: 4.52\n",
      "batch: 554, batch train loss: 1.12, train acc: 55.15%, consuming tine: 4.66\n",
      "batch: 555, batch train loss: 1.12, train acc: 55.16%, consuming tine: 4.69\n",
      "batch: 556, batch train loss: 1.12, train acc: 55.18%, consuming tine: 4.99\n",
      "batch: 557, batch train loss: 1.12, train acc: 55.19%, consuming tine: 4.56\n",
      "batch: 558, batch train loss: 1.12, train acc: 55.21%, consuming tine: 4.59\n",
      "batch: 559, batch train loss: 1.12, train acc: 55.22%, consuming tine: 4.59\n",
      "batch: 560, batch train loss: 1.12, train acc: 55.23%, consuming tine: 4.78\n",
      "batch: 561, batch train loss: 1.11, train acc: 55.25%, consuming tine: 5.00\n",
      "batch: 562, batch train loss: 1.11, train acc: 55.26%, consuming tine: 4.87\n",
      "batch: 563, batch train loss: 1.11, train acc: 55.27%, consuming tine: 4.58\n",
      "batch: 564, batch train loss: 1.11, train acc: 55.28%, consuming tine: 4.98\n",
      "batch: 565, batch train loss: 1.11, train acc: 55.29%, consuming tine: 4.90\n",
      "batch: 566, batch train loss: 1.11, train acc: 55.31%, consuming tine: 4.89\n",
      "batch: 567, batch train loss: 1.11, train acc: 55.32%, consuming tine: 5.07\n",
      "batch: 568, batch train loss: 1.11, train acc: 55.33%, consuming tine: 4.49\n",
      "batch: 569, batch train loss: 1.11, train acc: 55.34%, consuming tine: 5.08\n",
      "batch: 570, batch train loss: 1.11, train acc: 55.35%, consuming tine: 4.62\n",
      "batch: 571, batch train loss: 1.11, train acc: 55.36%, consuming tine: 4.77\n",
      "batch: 572, batch train loss: 1.11, train acc: 55.38%, consuming tine: 4.89\n",
      "batch: 573, batch train loss: 1.11, train acc: 55.39%, consuming tine: 4.77\n",
      "batch: 574, batch train loss: 1.11, train acc: 55.40%, consuming tine: 4.80\n",
      "batch: 575, batch train loss: 1.11, train acc: 55.41%, consuming tine: 4.67\n",
      "batch: 576, batch train loss: 1.11, train acc: 55.43%, consuming tine: 4.79\n",
      "batch: 577, batch train loss: 1.11, train acc: 55.43%, consuming tine: 4.80\n",
      "batch: 578, batch train loss: 1.11, train acc: 55.44%, consuming tine: 5.16\n",
      "batch: 579, batch train loss: 1.11, train acc: 55.46%, consuming tine: 4.81\n",
      "batch: 580, batch train loss: 1.11, train acc: 55.47%, consuming tine: 4.59\n",
      "batch: 581, batch train loss: 1.11, train acc: 55.48%, consuming tine: 4.80\n",
      "batch: 582, batch train loss: 1.11, train acc: 55.49%, consuming tine: 4.58\n",
      "batch: 583, batch train loss: 1.11, train acc: 55.50%, consuming tine: 5.00\n",
      "batch: 584, batch train loss: 1.11, train acc: 55.52%, consuming tine: 4.67\n",
      "batch: 585, batch train loss: 1.11, train acc: 55.53%, consuming tine: 4.60\n",
      "batch: 586, batch train loss: 1.11, train acc: 55.54%, consuming tine: 4.99\n",
      "batch: 587, batch train loss: 1.11, train acc: 55.55%, consuming tine: 4.68\n",
      "batch: 588, batch train loss: 1.11, train acc: 55.56%, consuming tine: 4.70\n",
      "batch: 589, batch train loss: 1.11, train acc: 55.57%, consuming tine: 4.89\n",
      "batch: 590, batch train loss: 1.11, train acc: 55.59%, consuming tine: 4.68\n",
      "batch: 591, batch train loss: 1.11, train acc: 55.60%, consuming tine: 5.10\n",
      "batch: 592, batch train loss: 1.11, train acc: 55.61%, consuming tine: 4.69\n",
      "batch: 593, batch train loss: 1.11, train acc: 55.62%, consuming tine: 4.78\n",
      "batch: 594, batch train loss: 1.11, train acc: 55.62%, consuming tine: 4.59\n",
      "batch: 595, batch train loss: 1.11, train acc: 55.63%, consuming tine: 4.51\n",
      "batch: 596, batch train loss: 1.11, train acc: 55.64%, consuming tine: 4.96\n",
      "batch: 597, batch train loss: 1.11, train acc: 55.65%, consuming tine: 4.50\n",
      "batch: 598, batch train loss: 1.11, train acc: 55.65%, consuming tine: 4.79\n",
      "batch: 599, batch train loss: 1.11, train acc: 55.66%, consuming tine: 4.68\n",
      "batch: 600, batch train loss: 1.11, train acc: 55.66%, consuming tine: 4.92\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 1.63, valid acc: 38.93%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 1.11, train acc: 55.67%, consuming tine: 4.59\n",
      "batch: 602, batch train loss: 1.11, train acc: 55.68%, consuming tine: 4.87\n",
      "batch: 603, batch train loss: 1.11, train acc: 55.69%, consuming tine: 4.59\n",
      "batch: 604, batch train loss: 1.11, train acc: 55.70%, consuming tine: 4.71\n",
      "batch: 605, batch train loss: 1.11, train acc: 55.70%, consuming tine: 4.67\n",
      "batch: 606, batch train loss: 1.11, train acc: 55.71%, consuming tine: 4.88\n",
      "batch: 607, batch train loss: 1.10, train acc: 55.72%, consuming tine: 4.87\n",
      "batch: 608, batch train loss: 1.10, train acc: 55.73%, consuming tine: 4.79\n",
      "batch: 609, batch train loss: 1.10, train acc: 55.73%, consuming tine: 5.08\n",
      "batch: 610, batch train loss: 1.10, train acc: 55.74%, consuming tine: 4.69\n",
      "batch: 611, batch train loss: 1.10, train acc: 55.75%, consuming tine: 4.68\n",
      "batch: 612, batch train loss: 1.10, train acc: 55.76%, consuming tine: 5.19\n",
      "batch: 613, batch train loss: 1.10, train acc: 55.78%, consuming tine: 4.81\n",
      "batch: 614, batch train loss: 1.10, train acc: 55.78%, consuming tine: 4.80\n",
      "batch: 615, batch train loss: 1.10, train acc: 55.79%, consuming tine: 4.81\n",
      "batch: 616, batch train loss: 1.10, train acc: 55.81%, consuming tine: 5.03\n",
      "batch: 617, batch train loss: 1.10, train acc: 55.82%, consuming tine: 5.01\n",
      "batch: 618, batch train loss: 1.10, train acc: 55.83%, consuming tine: 5.38\n",
      "batch: 619, batch train loss: 1.10, train acc: 55.84%, consuming tine: 4.98\n",
      "batch: 620, batch train loss: 1.10, train acc: 55.85%, consuming tine: 4.98\n",
      "batch: 621, batch train loss: 1.10, train acc: 55.86%, consuming tine: 4.49\n",
      "batch: 622, batch train loss: 1.10, train acc: 55.88%, consuming tine: 4.89\n",
      "batch: 623, batch train loss: 1.10, train acc: 55.89%, consuming tine: 4.78\n",
      "batch: 624, batch train loss: 1.10, train acc: 55.89%, consuming tine: 4.89\n",
      "batch: 625, batch train loss: 1.10, train acc: 55.90%, consuming tine: 4.59\n",
      "batch: 626, batch train loss: 1.10, train acc: 55.91%, consuming tine: 4.97\n",
      "batch: 627, batch train loss: 1.10, train acc: 55.92%, consuming tine: 4.59\n",
      "batch: 628, batch train loss: 1.10, train acc: 55.93%, consuming tine: 4.49\n",
      "batch: 629, batch train loss: 1.10, train acc: 55.95%, consuming tine: 4.95\n",
      "batch: 630, batch train loss: 1.10, train acc: 55.96%, consuming tine: 4.82\n",
      "batch: 631, batch train loss: 1.10, train acc: 55.97%, consuming tine: 4.47\n",
      "batch: 632, batch train loss: 1.10, train acc: 55.99%, consuming tine: 4.99\n",
      "batch: 633, batch train loss: 1.10, train acc: 55.99%, consuming tine: 4.55\n",
      "batch: 634, batch train loss: 1.10, train acc: 56.00%, consuming tine: 5.03\n",
      "batch: 635, batch train loss: 1.10, train acc: 56.02%, consuming tine: 5.04\n",
      "batch: 636, batch train loss: 1.10, train acc: 56.02%, consuming tine: 4.74\n",
      "batch: 637, batch train loss: 1.10, train acc: 56.03%, consuming tine: 4.68\n",
      "batch: 638, batch train loss: 1.10, train acc: 56.04%, consuming tine: 4.41\n",
      "batch: 639, batch train loss: 1.10, train acc: 56.05%, consuming tine: 4.67\n",
      "batch: 640, batch train loss: 1.10, train acc: 56.06%, consuming tine: 4.64\n",
      "batch: 641, batch train loss: 1.10, train acc: 56.06%, consuming tine: 4.74\n",
      "batch: 642, batch train loss: 1.10, train acc: 56.07%, consuming tine: 4.79\n",
      "batch: 643, batch train loss: 1.10, train acc: 56.09%, consuming tine: 5.10\n",
      "batch: 644, batch train loss: 1.10, train acc: 56.10%, consuming tine: 4.79\n",
      "batch: 645, batch train loss: 1.10, train acc: 56.11%, consuming tine: 5.09\n",
      "batch: 646, batch train loss: 1.10, train acc: 56.11%, consuming tine: 4.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 647, batch train loss: 1.10, train acc: 56.12%, consuming tine: 4.57\n",
      "batch: 648, batch train loss: 1.10, train acc: 56.13%, consuming tine: 4.78\n",
      "batch: 649, batch train loss: 1.10, train acc: 56.15%, consuming tine: 4.79\n",
      "batch: 650, batch train loss: 1.10, train acc: 56.15%, consuming tine: 4.79\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 1.63, valid acc: 38.86%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 1.10, train acc: 56.17%, consuming tine: 4.54\n",
      "batch: 652, batch train loss: 1.10, train acc: 56.18%, consuming tine: 5.25\n",
      "batch: 653, batch train loss: 1.09, train acc: 56.18%, consuming tine: 4.73\n",
      "batch: 654, batch train loss: 1.09, train acc: 56.19%, consuming tine: 4.38\n",
      "batch: 655, batch train loss: 1.09, train acc: 56.21%, consuming tine: 4.92\n",
      "batch: 656, batch train loss: 1.09, train acc: 56.22%, consuming tine: 4.57\n",
      "batch: 657, batch train loss: 1.09, train acc: 56.23%, consuming tine: 4.59\n",
      "batch: 658, batch train loss: 1.09, train acc: 56.24%, consuming tine: 4.79\n",
      "batch: 659, batch train loss: 1.09, train acc: 56.25%, consuming tine: 4.67\n",
      "batch: 660, batch train loss: 1.09, train acc: 56.26%, consuming tine: 4.49\n",
      "batch: 661, batch train loss: 1.09, train acc: 56.26%, consuming tine: 4.72\n",
      "batch: 662, batch train loss: 1.09, train acc: 56.27%, consuming tine: 4.56\n",
      "batch: 663, batch train loss: 1.09, train acc: 56.28%, consuming tine: 4.80\n",
      "batch: 664, batch train loss: 1.09, train acc: 56.29%, consuming tine: 4.77\n",
      "batch: 665, batch train loss: 1.09, train acc: 56.30%, consuming tine: 4.49\n",
      "batch: 666, batch train loss: 1.09, train acc: 56.31%, consuming tine: 4.79\n",
      "batch: 667, batch train loss: 1.09, train acc: 56.32%, consuming tine: 4.78\n",
      "batch: 668, batch train loss: 1.09, train acc: 56.32%, consuming tine: 5.08\n",
      "batch: 669, batch train loss: 1.09, train acc: 56.33%, consuming tine: 4.50\n",
      "batch: 670, batch train loss: 1.09, train acc: 56.34%, consuming tine: 4.90\n",
      "batch: 671, batch train loss: 1.09, train acc: 56.35%, consuming tine: 4.76\n",
      "batch: 672, batch train loss: 1.09, train acc: 56.36%, consuming tine: 4.60\n",
      "batch: 673, batch train loss: 1.09, train acc: 56.37%, consuming tine: 4.79\n",
      "batch: 674, batch train loss: 1.09, train acc: 56.38%, consuming tine: 4.88\n",
      "batch: 675, batch train loss: 1.09, train acc: 56.39%, consuming tine: 4.59\n",
      "batch: 676, batch train loss: 1.09, train acc: 56.40%, consuming tine: 4.68\n",
      "batch: 677, batch train loss: 1.09, train acc: 56.41%, consuming tine: 4.80\n",
      "batch: 678, batch train loss: 1.09, train acc: 56.43%, consuming tine: 4.69\n",
      "batch: 679, batch train loss: 1.09, train acc: 56.44%, consuming tine: 4.78\n",
      "batch: 680, batch train loss: 1.09, train acc: 56.45%, consuming tine: 4.81\n",
      "batch: 681, batch train loss: 1.09, train acc: 56.46%, consuming tine: 4.83\n",
      "batch: 682, batch train loss: 1.09, train acc: 56.47%, consuming tine: 4.64\n",
      "batch: 683, batch train loss: 1.09, train acc: 56.48%, consuming tine: 4.78\n",
      "batch: 684, batch train loss: 1.09, train acc: 56.49%, consuming tine: 4.75\n",
      "batch: 685, batch train loss: 1.09, train acc: 56.50%, consuming tine: 4.63\n",
      "batch: 686, batch train loss: 1.09, train acc: 56.50%, consuming tine: 4.69\n",
      "batch: 687, batch train loss: 1.09, train acc: 56.51%, consuming tine: 5.28\n",
      "batch: 688, batch train loss: 1.09, train acc: 56.52%, consuming tine: 4.59\n",
      "batch: 689, batch train loss: 1.09, train acc: 56.53%, consuming tine: 4.50\n",
      "batch: 690, batch train loss: 1.09, train acc: 56.54%, consuming tine: 4.38\n",
      "batch: 691, batch train loss: 1.09, train acc: 56.54%, consuming tine: 4.48\n",
      "batch: 692, batch train loss: 1.09, train acc: 56.55%, consuming tine: 4.49\n",
      "batch: 693, batch train loss: 1.09, train acc: 56.56%, consuming tine: 4.72\n",
      "batch: 694, batch train loss: 1.09, train acc: 56.57%, consuming tine: 4.48\n",
      "batch: 695, batch train loss: 1.09, train acc: 56.58%, consuming tine: 4.56\n",
      "batch: 696, batch train loss: 1.09, train acc: 56.59%, consuming tine: 5.11\n",
      "batch: 697, batch train loss: 1.09, train acc: 56.60%, consuming tine: 4.58\n",
      "batch: 698, batch train loss: 1.09, train acc: 56.61%, consuming tine: 4.78\n",
      "batch: 699, batch train loss: 1.09, train acc: 56.62%, consuming tine: 4.78\n",
      "batch: 700, batch train loss: 1.09, train acc: 56.63%, consuming tine: 4.58\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 1.64, valid acc: 38.83%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 1.09, train acc: 56.63%, consuming tine: 4.62\n",
      "batch: 702, batch train loss: 1.09, train acc: 56.65%, consuming tine: 4.47\n",
      "batch: 703, batch train loss: 1.08, train acc: 56.66%, consuming tine: 4.79\n",
      "batch: 704, batch train loss: 1.08, train acc: 56.67%, consuming tine: 4.61\n",
      "Epoch 2, Loss: 1.08, Accuracy: 56.67%, Valid Loss: 1.64, Valid Accuracy: 38.83%\n",
      "batch: 1, batch train loss: 0.93, train acc: 64.84%, consuming tine: 4.54\n",
      "batch: 2, batch train loss: 0.95, train acc: 63.38%, consuming tine: 5.01\n",
      "batch: 3, batch train loss: 0.94, train acc: 63.64%, consuming tine: 4.56\n",
      "batch: 4, batch train loss: 0.94, train acc: 63.55%, consuming tine: 4.69\n",
      "batch: 5, batch train loss: 0.94, train acc: 64.04%, consuming tine: 4.60\n",
      "batch: 6, batch train loss: 0.94, train acc: 63.75%, consuming tine: 4.57\n",
      "batch: 7, batch train loss: 0.94, train acc: 63.73%, consuming tine: 4.81\n",
      "batch: 8, batch train loss: 0.94, train acc: 63.71%, consuming tine: 4.57\n",
      "batch: 9, batch train loss: 0.95, train acc: 63.43%, consuming tine: 4.58\n",
      "batch: 10, batch train loss: 0.94, train acc: 63.62%, consuming tine: 4.64\n",
      "batch: 11, batch train loss: 0.94, train acc: 63.73%, consuming tine: 4.54\n",
      "batch: 12, batch train loss: 0.93, train acc: 63.64%, consuming tine: 5.28\n",
      "batch: 13, batch train loss: 0.93, train acc: 63.56%, consuming tine: 5.04\n",
      "batch: 14, batch train loss: 0.94, train acc: 63.41%, consuming tine: 4.66\n",
      "batch: 15, batch train loss: 0.94, train acc: 63.50%, consuming tine: 4.97\n",
      "batch: 16, batch train loss: 0.93, train acc: 63.59%, consuming tine: 4.48\n",
      "batch: 17, batch train loss: 0.93, train acc: 63.63%, consuming tine: 4.60\n",
      "batch: 18, batch train loss: 0.93, train acc: 63.57%, consuming tine: 4.88\n",
      "batch: 19, batch train loss: 0.93, train acc: 63.68%, consuming tine: 4.89\n",
      "batch: 20, batch train loss: 0.93, train acc: 63.65%, consuming tine: 4.60\n",
      "batch: 21, batch train loss: 0.93, train acc: 63.63%, consuming tine: 4.80\n",
      "batch: 22, batch train loss: 0.93, train acc: 63.74%, consuming tine: 5.03\n",
      "batch: 23, batch train loss: 0.93, train acc: 63.68%, consuming tine: 5.01\n",
      "batch: 24, batch train loss: 0.93, train acc: 63.64%, consuming tine: 5.01\n",
      "batch: 25, batch train loss: 0.93, train acc: 63.57%, consuming tine: 4.90\n",
      "batch: 26, batch train loss: 0.93, train acc: 63.62%, consuming tine: 5.10\n",
      "batch: 27, batch train loss: 0.93, train acc: 63.51%, consuming tine: 4.87\n",
      "batch: 28, batch train loss: 0.93, train acc: 63.55%, consuming tine: 4.67\n",
      "batch: 29, batch train loss: 0.93, train acc: 63.52%, consuming tine: 4.59\n",
      "batch: 30, batch train loss: 0.94, train acc: 63.51%, consuming tine: 4.51\n",
      "batch: 31, batch train loss: 0.94, train acc: 63.50%, consuming tine: 4.77\n",
      "batch: 32, batch train loss: 0.93, train acc: 63.49%, consuming tine: 4.58\n",
      "batch: 33, batch train loss: 0.93, train acc: 63.53%, consuming tine: 4.49\n",
      "batch: 34, batch train loss: 0.93, train acc: 63.58%, consuming tine: 4.61\n",
      "batch: 35, batch train loss: 0.93, train acc: 63.66%, consuming tine: 4.58\n",
      "batch: 36, batch train loss: 0.93, train acc: 63.71%, consuming tine: 4.89\n",
      "batch: 37, batch train loss: 0.93, train acc: 63.71%, consuming tine: 4.49\n",
      "batch: 38, batch train loss: 0.93, train acc: 63.67%, consuming tine: 4.79\n",
      "batch: 39, batch train loss: 0.93, train acc: 63.62%, consuming tine: 4.59\n",
      "batch: 40, batch train loss: 0.93, train acc: 63.70%, consuming tine: 4.98\n",
      "batch: 41, batch train loss: 0.93, train acc: 63.81%, consuming tine: 4.89\n",
      "batch: 42, batch train loss: 0.93, train acc: 63.83%, consuming tine: 4.59\n",
      "batch: 43, batch train loss: 0.93, train acc: 63.86%, consuming tine: 4.69\n",
      "batch: 44, batch train loss: 0.93, train acc: 63.91%, consuming tine: 4.72\n",
      "batch: 45, batch train loss: 0.92, train acc: 63.94%, consuming tine: 4.57\n",
      "batch: 46, batch train loss: 0.92, train acc: 64.06%, consuming tine: 4.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 47, batch train loss: 0.92, train acc: 64.11%, consuming tine: 4.87\n",
      "batch: 48, batch train loss: 0.92, train acc: 64.17%, consuming tine: 4.50\n",
      "batch: 49, batch train loss: 0.92, train acc: 64.22%, consuming tine: 4.77\n",
      "batch: 50, batch train loss: 0.92, train acc: 64.29%, consuming tine: 4.52\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 1.81, valid acc: 37.87%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.92, train acc: 64.32%, consuming tine: 4.71\n",
      "batch: 52, batch train loss: 0.92, train acc: 64.36%, consuming tine: 4.87\n",
      "batch: 53, batch train loss: 0.92, train acc: 64.38%, consuming tine: 4.80\n",
      "batch: 54, batch train loss: 0.92, train acc: 64.41%, consuming tine: 4.79\n",
      "batch: 55, batch train loss: 0.91, train acc: 64.44%, consuming tine: 4.79\n",
      "batch: 56, batch train loss: 0.91, train acc: 64.45%, consuming tine: 4.68\n",
      "batch: 57, batch train loss: 0.91, train acc: 64.43%, consuming tine: 4.80\n",
      "batch: 58, batch train loss: 0.91, train acc: 64.48%, consuming tine: 4.67\n",
      "batch: 59, batch train loss: 0.91, train acc: 64.53%, consuming tine: 4.80\n",
      "batch: 60, batch train loss: 0.91, train acc: 64.58%, consuming tine: 4.58\n",
      "batch: 61, batch train loss: 0.91, train acc: 64.58%, consuming tine: 4.59\n",
      "batch: 62, batch train loss: 0.91, train acc: 64.64%, consuming tine: 4.88\n",
      "batch: 63, batch train loss: 0.91, train acc: 64.68%, consuming tine: 4.88\n",
      "batch: 64, batch train loss: 0.91, train acc: 64.76%, consuming tine: 4.40\n",
      "batch: 65, batch train loss: 0.91, train acc: 64.77%, consuming tine: 4.79\n",
      "batch: 66, batch train loss: 0.90, train acc: 64.81%, consuming tine: 4.62\n",
      "batch: 67, batch train loss: 0.90, train acc: 64.88%, consuming tine: 4.86\n",
      "batch: 68, batch train loss: 0.90, train acc: 64.92%, consuming tine: 4.49\n",
      "batch: 69, batch train loss: 0.90, train acc: 64.95%, consuming tine: 5.06\n",
      "batch: 70, batch train loss: 0.90, train acc: 65.01%, consuming tine: 4.41\n",
      "batch: 71, batch train loss: 0.90, train acc: 65.05%, consuming tine: 4.88\n",
      "batch: 72, batch train loss: 0.90, train acc: 65.08%, consuming tine: 4.59\n",
      "batch: 73, batch train loss: 0.90, train acc: 65.15%, consuming tine: 4.59\n",
      "batch: 74, batch train loss: 0.90, train acc: 65.19%, consuming tine: 4.69\n",
      "batch: 75, batch train loss: 0.89, train acc: 65.23%, consuming tine: 4.79\n",
      "batch: 76, batch train loss: 0.89, train acc: 65.18%, consuming tine: 4.49\n",
      "batch: 77, batch train loss: 0.89, train acc: 65.23%, consuming tine: 4.58\n",
      "batch: 78, batch train loss: 0.89, train acc: 65.26%, consuming tine: 4.50\n",
      "batch: 79, batch train loss: 0.89, train acc: 65.28%, consuming tine: 4.66\n",
      "batch: 80, batch train loss: 0.89, train acc: 65.28%, consuming tine: 4.79\n",
      "batch: 81, batch train loss: 0.89, train acc: 65.34%, consuming tine: 4.89\n",
      "batch: 82, batch train loss: 0.89, train acc: 65.35%, consuming tine: 4.99\n",
      "batch: 83, batch train loss: 0.89, train acc: 65.37%, consuming tine: 4.89\n",
      "batch: 84, batch train loss: 0.89, train acc: 65.41%, consuming tine: 4.78\n",
      "batch: 85, batch train loss: 0.89, train acc: 65.44%, consuming tine: 4.49\n",
      "batch: 86, batch train loss: 0.89, train acc: 65.46%, consuming tine: 5.43\n",
      "batch: 87, batch train loss: 0.89, train acc: 65.47%, consuming tine: 4.85\n",
      "batch: 88, batch train loss: 0.89, train acc: 65.49%, consuming tine: 4.81\n",
      "batch: 89, batch train loss: 0.88, train acc: 65.50%, consuming tine: 5.02\n",
      "batch: 90, batch train loss: 0.88, train acc: 65.54%, consuming tine: 5.02\n",
      "batch: 91, batch train loss: 0.88, train acc: 65.58%, consuming tine: 4.91\n",
      "batch: 92, batch train loss: 0.88, train acc: 65.61%, consuming tine: 4.76\n",
      "batch: 93, batch train loss: 0.88, train acc: 65.64%, consuming tine: 6.41\n",
      "batch: 94, batch train loss: 0.88, train acc: 65.68%, consuming tine: 4.77\n",
      "batch: 95, batch train loss: 0.88, train acc: 65.71%, consuming tine: 4.70\n",
      "batch: 96, batch train loss: 0.88, train acc: 65.72%, consuming tine: 4.68\n",
      "batch: 97, batch train loss: 0.88, train acc: 65.73%, consuming tine: 4.79\n",
      "batch: 98, batch train loss: 0.88, train acc: 65.73%, consuming tine: 5.07\n",
      "batch: 99, batch train loss: 0.88, train acc: 65.76%, consuming tine: 4.69\n",
      "batch: 100, batch train loss: 0.88, train acc: 65.74%, consuming tine: 4.90\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 1.87, valid acc: 38.13%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.88, train acc: 65.76%, consuming tine: 4.59\n",
      "batch: 102, batch train loss: 0.88, train acc: 65.76%, consuming tine: 4.88\n",
      "batch: 103, batch train loss: 0.88, train acc: 65.74%, consuming tine: 4.70\n",
      "batch: 104, batch train loss: 0.88, train acc: 65.73%, consuming tine: 4.59\n",
      "batch: 105, batch train loss: 0.87, train acc: 65.77%, consuming tine: 4.87\n",
      "batch: 106, batch train loss: 0.87, train acc: 65.80%, consuming tine: 4.68\n",
      "batch: 107, batch train loss: 0.87, train acc: 65.77%, consuming tine: 4.60\n",
      "batch: 108, batch train loss: 0.87, train acc: 65.75%, consuming tine: 4.69\n",
      "batch: 109, batch train loss: 0.87, train acc: 65.74%, consuming tine: 4.39\n",
      "batch: 110, batch train loss: 0.87, train acc: 65.76%, consuming tine: 4.78\n",
      "batch: 111, batch train loss: 0.87, train acc: 65.77%, consuming tine: 4.99\n",
      "batch: 112, batch train loss: 0.87, train acc: 65.77%, consuming tine: 4.38\n",
      "batch: 113, batch train loss: 0.87, train acc: 65.81%, consuming tine: 4.81\n",
      "batch: 114, batch train loss: 0.87, train acc: 65.84%, consuming tine: 4.70\n",
      "batch: 115, batch train loss: 0.87, train acc: 65.87%, consuming tine: 4.79\n",
      "batch: 116, batch train loss: 0.87, train acc: 65.90%, consuming tine: 4.45\n",
      "batch: 117, batch train loss: 0.87, train acc: 65.92%, consuming tine: 5.20\n",
      "batch: 118, batch train loss: 0.87, train acc: 65.94%, consuming tine: 4.80\n",
      "batch: 119, batch train loss: 0.87, train acc: 65.97%, consuming tine: 4.67\n",
      "batch: 120, batch train loss: 0.87, train acc: 66.00%, consuming tine: 4.69\n",
      "batch: 121, batch train loss: 0.87, train acc: 66.03%, consuming tine: 5.48\n",
      "batch: 122, batch train loss: 0.87, train acc: 66.08%, consuming tine: 4.88\n",
      "batch: 123, batch train loss: 0.87, train acc: 66.11%, consuming tine: 4.68\n",
      "batch: 124, batch train loss: 0.87, train acc: 66.13%, consuming tine: 4.59\n",
      "batch: 125, batch train loss: 0.87, train acc: 66.14%, consuming tine: 4.49\n",
      "batch: 126, batch train loss: 0.86, train acc: 66.17%, consuming tine: 4.78\n",
      "batch: 127, batch train loss: 0.86, train acc: 66.20%, consuming tine: 4.50\n",
      "batch: 128, batch train loss: 0.86, train acc: 66.18%, consuming tine: 4.68\n",
      "batch: 129, batch train loss: 0.86, train acc: 66.19%, consuming tine: 4.69\n",
      "batch: 130, batch train loss: 0.86, train acc: 66.22%, consuming tine: 4.67\n",
      "batch: 131, batch train loss: 0.86, train acc: 66.22%, consuming tine: 4.89\n",
      "batch: 132, batch train loss: 0.86, train acc: 66.22%, consuming tine: 4.60\n",
      "batch: 133, batch train loss: 0.86, train acc: 66.26%, consuming tine: 4.68\n",
      "batch: 134, batch train loss: 0.86, train acc: 66.28%, consuming tine: 4.89\n",
      "batch: 135, batch train loss: 0.86, train acc: 66.28%, consuming tine: 4.60\n",
      "batch: 136, batch train loss: 0.86, train acc: 66.28%, consuming tine: 4.58\n",
      "batch: 137, batch train loss: 0.86, train acc: 66.29%, consuming tine: 4.49\n",
      "batch: 138, batch train loss: 0.86, train acc: 66.32%, consuming tine: 4.81\n",
      "batch: 139, batch train loss: 0.86, train acc: 66.35%, consuming tine: 4.58\n",
      "batch: 140, batch train loss: 0.86, train acc: 66.35%, consuming tine: 5.08\n",
      "batch: 141, batch train loss: 0.86, train acc: 66.35%, consuming tine: 4.99\n",
      "batch: 142, batch train loss: 0.86, train acc: 66.37%, consuming tine: 4.60\n",
      "batch: 143, batch train loss: 0.86, train acc: 66.40%, consuming tine: 4.58\n",
      "batch: 144, batch train loss: 0.86, train acc: 66.44%, consuming tine: 4.80\n",
      "batch: 145, batch train loss: 0.86, train acc: 66.45%, consuming tine: 4.68\n",
      "batch: 146, batch train loss: 0.86, train acc: 66.45%, consuming tine: 4.59\n",
      "batch: 147, batch train loss: 0.85, train acc: 66.49%, consuming tine: 5.10\n",
      "batch: 148, batch train loss: 0.85, train acc: 66.54%, consuming tine: 4.67\n",
      "batch: 149, batch train loss: 0.85, train acc: 66.56%, consuming tine: 4.58\n",
      "batch: 150, batch train loss: 0.85, train acc: 66.58%, consuming tine: 4.60\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 1.89, valid acc: 38.05%\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 151, batch train loss: 0.85, train acc: 66.62%, consuming tine: 4.70\n",
      "batch: 152, batch train loss: 0.85, train acc: 66.63%, consuming tine: 4.57\n",
      "batch: 153, batch train loss: 0.85, train acc: 66.65%, consuming tine: 4.70\n",
      "batch: 154, batch train loss: 0.85, train acc: 66.68%, consuming tine: 4.78\n",
      "batch: 155, batch train loss: 0.85, train acc: 66.72%, consuming tine: 4.69\n",
      "batch: 156, batch train loss: 0.85, train acc: 66.73%, consuming tine: 4.59\n",
      "batch: 157, batch train loss: 0.85, train acc: 66.74%, consuming tine: 4.70\n",
      "batch: 158, batch train loss: 0.85, train acc: 66.78%, consuming tine: 4.48\n",
      "batch: 159, batch train loss: 0.85, train acc: 66.81%, consuming tine: 4.59\n",
      "batch: 160, batch train loss: 0.85, train acc: 66.84%, consuming tine: 4.70\n",
      "batch: 161, batch train loss: 0.85, train acc: 66.85%, consuming tine: 4.57\n",
      "batch: 162, batch train loss: 0.85, train acc: 66.84%, consuming tine: 4.89\n",
      "batch: 163, batch train loss: 0.85, train acc: 66.85%, consuming tine: 4.78\n",
      "batch: 164, batch train loss: 0.84, train acc: 66.89%, consuming tine: 4.59\n",
      "batch: 165, batch train loss: 0.84, train acc: 66.90%, consuming tine: 4.49\n",
      "batch: 166, batch train loss: 0.84, train acc: 66.88%, consuming tine: 4.58\n",
      "batch: 167, batch train loss: 0.84, train acc: 66.88%, consuming tine: 4.60\n",
      "batch: 168, batch train loss: 0.84, train acc: 66.91%, consuming tine: 4.38\n",
      "batch: 169, batch train loss: 0.84, train acc: 66.93%, consuming tine: 4.59\n",
      "batch: 170, batch train loss: 0.84, train acc: 66.93%, consuming tine: 5.09\n",
      "batch: 171, batch train loss: 0.84, train acc: 66.96%, consuming tine: 4.41\n",
      "batch: 172, batch train loss: 0.84, train acc: 66.97%, consuming tine: 4.78\n",
      "batch: 173, batch train loss: 0.84, train acc: 66.99%, consuming tine: 4.68\n",
      "batch: 174, batch train loss: 0.84, train acc: 67.00%, consuming tine: 4.69\n",
      "batch: 175, batch train loss: 0.84, train acc: 67.02%, consuming tine: 4.69\n",
      "batch: 176, batch train loss: 0.84, train acc: 67.05%, consuming tine: 4.68\n",
      "batch: 177, batch train loss: 0.84, train acc: 67.09%, consuming tine: 4.59\n",
      "batch: 178, batch train loss: 0.84, train acc: 67.12%, consuming tine: 4.69\n",
      "batch: 179, batch train loss: 0.84, train acc: 67.14%, consuming tine: 4.69\n",
      "batch: 180, batch train loss: 0.84, train acc: 67.15%, consuming tine: 4.49\n",
      "batch: 181, batch train loss: 0.84, train acc: 67.18%, consuming tine: 4.48\n",
      "batch: 182, batch train loss: 0.83, train acc: 67.20%, consuming tine: 4.69\n",
      "batch: 183, batch train loss: 0.83, train acc: 67.23%, consuming tine: 4.59\n",
      "batch: 184, batch train loss: 0.83, train acc: 67.27%, consuming tine: 4.59\n",
      "batch: 185, batch train loss: 0.83, train acc: 67.28%, consuming tine: 4.59\n",
      "batch: 186, batch train loss: 0.83, train acc: 67.32%, consuming tine: 4.59\n",
      "batch: 187, batch train loss: 0.83, train acc: 67.36%, consuming tine: 4.79\n",
      "batch: 188, batch train loss: 0.83, train acc: 67.41%, consuming tine: 4.58\n",
      "batch: 189, batch train loss: 0.83, train acc: 67.44%, consuming tine: 4.89\n",
      "batch: 190, batch train loss: 0.83, train acc: 67.45%, consuming tine: 4.79\n",
      "batch: 191, batch train loss: 0.83, train acc: 67.47%, consuming tine: 4.77\n",
      "batch: 192, batch train loss: 0.83, train acc: 67.49%, consuming tine: 4.59\n",
      "batch: 193, batch train loss: 0.83, train acc: 67.53%, consuming tine: 4.64\n",
      "batch: 194, batch train loss: 0.83, train acc: 67.54%, consuming tine: 4.64\n",
      "batch: 195, batch train loss: 0.83, train acc: 67.57%, consuming tine: 4.58\n",
      "batch: 196, batch train loss: 0.83, train acc: 67.61%, consuming tine: 4.60\n",
      "batch: 197, batch train loss: 0.82, train acc: 67.64%, consuming tine: 4.79\n",
      "batch: 198, batch train loss: 0.82, train acc: 67.69%, consuming tine: 4.78\n",
      "batch: 199, batch train loss: 0.82, train acc: 67.72%, consuming tine: 4.69\n",
      "batch: 200, batch train loss: 0.82, train acc: 67.74%, consuming tine: 4.69\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 1.93, valid acc: 38.00%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.82, train acc: 67.75%, consuming tine: 4.71\n",
      "batch: 202, batch train loss: 0.82, train acc: 67.78%, consuming tine: 4.80\n",
      "batch: 203, batch train loss: 0.82, train acc: 67.80%, consuming tine: 4.60\n",
      "batch: 204, batch train loss: 0.82, train acc: 67.83%, consuming tine: 4.78\n",
      "batch: 205, batch train loss: 0.82, train acc: 67.87%, consuming tine: 4.68\n",
      "batch: 206, batch train loss: 0.82, train acc: 67.88%, consuming tine: 4.59\n",
      "batch: 207, batch train loss: 0.82, train acc: 67.91%, consuming tine: 4.49\n",
      "batch: 208, batch train loss: 0.82, train acc: 67.93%, consuming tine: 5.09\n",
      "batch: 209, batch train loss: 0.82, train acc: 67.96%, consuming tine: 4.69\n",
      "batch: 210, batch train loss: 0.82, train acc: 67.96%, consuming tine: 4.78\n",
      "batch: 211, batch train loss: 0.82, train acc: 67.95%, consuming tine: 4.79\n",
      "batch: 212, batch train loss: 0.82, train acc: 67.96%, consuming tine: 4.80\n",
      "batch: 213, batch train loss: 0.82, train acc: 67.98%, consuming tine: 4.69\n",
      "batch: 214, batch train loss: 0.82, train acc: 67.98%, consuming tine: 4.88\n",
      "batch: 215, batch train loss: 0.82, train acc: 67.98%, consuming tine: 4.40\n",
      "batch: 216, batch train loss: 0.82, train acc: 67.98%, consuming tine: 5.08\n",
      "batch: 217, batch train loss: 0.82, train acc: 67.98%, consuming tine: 5.01\n",
      "batch: 218, batch train loss: 0.82, train acc: 67.99%, consuming tine: 4.48\n",
      "batch: 219, batch train loss: 0.82, train acc: 68.01%, consuming tine: 4.96\n",
      "batch: 220, batch train loss: 0.82, train acc: 68.02%, consuming tine: 4.69\n",
      "batch: 221, batch train loss: 0.82, train acc: 68.02%, consuming tine: 4.69\n",
      "batch: 222, batch train loss: 0.82, train acc: 67.99%, consuming tine: 4.71\n",
      "batch: 223, batch train loss: 0.82, train acc: 67.99%, consuming tine: 4.67\n",
      "batch: 224, batch train loss: 0.82, train acc: 67.99%, consuming tine: 4.71\n",
      "batch: 225, batch train loss: 0.82, train acc: 68.00%, consuming tine: 4.77\n",
      "batch: 226, batch train loss: 0.82, train acc: 67.97%, consuming tine: 4.56\n",
      "batch: 227, batch train loss: 0.82, train acc: 67.94%, consuming tine: 4.73\n",
      "batch: 228, batch train loss: 0.82, train acc: 67.93%, consuming tine: 4.47\n",
      "batch: 229, batch train loss: 0.82, train acc: 67.94%, consuming tine: 4.68\n",
      "batch: 230, batch train loss: 0.82, train acc: 67.95%, consuming tine: 4.79\n",
      "batch: 231, batch train loss: 0.82, train acc: 67.95%, consuming tine: 4.60\n",
      "batch: 232, batch train loss: 0.82, train acc: 67.95%, consuming tine: 4.91\n",
      "batch: 233, batch train loss: 0.82, train acc: 67.96%, consuming tine: 4.58\n",
      "batch: 234, batch train loss: 0.82, train acc: 67.98%, consuming tine: 4.97\n",
      "batch: 235, batch train loss: 0.82, train acc: 67.99%, consuming tine: 4.65\n",
      "batch: 236, batch train loss: 0.82, train acc: 68.01%, consuming tine: 4.82\n",
      "batch: 237, batch train loss: 0.82, train acc: 68.04%, consuming tine: 4.69\n",
      "batch: 238, batch train loss: 0.82, train acc: 68.06%, consuming tine: 4.91\n",
      "batch: 239, batch train loss: 0.82, train acc: 68.08%, consuming tine: 4.45\n",
      "batch: 240, batch train loss: 0.81, train acc: 68.10%, consuming tine: 4.79\n",
      "batch: 241, batch train loss: 0.81, train acc: 68.12%, consuming tine: 4.69\n",
      "batch: 242, batch train loss: 0.81, train acc: 68.14%, consuming tine: 4.68\n",
      "batch: 243, batch train loss: 0.81, train acc: 68.15%, consuming tine: 5.02\n",
      "batch: 244, batch train loss: 0.81, train acc: 68.17%, consuming tine: 4.57\n",
      "batch: 245, batch train loss: 0.81, train acc: 68.19%, consuming tine: 4.21\n",
      "batch: 246, batch train loss: 0.81, train acc: 68.20%, consuming tine: 4.96\n",
      "batch: 247, batch train loss: 0.81, train acc: 68.22%, consuming tine: 4.59\n",
      "batch: 248, batch train loss: 0.81, train acc: 68.24%, consuming tine: 4.68\n",
      "batch: 249, batch train loss: 0.81, train acc: 68.26%, consuming tine: 4.51\n",
      "batch: 250, batch train loss: 0.81, train acc: 68.28%, consuming tine: 4.48\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 1.95, valid acc: 37.83%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.81, train acc: 68.30%, consuming tine: 4.59\n",
      "batch: 252, batch train loss: 0.81, train acc: 68.31%, consuming tine: 4.59\n",
      "batch: 253, batch train loss: 0.81, train acc: 68.34%, consuming tine: 4.68\n",
      "batch: 254, batch train loss: 0.81, train acc: 68.36%, consuming tine: 4.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 255, batch train loss: 0.81, train acc: 68.39%, consuming tine: 4.48\n",
      "batch: 256, batch train loss: 0.81, train acc: 68.40%, consuming tine: 4.51\n",
      "batch: 257, batch train loss: 0.81, train acc: 68.42%, consuming tine: 4.78\n",
      "batch: 258, batch train loss: 0.81, train acc: 68.44%, consuming tine: 4.75\n",
      "batch: 259, batch train loss: 0.81, train acc: 68.45%, consuming tine: 4.53\n",
      "batch: 260, batch train loss: 0.81, train acc: 68.48%, consuming tine: 4.98\n",
      "batch: 261, batch train loss: 0.81, train acc: 68.51%, consuming tine: 4.59\n",
      "batch: 262, batch train loss: 0.81, train acc: 68.53%, consuming tine: 4.59\n",
      "batch: 263, batch train loss: 0.80, train acc: 68.56%, consuming tine: 4.80\n",
      "batch: 264, batch train loss: 0.80, train acc: 68.57%, consuming tine: 4.89\n",
      "batch: 265, batch train loss: 0.80, train acc: 68.59%, consuming tine: 4.48\n",
      "batch: 266, batch train loss: 0.80, train acc: 68.61%, consuming tine: 4.89\n",
      "batch: 267, batch train loss: 0.80, train acc: 68.61%, consuming tine: 4.57\n",
      "batch: 268, batch train loss: 0.80, train acc: 68.63%, consuming tine: 4.49\n",
      "batch: 269, batch train loss: 0.80, train acc: 68.65%, consuming tine: 4.92\n",
      "batch: 270, batch train loss: 0.80, train acc: 68.67%, consuming tine: 4.47\n",
      "batch: 271, batch train loss: 0.80, train acc: 68.69%, consuming tine: 4.49\n",
      "batch: 272, batch train loss: 0.80, train acc: 68.71%, consuming tine: 4.80\n",
      "batch: 273, batch train loss: 0.80, train acc: 68.73%, consuming tine: 4.56\n",
      "batch: 274, batch train loss: 0.80, train acc: 68.75%, consuming tine: 4.49\n",
      "batch: 275, batch train loss: 0.80, train acc: 68.77%, consuming tine: 4.70\n",
      "batch: 276, batch train loss: 0.80, train acc: 68.79%, consuming tine: 4.39\n",
      "batch: 277, batch train loss: 0.80, train acc: 68.81%, consuming tine: 4.28\n",
      "batch: 278, batch train loss: 0.80, train acc: 68.84%, consuming tine: 4.70\n",
      "batch: 279, batch train loss: 0.80, train acc: 68.86%, consuming tine: 4.52\n",
      "batch: 280, batch train loss: 0.80, train acc: 68.88%, consuming tine: 4.45\n",
      "batch: 281, batch train loss: 0.80, train acc: 68.89%, consuming tine: 4.58\n",
      "batch: 282, batch train loss: 0.80, train acc: 68.91%, consuming tine: 4.69\n",
      "batch: 283, batch train loss: 0.80, train acc: 68.93%, consuming tine: 4.69\n",
      "batch: 284, batch train loss: 0.80, train acc: 68.96%, consuming tine: 4.49\n",
      "batch: 285, batch train loss: 0.80, train acc: 68.97%, consuming tine: 5.11\n",
      "batch: 286, batch train loss: 0.79, train acc: 68.97%, consuming tine: 4.57\n",
      "batch: 287, batch train loss: 0.79, train acc: 68.98%, consuming tine: 4.49\n",
      "batch: 288, batch train loss: 0.79, train acc: 68.99%, consuming tine: 4.89\n",
      "batch: 289, batch train loss: 0.79, train acc: 69.00%, consuming tine: 4.40\n",
      "batch: 290, batch train loss: 0.79, train acc: 69.01%, consuming tine: 4.58\n",
      "batch: 291, batch train loss: 0.79, train acc: 69.02%, consuming tine: 4.50\n",
      "batch: 292, batch train loss: 0.79, train acc: 69.04%, consuming tine: 4.59\n",
      "batch: 293, batch train loss: 0.79, train acc: 69.06%, consuming tine: 4.68\n",
      "batch: 294, batch train loss: 0.79, train acc: 69.08%, consuming tine: 4.69\n",
      "batch: 295, batch train loss: 0.79, train acc: 69.10%, consuming tine: 4.98\n",
      "batch: 296, batch train loss: 0.79, train acc: 69.13%, consuming tine: 4.52\n",
      "batch: 297, batch train loss: 0.79, train acc: 69.15%, consuming tine: 4.58\n",
      "batch: 298, batch train loss: 0.79, train acc: 69.17%, consuming tine: 4.56\n",
      "batch: 299, batch train loss: 0.79, train acc: 69.19%, consuming tine: 4.61\n",
      "batch: 300, batch train loss: 0.79, train acc: 69.23%, consuming tine: 4.49\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 1.97, valid acc: 37.82%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.79, train acc: 69.24%, consuming tine: 4.49\n",
      "batch: 302, batch train loss: 0.79, train acc: 69.26%, consuming tine: 4.88\n",
      "batch: 303, batch train loss: 0.79, train acc: 69.27%, consuming tine: 4.68\n",
      "batch: 304, batch train loss: 0.79, train acc: 69.28%, consuming tine: 4.69\n",
      "batch: 305, batch train loss: 0.79, train acc: 69.30%, consuming tine: 5.00\n",
      "batch: 306, batch train loss: 0.79, train acc: 69.32%, consuming tine: 4.67\n",
      "batch: 307, batch train loss: 0.79, train acc: 69.33%, consuming tine: 4.80\n",
      "batch: 308, batch train loss: 0.79, train acc: 69.33%, consuming tine: 4.70\n",
      "batch: 309, batch train loss: 0.79, train acc: 69.31%, consuming tine: 4.56\n",
      "batch: 310, batch train loss: 0.79, train acc: 69.32%, consuming tine: 4.70\n",
      "batch: 311, batch train loss: 0.79, train acc: 69.34%, consuming tine: 4.49\n",
      "batch: 312, batch train loss: 0.79, train acc: 69.35%, consuming tine: 4.48\n",
      "batch: 313, batch train loss: 0.79, train acc: 69.35%, consuming tine: 4.70\n",
      "batch: 314, batch train loss: 0.79, train acc: 69.36%, consuming tine: 4.46\n",
      "batch: 315, batch train loss: 0.79, train acc: 69.37%, consuming tine: 4.59\n",
      "batch: 316, batch train loss: 0.79, train acc: 69.39%, consuming tine: 4.60\n",
      "batch: 317, batch train loss: 0.78, train acc: 69.42%, consuming tine: 4.77\n",
      "batch: 318, batch train loss: 0.78, train acc: 69.42%, consuming tine: 4.40\n",
      "batch: 319, batch train loss: 0.78, train acc: 69.43%, consuming tine: 4.79\n",
      "batch: 320, batch train loss: 0.78, train acc: 69.44%, consuming tine: 4.68\n",
      "batch: 321, batch train loss: 0.78, train acc: 69.46%, consuming tine: 4.59\n",
      "batch: 322, batch train loss: 0.78, train acc: 69.47%, consuming tine: 4.80\n",
      "batch: 323, batch train loss: 0.78, train acc: 69.49%, consuming tine: 4.57\n",
      "batch: 324, batch train loss: 0.78, train acc: 69.51%, consuming tine: 4.60\n",
      "batch: 325, batch train loss: 0.78, train acc: 69.53%, consuming tine: 4.67\n",
      "batch: 326, batch train loss: 0.78, train acc: 69.55%, consuming tine: 4.89\n",
      "batch: 327, batch train loss: 0.78, train acc: 69.58%, consuming tine: 4.60\n",
      "batch: 328, batch train loss: 0.78, train acc: 69.61%, consuming tine: 4.78\n",
      "batch: 329, batch train loss: 0.78, train acc: 69.63%, consuming tine: 4.48\n",
      "batch: 330, batch train loss: 0.78, train acc: 69.66%, consuming tine: 4.49\n",
      "batch: 331, batch train loss: 0.78, train acc: 69.67%, consuming tine: 4.99\n",
      "batch: 332, batch train loss: 0.78, train acc: 69.70%, consuming tine: 4.68\n",
      "batch: 333, batch train loss: 0.78, train acc: 69.71%, consuming tine: 4.79\n",
      "batch: 334, batch train loss: 0.78, train acc: 69.73%, consuming tine: 4.98\n",
      "batch: 335, batch train loss: 0.78, train acc: 69.75%, consuming tine: 4.49\n",
      "batch: 336, batch train loss: 0.78, train acc: 69.77%, consuming tine: 4.60\n",
      "batch: 337, batch train loss: 0.78, train acc: 69.79%, consuming tine: 4.69\n",
      "batch: 338, batch train loss: 0.78, train acc: 69.81%, consuming tine: 4.48\n",
      "batch: 339, batch train loss: 0.78, train acc: 69.84%, consuming tine: 4.59\n",
      "batch: 340, batch train loss: 0.78, train acc: 69.86%, consuming tine: 4.99\n",
      "batch: 341, batch train loss: 0.77, train acc: 69.88%, consuming tine: 4.38\n",
      "batch: 342, batch train loss: 0.77, train acc: 69.89%, consuming tine: 5.09\n",
      "batch: 343, batch train loss: 0.77, train acc: 69.91%, consuming tine: 4.78\n",
      "batch: 344, batch train loss: 0.77, train acc: 69.93%, consuming tine: 4.68\n",
      "batch: 345, batch train loss: 0.77, train acc: 69.96%, consuming tine: 4.59\n",
      "batch: 346, batch train loss: 0.77, train acc: 69.98%, consuming tine: 4.68\n",
      "batch: 347, batch train loss: 0.77, train acc: 70.01%, consuming tine: 4.59\n",
      "batch: 348, batch train loss: 0.77, train acc: 70.03%, consuming tine: 4.50\n",
      "batch: 349, batch train loss: 0.77, train acc: 70.05%, consuming tine: 4.88\n",
      "batch: 350, batch train loss: 0.77, train acc: 70.08%, consuming tine: 4.49\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 1.99, valid acc: 37.76%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.77, train acc: 70.10%, consuming tine: 4.51\n",
      "batch: 352, batch train loss: 0.77, train acc: 70.12%, consuming tine: 4.58\n",
      "batch: 353, batch train loss: 0.77, train acc: 70.14%, consuming tine: 4.48\n",
      "batch: 354, batch train loss: 0.77, train acc: 70.17%, consuming tine: 4.59\n",
      "batch: 355, batch train loss: 0.77, train acc: 70.18%, consuming tine: 4.80\n",
      "batch: 356, batch train loss: 0.77, train acc: 70.20%, consuming tine: 4.49\n",
      "batch: 357, batch train loss: 0.77, train acc: 70.22%, consuming tine: 4.69\n",
      "batch: 358, batch train loss: 0.77, train acc: 70.24%, consuming tine: 4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 359, batch train loss: 0.77, train acc: 70.26%, consuming tine: 4.53\n",
      "batch: 360, batch train loss: 0.77, train acc: 70.28%, consuming tine: 4.77\n",
      "batch: 361, batch train loss: 0.77, train acc: 70.30%, consuming tine: 4.48\n",
      "batch: 362, batch train loss: 0.77, train acc: 70.32%, consuming tine: 4.47\n",
      "batch: 363, batch train loss: 0.76, train acc: 70.33%, consuming tine: 4.69\n",
      "batch: 364, batch train loss: 0.76, train acc: 70.36%, consuming tine: 4.69\n",
      "batch: 365, batch train loss: 0.76, train acc: 70.38%, consuming tine: 4.79\n",
      "batch: 366, batch train loss: 0.76, train acc: 70.40%, consuming tine: 4.68\n",
      "batch: 367, batch train loss: 0.76, train acc: 70.42%, consuming tine: 4.70\n",
      "batch: 368, batch train loss: 0.76, train acc: 70.44%, consuming tine: 4.83\n",
      "batch: 369, batch train loss: 0.76, train acc: 70.46%, consuming tine: 4.43\n",
      "batch: 370, batch train loss: 0.76, train acc: 70.49%, consuming tine: 4.69\n",
      "batch: 371, batch train loss: 0.76, train acc: 70.51%, consuming tine: 4.78\n",
      "batch: 372, batch train loss: 0.76, train acc: 70.53%, consuming tine: 4.71\n",
      "batch: 373, batch train loss: 0.76, train acc: 70.56%, consuming tine: 4.57\n",
      "batch: 374, batch train loss: 0.76, train acc: 70.58%, consuming tine: 4.60\n",
      "batch: 375, batch train loss: 0.76, train acc: 70.60%, consuming tine: 4.58\n",
      "batch: 376, batch train loss: 0.76, train acc: 70.62%, consuming tine: 4.59\n",
      "batch: 377, batch train loss: 0.76, train acc: 70.64%, consuming tine: 4.60\n",
      "batch: 378, batch train loss: 0.76, train acc: 70.66%, consuming tine: 4.47\n",
      "batch: 379, batch train loss: 0.76, train acc: 70.69%, consuming tine: 4.49\n",
      "batch: 380, batch train loss: 0.76, train acc: 70.71%, consuming tine: 4.29\n",
      "batch: 381, batch train loss: 0.76, train acc: 70.74%, consuming tine: 4.78\n",
      "batch: 382, batch train loss: 0.76, train acc: 70.76%, consuming tine: 4.60\n",
      "batch: 383, batch train loss: 0.76, train acc: 70.78%, consuming tine: 4.78\n",
      "batch: 384, batch train loss: 0.75, train acc: 70.80%, consuming tine: 4.73\n",
      "batch: 385, batch train loss: 0.75, train acc: 70.83%, consuming tine: 4.56\n",
      "batch: 386, batch train loss: 0.75, train acc: 70.84%, consuming tine: 4.70\n",
      "batch: 387, batch train loss: 0.75, train acc: 70.85%, consuming tine: 4.65\n",
      "batch: 388, batch train loss: 0.75, train acc: 70.87%, consuming tine: 4.91\n",
      "batch: 389, batch train loss: 0.75, train acc: 70.90%, consuming tine: 4.58\n",
      "batch: 390, batch train loss: 0.75, train acc: 70.92%, consuming tine: 4.48\n",
      "batch: 391, batch train loss: 0.75, train acc: 70.93%, consuming tine: 4.69\n",
      "batch: 392, batch train loss: 0.75, train acc: 70.95%, consuming tine: 4.60\n",
      "batch: 393, batch train loss: 0.75, train acc: 70.97%, consuming tine: 4.59\n",
      "batch: 394, batch train loss: 0.75, train acc: 70.99%, consuming tine: 4.79\n",
      "batch: 395, batch train loss: 0.75, train acc: 71.01%, consuming tine: 4.59\n",
      "batch: 396, batch train loss: 0.75, train acc: 71.03%, consuming tine: 4.60\n",
      "batch: 397, batch train loss: 0.75, train acc: 71.05%, consuming tine: 4.69\n",
      "batch: 398, batch train loss: 0.75, train acc: 71.07%, consuming tine: 4.87\n",
      "batch: 399, batch train loss: 0.75, train acc: 71.09%, consuming tine: 4.69\n",
      "batch: 400, batch train loss: 0.75, train acc: 71.12%, consuming tine: 4.39\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 2.02, valid acc: 37.71%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.75, train acc: 71.14%, consuming tine: 4.71\n",
      "batch: 402, batch train loss: 0.75, train acc: 71.15%, consuming tine: 4.78\n",
      "batch: 403, batch train loss: 0.75, train acc: 71.17%, consuming tine: 4.60\n",
      "batch: 404, batch train loss: 0.75, train acc: 71.18%, consuming tine: 4.78\n",
      "batch: 405, batch train loss: 0.75, train acc: 71.21%, consuming tine: 4.88\n",
      "batch: 406, batch train loss: 0.74, train acc: 71.23%, consuming tine: 4.39\n",
      "batch: 407, batch train loss: 0.74, train acc: 71.25%, consuming tine: 4.48\n",
      "batch: 408, batch train loss: 0.74, train acc: 71.28%, consuming tine: 4.69\n",
      "batch: 409, batch train loss: 0.74, train acc: 71.29%, consuming tine: 4.60\n",
      "batch: 410, batch train loss: 0.74, train acc: 71.31%, consuming tine: 4.50\n",
      "batch: 411, batch train loss: 0.74, train acc: 71.33%, consuming tine: 4.67\n",
      "batch: 412, batch train loss: 0.74, train acc: 71.35%, consuming tine: 4.78\n",
      "batch: 413, batch train loss: 0.74, train acc: 71.37%, consuming tine: 4.49\n",
      "batch: 414, batch train loss: 0.74, train acc: 71.39%, consuming tine: 4.61\n",
      "batch: 415, batch train loss: 0.74, train acc: 71.41%, consuming tine: 4.67\n",
      "batch: 416, batch train loss: 0.74, train acc: 71.43%, consuming tine: 4.59\n",
      "batch: 417, batch train loss: 0.74, train acc: 71.45%, consuming tine: 4.59\n",
      "batch: 418, batch train loss: 0.74, train acc: 71.46%, consuming tine: 4.68\n",
      "batch: 419, batch train loss: 0.74, train acc: 71.48%, consuming tine: 4.80\n",
      "batch: 420, batch train loss: 0.74, train acc: 71.50%, consuming tine: 4.59\n",
      "batch: 421, batch train loss: 0.74, train acc: 71.51%, consuming tine: 4.48\n",
      "batch: 422, batch train loss: 0.74, train acc: 71.53%, consuming tine: 4.89\n",
      "batch: 423, batch train loss: 0.74, train acc: 71.54%, consuming tine: 4.54\n",
      "batch: 424, batch train loss: 0.74, train acc: 71.56%, consuming tine: 4.65\n",
      "batch: 425, batch train loss: 0.74, train acc: 71.58%, consuming tine: 4.87\n",
      "batch: 426, batch train loss: 0.74, train acc: 71.60%, consuming tine: 4.51\n",
      "batch: 427, batch train loss: 0.74, train acc: 71.61%, consuming tine: 4.38\n",
      "batch: 428, batch train loss: 0.74, train acc: 71.62%, consuming tine: 4.89\n",
      "batch: 429, batch train loss: 0.74, train acc: 71.64%, consuming tine: 4.69\n",
      "batch: 430, batch train loss: 0.73, train acc: 71.66%, consuming tine: 4.58\n",
      "batch: 431, batch train loss: 0.73, train acc: 71.67%, consuming tine: 4.79\n",
      "batch: 432, batch train loss: 0.73, train acc: 71.69%, consuming tine: 4.68\n",
      "batch: 433, batch train loss: 0.73, train acc: 71.70%, consuming tine: 4.51\n",
      "batch: 434, batch train loss: 0.73, train acc: 71.72%, consuming tine: 4.97\n",
      "batch: 435, batch train loss: 0.73, train acc: 71.73%, consuming tine: 4.78\n",
      "batch: 436, batch train loss: 0.73, train acc: 71.75%, consuming tine: 4.81\n",
      "batch: 437, batch train loss: 0.73, train acc: 71.77%, consuming tine: 4.67\n",
      "batch: 438, batch train loss: 0.73, train acc: 71.78%, consuming tine: 4.70\n",
      "batch: 439, batch train loss: 0.73, train acc: 71.79%, consuming tine: 4.57\n",
      "batch: 440, batch train loss: 0.73, train acc: 71.81%, consuming tine: 4.71\n",
      "batch: 441, batch train loss: 0.73, train acc: 71.82%, consuming tine: 4.87\n",
      "batch: 442, batch train loss: 0.73, train acc: 71.83%, consuming tine: 4.70\n",
      "batch: 443, batch train loss: 0.73, train acc: 71.86%, consuming tine: 4.87\n",
      "batch: 444, batch train loss: 0.73, train acc: 71.87%, consuming tine: 4.79\n",
      "batch: 445, batch train loss: 0.73, train acc: 71.89%, consuming tine: 4.79\n",
      "batch: 446, batch train loss: 0.73, train acc: 71.90%, consuming tine: 4.99\n",
      "batch: 447, batch train loss: 0.73, train acc: 71.92%, consuming tine: 4.44\n",
      "batch: 448, batch train loss: 0.73, train acc: 71.94%, consuming tine: 4.63\n",
      "batch: 449, batch train loss: 0.73, train acc: 71.96%, consuming tine: 4.49\n",
      "batch: 450, batch train loss: 0.73, train acc: 71.98%, consuming tine: 4.79\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 2.04, valid acc: 37.67%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.73, train acc: 71.99%, consuming tine: 4.53\n",
      "batch: 452, batch train loss: 0.73, train acc: 72.01%, consuming tine: 4.58\n",
      "batch: 453, batch train loss: 0.73, train acc: 72.04%, consuming tine: 4.98\n",
      "batch: 454, batch train loss: 0.73, train acc: 72.06%, consuming tine: 4.50\n",
      "batch: 455, batch train loss: 0.72, train acc: 72.08%, consuming tine: 4.71\n",
      "batch: 456, batch train loss: 0.72, train acc: 72.10%, consuming tine: 4.88\n",
      "batch: 457, batch train loss: 0.72, train acc: 72.12%, consuming tine: 4.84\n",
      "batch: 458, batch train loss: 0.72, train acc: 72.13%, consuming tine: 4.74\n",
      "batch: 459, batch train loss: 0.72, train acc: 72.15%, consuming tine: 4.68\n",
      "batch: 460, batch train loss: 0.72, train acc: 72.17%, consuming tine: 4.68\n",
      "batch: 461, batch train loss: 0.72, train acc: 72.19%, consuming tine: 4.40\n",
      "batch: 462, batch train loss: 0.72, train acc: 72.21%, consuming tine: 4.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 463, batch train loss: 0.72, train acc: 72.23%, consuming tine: 4.48\n",
      "batch: 464, batch train loss: 0.72, train acc: 72.25%, consuming tine: 4.69\n",
      "batch: 465, batch train loss: 0.72, train acc: 72.27%, consuming tine: 4.69\n",
      "batch: 466, batch train loss: 0.72, train acc: 72.29%, consuming tine: 4.59\n",
      "batch: 467, batch train loss: 0.72, train acc: 72.31%, consuming tine: 4.49\n",
      "batch: 468, batch train loss: 0.72, train acc: 72.33%, consuming tine: 4.79\n",
      "batch: 469, batch train loss: 0.72, train acc: 72.36%, consuming tine: 4.84\n",
      "batch: 470, batch train loss: 0.72, train acc: 72.38%, consuming tine: 4.56\n",
      "batch: 471, batch train loss: 0.72, train acc: 72.40%, consuming tine: 4.55\n",
      "batch: 472, batch train loss: 0.72, train acc: 72.42%, consuming tine: 4.90\n",
      "batch: 473, batch train loss: 0.72, train acc: 72.44%, consuming tine: 4.77\n",
      "batch: 474, batch train loss: 0.72, train acc: 72.45%, consuming tine: 4.79\n",
      "batch: 475, batch train loss: 0.72, train acc: 72.47%, consuming tine: 4.78\n",
      "batch: 476, batch train loss: 0.72, train acc: 72.49%, consuming tine: 4.83\n",
      "batch: 477, batch train loss: 0.71, train acc: 72.50%, consuming tine: 4.97\n",
      "batch: 478, batch train loss: 0.71, train acc: 72.53%, consuming tine: 4.37\n",
      "batch: 479, batch train loss: 0.71, train acc: 72.55%, consuming tine: 4.49\n",
      "batch: 480, batch train loss: 0.71, train acc: 72.57%, consuming tine: 4.89\n",
      "batch: 481, batch train loss: 0.71, train acc: 72.59%, consuming tine: 4.78\n",
      "batch: 482, batch train loss: 0.71, train acc: 72.60%, consuming tine: 4.70\n",
      "batch: 483, batch train loss: 0.71, train acc: 72.62%, consuming tine: 4.78\n",
      "batch: 484, batch train loss: 0.71, train acc: 72.63%, consuming tine: 4.78\n",
      "batch: 485, batch train loss: 0.71, train acc: 72.64%, consuming tine: 4.70\n",
      "batch: 486, batch train loss: 0.71, train acc: 72.65%, consuming tine: 4.68\n",
      "batch: 487, batch train loss: 0.71, train acc: 72.66%, consuming tine: 4.88\n",
      "batch: 488, batch train loss: 0.71, train acc: 72.68%, consuming tine: 4.68\n",
      "batch: 489, batch train loss: 0.71, train acc: 72.69%, consuming tine: 4.60\n",
      "batch: 490, batch train loss: 0.71, train acc: 72.71%, consuming tine: 4.39\n",
      "batch: 491, batch train loss: 0.71, train acc: 72.72%, consuming tine: 4.59\n",
      "batch: 492, batch train loss: 0.71, train acc: 72.73%, consuming tine: 4.81\n",
      "batch: 493, batch train loss: 0.71, train acc: 72.74%, consuming tine: 4.57\n",
      "batch: 494, batch train loss: 0.71, train acc: 72.76%, consuming tine: 4.78\n",
      "batch: 495, batch train loss: 0.71, train acc: 72.77%, consuming tine: 4.79\n",
      "batch: 496, batch train loss: 0.71, train acc: 72.78%, consuming tine: 4.69\n",
      "batch: 497, batch train loss: 0.71, train acc: 72.78%, consuming tine: 4.91\n",
      "batch: 498, batch train loss: 0.71, train acc: 72.80%, consuming tine: 4.58\n",
      "batch: 499, batch train loss: 0.71, train acc: 72.81%, consuming tine: 4.38\n",
      "batch: 500, batch train loss: 0.71, train acc: 72.82%, consuming tine: 5.09\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 2.06, valid acc: 37.62%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.71, train acc: 72.83%, consuming tine: 4.63\n",
      "batch: 502, batch train loss: 0.71, train acc: 72.84%, consuming tine: 4.48\n",
      "batch: 503, batch train loss: 0.71, train acc: 72.85%, consuming tine: 4.78\n",
      "batch: 504, batch train loss: 0.71, train acc: 72.86%, consuming tine: 4.60\n",
      "batch: 505, batch train loss: 0.71, train acc: 72.87%, consuming tine: 5.00\n",
      "batch: 506, batch train loss: 0.71, train acc: 72.88%, consuming tine: 4.78\n",
      "batch: 507, batch train loss: 0.71, train acc: 72.89%, consuming tine: 4.48\n",
      "batch: 508, batch train loss: 0.70, train acc: 72.91%, consuming tine: 4.90\n",
      "batch: 509, batch train loss: 0.70, train acc: 72.92%, consuming tine: 4.89\n",
      "batch: 510, batch train loss: 0.70, train acc: 72.93%, consuming tine: 4.78\n",
      "batch: 511, batch train loss: 0.70, train acc: 72.94%, consuming tine: 4.77\n",
      "batch: 512, batch train loss: 0.70, train acc: 72.95%, consuming tine: 4.83\n",
      "batch: 513, batch train loss: 0.70, train acc: 72.96%, consuming tine: 4.59\n",
      "batch: 514, batch train loss: 0.70, train acc: 72.97%, consuming tine: 4.86\n",
      "batch: 515, batch train loss: 0.70, train acc: 72.98%, consuming tine: 4.40\n",
      "batch: 516, batch train loss: 0.70, train acc: 73.00%, consuming tine: 4.69\n",
      "batch: 517, batch train loss: 0.70, train acc: 73.01%, consuming tine: 4.50\n",
      "batch: 518, batch train loss: 0.70, train acc: 73.03%, consuming tine: 4.36\n",
      "batch: 519, batch train loss: 0.70, train acc: 73.04%, consuming tine: 4.79\n",
      "batch: 520, batch train loss: 0.70, train acc: 73.05%, consuming tine: 4.78\n",
      "batch: 521, batch train loss: 0.70, train acc: 73.06%, consuming tine: 4.78\n",
      "batch: 522, batch train loss: 0.70, train acc: 73.07%, consuming tine: 5.28\n",
      "batch: 523, batch train loss: 0.70, train acc: 73.09%, consuming tine: 5.09\n",
      "batch: 524, batch train loss: 0.70, train acc: 73.10%, consuming tine: 4.69\n",
      "batch: 525, batch train loss: 0.70, train acc: 73.11%, consuming tine: 4.58\n",
      "batch: 526, batch train loss: 0.70, train acc: 73.13%, consuming tine: 4.70\n",
      "batch: 527, batch train loss: 0.70, train acc: 73.14%, consuming tine: 4.57\n",
      "batch: 528, batch train loss: 0.70, train acc: 73.16%, consuming tine: 4.59\n",
      "batch: 529, batch train loss: 0.70, train acc: 73.17%, consuming tine: 4.79\n",
      "batch: 530, batch train loss: 0.70, train acc: 73.19%, consuming tine: 4.59\n",
      "batch: 531, batch train loss: 0.70, train acc: 73.20%, consuming tine: 4.60\n",
      "batch: 532, batch train loss: 0.70, train acc: 73.21%, consuming tine: 4.89\n",
      "batch: 533, batch train loss: 0.70, train acc: 73.23%, consuming tine: 4.98\n",
      "batch: 534, batch train loss: 0.70, train acc: 73.25%, consuming tine: 4.59\n",
      "batch: 535, batch train loss: 0.70, train acc: 73.26%, consuming tine: 4.60\n",
      "batch: 536, batch train loss: 0.70, train acc: 73.27%, consuming tine: 5.18\n",
      "batch: 537, batch train loss: 0.70, train acc: 73.29%, consuming tine: 4.99\n",
      "batch: 538, batch train loss: 0.70, train acc: 73.30%, consuming tine: 4.88\n",
      "batch: 539, batch train loss: 0.70, train acc: 73.32%, consuming tine: 4.81\n",
      "batch: 540, batch train loss: 0.69, train acc: 73.33%, consuming tine: 4.87\n",
      "batch: 541, batch train loss: 0.69, train acc: 73.35%, consuming tine: 4.58\n",
      "batch: 542, batch train loss: 0.69, train acc: 73.37%, consuming tine: 4.89\n",
      "batch: 543, batch train loss: 0.69, train acc: 73.38%, consuming tine: 5.01\n",
      "batch: 544, batch train loss: 0.69, train acc: 73.39%, consuming tine: 4.76\n",
      "batch: 545, batch train loss: 0.69, train acc: 73.41%, consuming tine: 4.70\n",
      "batch: 546, batch train loss: 0.69, train acc: 73.42%, consuming tine: 4.48\n",
      "batch: 547, batch train loss: 0.69, train acc: 73.43%, consuming tine: 4.96\n",
      "batch: 548, batch train loss: 0.69, train acc: 73.44%, consuming tine: 4.61\n",
      "batch: 549, batch train loss: 0.69, train acc: 73.46%, consuming tine: 4.89\n",
      "batch: 550, batch train loss: 0.69, train acc: 73.47%, consuming tine: 4.80\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 2.08, valid acc: 37.53%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.69, train acc: 73.48%, consuming tine: 4.54\n",
      "batch: 552, batch train loss: 0.69, train acc: 73.50%, consuming tine: 4.79\n",
      "batch: 553, batch train loss: 0.69, train acc: 73.51%, consuming tine: 5.10\n",
      "batch: 554, batch train loss: 0.69, train acc: 73.53%, consuming tine: 4.59\n",
      "batch: 555, batch train loss: 0.69, train acc: 73.55%, consuming tine: 4.67\n",
      "batch: 556, batch train loss: 0.69, train acc: 73.56%, consuming tine: 4.90\n",
      "batch: 557, batch train loss: 0.69, train acc: 73.58%, consuming tine: 4.77\n",
      "batch: 558, batch train loss: 0.69, train acc: 73.59%, consuming tine: 4.81\n",
      "batch: 559, batch train loss: 0.69, train acc: 73.61%, consuming tine: 4.78\n",
      "batch: 560, batch train loss: 0.69, train acc: 73.62%, consuming tine: 4.78\n",
      "batch: 561, batch train loss: 0.69, train acc: 73.63%, consuming tine: 4.90\n",
      "batch: 562, batch train loss: 0.69, train acc: 73.64%, consuming tine: 4.77\n",
      "batch: 563, batch train loss: 0.69, train acc: 73.66%, consuming tine: 4.69\n",
      "batch: 564, batch train loss: 0.69, train acc: 73.67%, consuming tine: 4.81\n",
      "batch: 565, batch train loss: 0.69, train acc: 73.68%, consuming tine: 4.57\n",
      "batch: 566, batch train loss: 0.69, train acc: 73.70%, consuming tine: 4.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 567, batch train loss: 0.69, train acc: 73.71%, consuming tine: 4.61\n",
      "batch: 568, batch train loss: 0.69, train acc: 73.73%, consuming tine: 4.37\n",
      "batch: 569, batch train loss: 0.68, train acc: 73.74%, consuming tine: 4.68\n",
      "batch: 570, batch train loss: 0.68, train acc: 73.76%, consuming tine: 4.70\n",
      "batch: 571, batch train loss: 0.68, train acc: 73.77%, consuming tine: 4.76\n",
      "batch: 572, batch train loss: 0.68, train acc: 73.78%, consuming tine: 4.52\n",
      "batch: 573, batch train loss: 0.68, train acc: 73.79%, consuming tine: 4.60\n",
      "batch: 574, batch train loss: 0.68, train acc: 73.80%, consuming tine: 4.79\n",
      "batch: 575, batch train loss: 0.68, train acc: 73.81%, consuming tine: 4.76\n",
      "batch: 576, batch train loss: 0.68, train acc: 73.82%, consuming tine: 4.91\n",
      "batch: 577, batch train loss: 0.68, train acc: 73.83%, consuming tine: 4.79\n",
      "batch: 578, batch train loss: 0.68, train acc: 73.84%, consuming tine: 4.80\n",
      "batch: 579, batch train loss: 0.68, train acc: 73.85%, consuming tine: 4.89\n",
      "batch: 580, batch train loss: 0.68, train acc: 73.86%, consuming tine: 4.86\n",
      "batch: 581, batch train loss: 0.68, train acc: 73.87%, consuming tine: 4.99\n",
      "batch: 582, batch train loss: 0.68, train acc: 73.88%, consuming tine: 4.69\n",
      "batch: 583, batch train loss: 0.68, train acc: 73.89%, consuming tine: 5.04\n",
      "batch: 584, batch train loss: 0.68, train acc: 73.91%, consuming tine: 4.56\n",
      "batch: 585, batch train loss: 0.68, train acc: 73.92%, consuming tine: 4.98\n",
      "batch: 586, batch train loss: 0.68, train acc: 73.94%, consuming tine: 4.58\n",
      "batch: 587, batch train loss: 0.68, train acc: 73.94%, consuming tine: 4.80\n",
      "batch: 588, batch train loss: 0.68, train acc: 73.95%, consuming tine: 4.48\n",
      "batch: 589, batch train loss: 0.68, train acc: 73.96%, consuming tine: 4.94\n",
      "batch: 590, batch train loss: 0.68, train acc: 73.97%, consuming tine: 4.68\n",
      "batch: 591, batch train loss: 0.68, train acc: 73.99%, consuming tine: 4.47\n",
      "batch: 592, batch train loss: 0.68, train acc: 74.00%, consuming tine: 4.50\n",
      "batch: 593, batch train loss: 0.68, train acc: 74.02%, consuming tine: 4.85\n",
      "batch: 594, batch train loss: 0.68, train acc: 74.03%, consuming tine: 4.40\n",
      "batch: 595, batch train loss: 0.68, train acc: 74.04%, consuming tine: 4.48\n",
      "batch: 596, batch train loss: 0.68, train acc: 74.05%, consuming tine: 4.98\n",
      "batch: 597, batch train loss: 0.68, train acc: 74.06%, consuming tine: 4.68\n",
      "batch: 598, batch train loss: 0.68, train acc: 74.07%, consuming tine: 4.61\n",
      "batch: 599, batch train loss: 0.68, train acc: 74.08%, consuming tine: 4.78\n",
      "batch: 600, batch train loss: 0.68, train acc: 74.09%, consuming tine: 4.88\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 2.09, valid acc: 37.49%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.68, train acc: 74.10%, consuming tine: 5.13\n",
      "batch: 602, batch train loss: 0.68, train acc: 74.11%, consuming tine: 4.88\n",
      "batch: 603, batch train loss: 0.68, train acc: 74.11%, consuming tine: 4.82\n",
      "batch: 604, batch train loss: 0.68, train acc: 74.12%, consuming tine: 4.86\n",
      "batch: 605, batch train loss: 0.68, train acc: 74.12%, consuming tine: 4.50\n",
      "batch: 606, batch train loss: 0.68, train acc: 74.13%, consuming tine: 4.88\n",
      "batch: 607, batch train loss: 0.67, train acc: 74.15%, consuming tine: 5.01\n",
      "batch: 608, batch train loss: 0.67, train acc: 74.15%, consuming tine: 4.99\n",
      "batch: 609, batch train loss: 0.67, train acc: 74.16%, consuming tine: 4.96\n",
      "batch: 610, batch train loss: 0.67, train acc: 74.16%, consuming tine: 5.12\n",
      "batch: 611, batch train loss: 0.67, train acc: 74.16%, consuming tine: 4.86\n",
      "batch: 612, batch train loss: 0.67, train acc: 74.17%, consuming tine: 4.59\n",
      "batch: 613, batch train loss: 0.67, train acc: 74.18%, consuming tine: 5.40\n",
      "batch: 614, batch train loss: 0.67, train acc: 74.19%, consuming tine: 4.87\n",
      "batch: 615, batch train loss: 0.67, train acc: 74.20%, consuming tine: 4.90\n",
      "batch: 616, batch train loss: 0.67, train acc: 74.21%, consuming tine: 5.19\n",
      "batch: 617, batch train loss: 0.67, train acc: 74.22%, consuming tine: 4.86\n",
      "batch: 618, batch train loss: 0.67, train acc: 74.23%, consuming tine: 5.11\n",
      "batch: 619, batch train loss: 0.67, train acc: 74.25%, consuming tine: 7.21\n",
      "batch: 620, batch train loss: 0.67, train acc: 74.26%, consuming tine: 5.01\n",
      "batch: 621, batch train loss: 0.67, train acc: 74.27%, consuming tine: 5.15\n",
      "batch: 622, batch train loss: 0.67, train acc: 74.28%, consuming tine: 4.98\n",
      "batch: 623, batch train loss: 0.67, train acc: 74.30%, consuming tine: 5.50\n",
      "batch: 624, batch train loss: 0.67, train acc: 74.31%, consuming tine: 5.27\n",
      "batch: 625, batch train loss: 0.67, train acc: 74.32%, consuming tine: 4.81\n",
      "batch: 626, batch train loss: 0.67, train acc: 74.33%, consuming tine: 5.20\n",
      "batch: 627, batch train loss: 0.67, train acc: 74.34%, consuming tine: 5.25\n",
      "batch: 628, batch train loss: 0.67, train acc: 74.35%, consuming tine: 4.90\n",
      "batch: 629, batch train loss: 0.67, train acc: 74.37%, consuming tine: 5.20\n",
      "batch: 630, batch train loss: 0.67, train acc: 74.38%, consuming tine: 4.98\n",
      "batch: 631, batch train loss: 0.67, train acc: 74.39%, consuming tine: 5.09\n",
      "batch: 632, batch train loss: 0.67, train acc: 74.41%, consuming tine: 5.06\n",
      "batch: 633, batch train loss: 0.67, train acc: 74.41%, consuming tine: 4.99\n",
      "batch: 634, batch train loss: 0.67, train acc: 74.43%, consuming tine: 5.19\n",
      "batch: 635, batch train loss: 0.67, train acc: 74.44%, consuming tine: 5.17\n",
      "batch: 636, batch train loss: 0.67, train acc: 74.45%, consuming tine: 4.99\n",
      "batch: 637, batch train loss: 0.67, train acc: 74.46%, consuming tine: 4.90\n",
      "batch: 638, batch train loss: 0.67, train acc: 74.47%, consuming tine: 5.07\n",
      "batch: 639, batch train loss: 0.67, train acc: 74.49%, consuming tine: 5.10\n",
      "batch: 640, batch train loss: 0.67, train acc: 74.50%, consuming tine: 5.40\n",
      "batch: 641, batch train loss: 0.67, train acc: 74.51%, consuming tine: 4.84\n",
      "batch: 642, batch train loss: 0.67, train acc: 74.52%, consuming tine: 4.94\n",
      "batch: 643, batch train loss: 0.67, train acc: 74.54%, consuming tine: 5.09\n",
      "batch: 644, batch train loss: 0.67, train acc: 74.55%, consuming tine: 4.98\n",
      "batch: 645, batch train loss: 0.66, train acc: 74.57%, consuming tine: 4.98\n",
      "batch: 646, batch train loss: 0.66, train acc: 74.58%, consuming tine: 5.00\n",
      "batch: 647, batch train loss: 0.66, train acc: 74.59%, consuming tine: 4.68\n",
      "batch: 648, batch train loss: 0.66, train acc: 74.60%, consuming tine: 4.89\n",
      "batch: 649, batch train loss: 0.66, train acc: 74.61%, consuming tine: 4.99\n",
      "batch: 650, batch train loss: 0.66, train acc: 74.62%, consuming tine: 4.78\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 2.11, valid acc: 37.48%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.66, train acc: 74.63%, consuming tine: 4.95\n",
      "batch: 652, batch train loss: 0.66, train acc: 74.65%, consuming tine: 4.76\n",
      "batch: 653, batch train loss: 0.66, train acc: 74.66%, consuming tine: 4.79\n",
      "batch: 654, batch train loss: 0.66, train acc: 74.67%, consuming tine: 4.69\n",
      "batch: 655, batch train loss: 0.66, train acc: 74.69%, consuming tine: 5.10\n",
      "batch: 656, batch train loss: 0.66, train acc: 74.70%, consuming tine: 4.77\n",
      "batch: 657, batch train loss: 0.66, train acc: 74.71%, consuming tine: 4.89\n",
      "batch: 658, batch train loss: 0.66, train acc: 74.72%, consuming tine: 5.18\n",
      "batch: 659, batch train loss: 0.66, train acc: 74.74%, consuming tine: 4.70\n",
      "batch: 660, batch train loss: 0.66, train acc: 74.75%, consuming tine: 4.88\n",
      "batch: 661, batch train loss: 0.66, train acc: 74.76%, consuming tine: 4.57\n",
      "batch: 662, batch train loss: 0.66, train acc: 74.77%, consuming tine: 4.80\n",
      "batch: 663, batch train loss: 0.66, train acc: 74.78%, consuming tine: 4.90\n",
      "batch: 664, batch train loss: 0.66, train acc: 74.79%, consuming tine: 4.78\n",
      "batch: 665, batch train loss: 0.66, train acc: 74.80%, consuming tine: 4.62\n",
      "batch: 666, batch train loss: 0.66, train acc: 74.81%, consuming tine: 4.86\n",
      "batch: 667, batch train loss: 0.66, train acc: 74.82%, consuming tine: 4.89\n",
      "batch: 668, batch train loss: 0.66, train acc: 74.83%, consuming tine: 4.68\n",
      "batch: 669, batch train loss: 0.66, train acc: 74.84%, consuming tine: 5.24\n",
      "batch: 670, batch train loss: 0.66, train acc: 74.85%, consuming tine: 4.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 671, batch train loss: 0.66, train acc: 74.87%, consuming tine: 4.59\n",
      "batch: 672, batch train loss: 0.66, train acc: 74.88%, consuming tine: 4.61\n",
      "batch: 673, batch train loss: 0.66, train acc: 74.89%, consuming tine: 4.54\n",
      "batch: 674, batch train loss: 0.66, train acc: 74.90%, consuming tine: 4.72\n",
      "batch: 675, batch train loss: 0.66, train acc: 74.91%, consuming tine: 4.59\n",
      "batch: 676, batch train loss: 0.66, train acc: 74.93%, consuming tine: 4.39\n",
      "batch: 677, batch train loss: 0.66, train acc: 74.94%, consuming tine: 5.19\n",
      "batch: 678, batch train loss: 0.66, train acc: 74.95%, consuming tine: 4.68\n",
      "batch: 679, batch train loss: 0.66, train acc: 74.96%, consuming tine: 4.60\n",
      "batch: 680, batch train loss: 0.65, train acc: 74.97%, consuming tine: 4.59\n",
      "batch: 681, batch train loss: 0.65, train acc: 74.98%, consuming tine: 4.59\n",
      "batch: 682, batch train loss: 0.65, train acc: 74.99%, consuming tine: 4.51\n",
      "batch: 683, batch train loss: 0.65, train acc: 75.01%, consuming tine: 4.57\n",
      "batch: 684, batch train loss: 0.65, train acc: 75.02%, consuming tine: 4.58\n",
      "batch: 685, batch train loss: 0.65, train acc: 75.02%, consuming tine: 4.49\n",
      "batch: 686, batch train loss: 0.65, train acc: 75.04%, consuming tine: 4.59\n",
      "batch: 687, batch train loss: 0.65, train acc: 75.05%, consuming tine: 4.69\n",
      "batch: 688, batch train loss: 0.65, train acc: 75.06%, consuming tine: 4.60\n",
      "batch: 689, batch train loss: 0.65, train acc: 75.08%, consuming tine: 4.48\n",
      "batch: 690, batch train loss: 0.65, train acc: 75.09%, consuming tine: 4.69\n",
      "batch: 691, batch train loss: 0.65, train acc: 75.09%, consuming tine: 4.59\n",
      "batch: 692, batch train loss: 0.65, train acc: 75.10%, consuming tine: 4.48\n",
      "batch: 693, batch train loss: 0.65, train acc: 75.11%, consuming tine: 4.40\n",
      "batch: 694, batch train loss: 0.65, train acc: 75.12%, consuming tine: 4.48\n",
      "batch: 695, batch train loss: 0.65, train acc: 75.13%, consuming tine: 4.40\n",
      "batch: 696, batch train loss: 0.65, train acc: 75.15%, consuming tine: 4.57\n",
      "batch: 697, batch train loss: 0.65, train acc: 75.16%, consuming tine: 4.70\n",
      "batch: 698, batch train loss: 0.65, train acc: 75.17%, consuming tine: 4.58\n",
      "batch: 699, batch train loss: 0.65, train acc: 75.18%, consuming tine: 4.73\n",
      "batch: 700, batch train loss: 0.65, train acc: 75.19%, consuming tine: 4.48\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 2.13, valid acc: 37.46%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.65, train acc: 75.20%, consuming tine: 4.53\n",
      "batch: 702, batch train loss: 0.65, train acc: 75.21%, consuming tine: 4.77\n",
      "batch: 703, batch train loss: 0.65, train acc: 75.23%, consuming tine: 4.50\n",
      "batch: 704, batch train loss: 0.65, train acc: 75.23%, consuming tine: 4.30\n",
      "Epoch 3, Loss: 0.65, Accuracy: 75.23%, Valid Loss: 2.13, Valid Accuracy: 37.46%\n",
      "batch: 1, batch train loss: 0.44, train acc: 83.98%, consuming tine: 4.86\n",
      "batch: 2, batch train loss: 0.46, train acc: 83.20%, consuming tine: 4.31\n",
      "batch: 3, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.48\n",
      "batch: 4, batch train loss: 0.46, train acc: 82.64%, consuming tine: 4.70\n",
      "batch: 5, batch train loss: 0.45, train acc: 83.05%, consuming tine: 4.51\n",
      "batch: 6, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.55\n",
      "batch: 7, batch train loss: 0.46, train acc: 82.85%, consuming tine: 4.49\n",
      "batch: 8, batch train loss: 0.46, train acc: 82.51%, consuming tine: 4.69\n",
      "batch: 9, batch train loss: 0.46, train acc: 82.57%, consuming tine: 4.57\n",
      "batch: 10, batch train loss: 0.46, train acc: 82.60%, consuming tine: 4.49\n",
      "batch: 11, batch train loss: 0.46, train acc: 82.69%, consuming tine: 4.60\n",
      "batch: 12, batch train loss: 0.46, train acc: 82.80%, consuming tine: 4.68\n",
      "batch: 13, batch train loss: 0.46, train acc: 82.96%, consuming tine: 4.39\n",
      "batch: 14, batch train loss: 0.46, train acc: 82.90%, consuming tine: 4.79\n",
      "batch: 15, batch train loss: 0.46, train acc: 83.01%, consuming tine: 4.68\n",
      "batch: 16, batch train loss: 0.46, train acc: 82.95%, consuming tine: 4.57\n",
      "batch: 17, batch train loss: 0.46, train acc: 82.87%, consuming tine: 4.50\n",
      "batch: 18, batch train loss: 0.46, train acc: 82.61%, consuming tine: 5.24\n",
      "batch: 19, batch train loss: 0.46, train acc: 82.54%, consuming tine: 4.73\n",
      "batch: 20, batch train loss: 0.47, train acc: 82.39%, consuming tine: 4.49\n",
      "batch: 21, batch train loss: 0.47, train acc: 82.47%, consuming tine: 4.79\n",
      "batch: 22, batch train loss: 0.46, train acc: 82.60%, consuming tine: 4.74\n",
      "batch: 23, batch train loss: 0.47, train acc: 82.53%, consuming tine: 4.34\n",
      "batch: 24, batch train loss: 0.47, train acc: 82.39%, consuming tine: 4.59\n",
      "batch: 25, batch train loss: 0.47, train acc: 82.45%, consuming tine: 4.78\n",
      "batch: 26, batch train loss: 0.47, train acc: 82.49%, consuming tine: 4.81\n",
      "batch: 27, batch train loss: 0.46, train acc: 82.56%, consuming tine: 4.39\n",
      "batch: 28, batch train loss: 0.46, train acc: 82.67%, consuming tine: 4.56\n",
      "batch: 29, batch train loss: 0.46, train acc: 82.75%, consuming tine: 4.54\n",
      "batch: 30, batch train loss: 0.46, train acc: 82.66%, consuming tine: 4.64\n",
      "batch: 31, batch train loss: 0.46, train acc: 82.60%, consuming tine: 4.46\n",
      "batch: 32, batch train loss: 0.47, train acc: 82.50%, consuming tine: 4.80\n",
      "batch: 33, batch train loss: 0.47, train acc: 82.49%, consuming tine: 4.58\n",
      "batch: 34, batch train loss: 0.47, train acc: 82.44%, consuming tine: 4.58\n",
      "batch: 35, batch train loss: 0.47, train acc: 82.50%, consuming tine: 4.69\n",
      "batch: 36, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.68\n",
      "batch: 37, batch train loss: 0.46, train acc: 82.57%, consuming tine: 4.39\n",
      "batch: 38, batch train loss: 0.46, train acc: 82.56%, consuming tine: 4.91\n",
      "batch: 39, batch train loss: 0.46, train acc: 82.57%, consuming tine: 4.68\n",
      "batch: 40, batch train loss: 0.46, train acc: 82.62%, consuming tine: 4.47\n",
      "batch: 41, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.57\n",
      "batch: 42, batch train loss: 0.46, train acc: 82.66%, consuming tine: 4.60\n",
      "batch: 43, batch train loss: 0.46, train acc: 82.69%, consuming tine: 4.88\n",
      "batch: 44, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.69\n",
      "batch: 45, batch train loss: 0.46, train acc: 82.74%, consuming tine: 4.50\n",
      "batch: 46, batch train loss: 0.46, train acc: 82.77%, consuming tine: 4.59\n",
      "batch: 47, batch train loss: 0.46, train acc: 82.80%, consuming tine: 4.59\n",
      "batch: 48, batch train loss: 0.46, train acc: 82.85%, consuming tine: 4.38\n",
      "batch: 49, batch train loss: 0.46, train acc: 82.91%, consuming tine: 4.38\n",
      "batch: 50, batch train loss: 0.46, train acc: 82.95%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 2.39, valid acc: 36.91%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.45, train acc: 82.98%, consuming tine: 4.48\n",
      "batch: 52, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.67\n",
      "batch: 53, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.69\n",
      "batch: 54, batch train loss: 0.45, train acc: 83.06%, consuming tine: 4.58\n",
      "batch: 55, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.59\n",
      "batch: 56, batch train loss: 0.45, train acc: 83.00%, consuming tine: 4.79\n",
      "batch: 57, batch train loss: 0.45, train acc: 82.97%, consuming tine: 4.50\n",
      "batch: 58, batch train loss: 0.45, train acc: 82.95%, consuming tine: 4.55\n",
      "batch: 59, batch train loss: 0.45, train acc: 82.95%, consuming tine: 4.53\n",
      "batch: 60, batch train loss: 0.45, train acc: 82.98%, consuming tine: 4.70\n",
      "batch: 61, batch train loss: 0.45, train acc: 82.95%, consuming tine: 4.47\n",
      "batch: 62, batch train loss: 0.45, train acc: 82.98%, consuming tine: 4.60\n",
      "batch: 63, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.75\n",
      "batch: 64, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.41\n",
      "batch: 65, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.61\n",
      "batch: 66, batch train loss: 0.45, train acc: 83.00%, consuming tine: 4.37\n",
      "batch: 67, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.39\n",
      "batch: 68, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.58\n",
      "batch: 69, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.61\n",
      "batch: 70, batch train loss: 0.45, train acc: 83.03%, consuming tine: 4.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 71, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.59\n",
      "batch: 72, batch train loss: 0.45, train acc: 82.97%, consuming tine: 4.59\n",
      "batch: 73, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.49\n",
      "batch: 74, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.89\n",
      "batch: 75, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.68\n",
      "batch: 76, batch train loss: 0.45, train acc: 83.01%, consuming tine: 4.49\n",
      "batch: 77, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.59\n",
      "batch: 78, batch train loss: 0.45, train acc: 83.03%, consuming tine: 4.77\n",
      "batch: 79, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.42\n",
      "batch: 80, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.37\n",
      "batch: 81, batch train loss: 0.45, train acc: 83.07%, consuming tine: 4.59\n",
      "batch: 82, batch train loss: 0.45, train acc: 83.07%, consuming tine: 4.69\n",
      "batch: 83, batch train loss: 0.45, train acc: 83.11%, consuming tine: 4.60\n",
      "batch: 84, batch train loss: 0.45, train acc: 83.12%, consuming tine: 4.67\n",
      "batch: 85, batch train loss: 0.45, train acc: 83.12%, consuming tine: 4.58\n",
      "batch: 86, batch train loss: 0.45, train acc: 83.14%, consuming tine: 4.59\n",
      "batch: 87, batch train loss: 0.45, train acc: 83.13%, consuming tine: 4.69\n",
      "batch: 88, batch train loss: 0.45, train acc: 83.12%, consuming tine: 4.59\n",
      "batch: 89, batch train loss: 0.45, train acc: 83.11%, consuming tine: 4.60\n",
      "batch: 90, batch train loss: 0.45, train acc: 83.10%, consuming tine: 4.69\n",
      "batch: 91, batch train loss: 0.45, train acc: 83.09%, consuming tine: 4.48\n",
      "batch: 92, batch train loss: 0.45, train acc: 83.10%, consuming tine: 4.79\n",
      "batch: 93, batch train loss: 0.45, train acc: 83.08%, consuming tine: 4.77\n",
      "batch: 94, batch train loss: 0.45, train acc: 83.06%, consuming tine: 4.29\n",
      "batch: 95, batch train loss: 0.45, train acc: 83.05%, consuming tine: 4.59\n",
      "batch: 96, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.49\n",
      "batch: 97, batch train loss: 0.45, train acc: 83.05%, consuming tine: 4.40\n",
      "batch: 98, batch train loss: 0.45, train acc: 83.02%, consuming tine: 4.37\n",
      "batch: 99, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.71\n",
      "batch: 100, batch train loss: 0.45, train acc: 82.96%, consuming tine: 4.56\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 2.39, valid acc: 36.69%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.45, train acc: 82.91%, consuming tine: 4.43\n",
      "batch: 102, batch train loss: 0.46, train acc: 82.88%, consuming tine: 4.68\n",
      "batch: 103, batch train loss: 0.46, train acc: 82.85%, consuming tine: 4.60\n",
      "batch: 104, batch train loss: 0.46, train acc: 82.86%, consuming tine: 4.49\n",
      "batch: 105, batch train loss: 0.46, train acc: 82.86%, consuming tine: 4.78\n",
      "batch: 106, batch train loss: 0.46, train acc: 82.82%, consuming tine: 4.49\n",
      "batch: 107, batch train loss: 0.46, train acc: 82.78%, consuming tine: 4.60\n",
      "batch: 108, batch train loss: 0.46, train acc: 82.77%, consuming tine: 4.61\n",
      "batch: 109, batch train loss: 0.46, train acc: 82.78%, consuming tine: 4.54\n",
      "batch: 110, batch train loss: 0.46, train acc: 82.77%, consuming tine: 4.49\n",
      "batch: 111, batch train loss: 0.46, train acc: 82.74%, consuming tine: 4.68\n",
      "batch: 112, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.33\n",
      "batch: 113, batch train loss: 0.46, train acc: 82.65%, consuming tine: 4.55\n",
      "batch: 114, batch train loss: 0.46, train acc: 82.62%, consuming tine: 4.59\n",
      "batch: 115, batch train loss: 0.46, train acc: 82.61%, consuming tine: 4.71\n",
      "batch: 116, batch train loss: 0.46, train acc: 82.64%, consuming tine: 4.48\n",
      "batch: 117, batch train loss: 0.46, train acc: 82.67%, consuming tine: 4.60\n",
      "batch: 118, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.57\n",
      "batch: 119, batch train loss: 0.46, train acc: 82.69%, consuming tine: 4.50\n",
      "batch: 120, batch train loss: 0.46, train acc: 82.69%, consuming tine: 4.67\n",
      "batch: 121, batch train loss: 0.46, train acc: 82.69%, consuming tine: 4.59\n",
      "batch: 122, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.79\n",
      "batch: 123, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.60\n",
      "batch: 124, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.38\n",
      "batch: 125, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.84\n",
      "batch: 126, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.65\n",
      "batch: 127, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.28\n",
      "batch: 128, batch train loss: 0.46, train acc: 82.73%, consuming tine: 4.68\n",
      "batch: 129, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.48\n",
      "batch: 130, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.58\n",
      "batch: 131, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.61\n",
      "batch: 132, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.68\n",
      "batch: 133, batch train loss: 0.46, train acc: 82.75%, consuming tine: 4.88\n",
      "batch: 134, batch train loss: 0.46, train acc: 82.74%, consuming tine: 4.82\n",
      "batch: 135, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.47\n",
      "batch: 136, batch train loss: 0.46, train acc: 82.71%, consuming tine: 4.76\n",
      "batch: 137, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.59\n",
      "batch: 138, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.58\n",
      "batch: 139, batch train loss: 0.46, train acc: 82.68%, consuming tine: 4.68\n",
      "batch: 140, batch train loss: 0.46, train acc: 82.60%, consuming tine: 4.50\n",
      "batch: 141, batch train loss: 0.46, train acc: 82.54%, consuming tine: 4.77\n",
      "batch: 142, batch train loss: 0.46, train acc: 82.52%, consuming tine: 4.57\n",
      "batch: 143, batch train loss: 0.46, train acc: 82.52%, consuming tine: 4.60\n",
      "batch: 144, batch train loss: 0.46, train acc: 82.48%, consuming tine: 4.60\n",
      "batch: 145, batch train loss: 0.46, train acc: 82.42%, consuming tine: 4.69\n",
      "batch: 146, batch train loss: 0.47, train acc: 82.35%, consuming tine: 4.47\n",
      "batch: 147, batch train loss: 0.47, train acc: 82.33%, consuming tine: 4.49\n",
      "batch: 148, batch train loss: 0.47, train acc: 82.35%, consuming tine: 4.67\n",
      "batch: 149, batch train loss: 0.46, train acc: 82.37%, consuming tine: 4.74\n",
      "batch: 150, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.45\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 2.37, valid acc: 36.93%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.46, train acc: 82.40%, consuming tine: 4.67\n",
      "batch: 152, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.58\n",
      "batch: 153, batch train loss: 0.46, train acc: 82.38%, consuming tine: 4.41\n",
      "batch: 154, batch train loss: 0.46, train acc: 82.36%, consuming tine: 4.37\n",
      "batch: 155, batch train loss: 0.46, train acc: 82.34%, consuming tine: 4.96\n",
      "batch: 156, batch train loss: 0.47, train acc: 82.29%, consuming tine: 4.70\n",
      "batch: 157, batch train loss: 0.47, train acc: 82.28%, consuming tine: 4.60\n",
      "batch: 158, batch train loss: 0.47, train acc: 82.30%, consuming tine: 4.79\n",
      "batch: 159, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.50\n",
      "batch: 160, batch train loss: 0.47, train acc: 82.29%, consuming tine: 4.67\n",
      "batch: 161, batch train loss: 0.47, train acc: 82.26%, consuming tine: 4.49\n",
      "batch: 162, batch train loss: 0.47, train acc: 82.24%, consuming tine: 4.70\n",
      "batch: 163, batch train loss: 0.47, train acc: 82.22%, consuming tine: 4.57\n",
      "batch: 164, batch train loss: 0.47, train acc: 82.23%, consuming tine: 4.78\n",
      "batch: 165, batch train loss: 0.47, train acc: 82.22%, consuming tine: 4.49\n",
      "batch: 166, batch train loss: 0.47, train acc: 82.21%, consuming tine: 4.70\n",
      "batch: 167, batch train loss: 0.47, train acc: 82.19%, consuming tine: 4.69\n",
      "batch: 168, batch train loss: 0.47, train acc: 82.18%, consuming tine: 4.39\n",
      "batch: 169, batch train loss: 0.47, train acc: 82.18%, consuming tine: 4.29\n",
      "batch: 170, batch train loss: 0.47, train acc: 82.20%, consuming tine: 4.58\n",
      "batch: 171, batch train loss: 0.47, train acc: 82.22%, consuming tine: 4.68\n",
      "batch: 172, batch train loss: 0.47, train acc: 82.23%, consuming tine: 4.69\n",
      "batch: 173, batch train loss: 0.47, train acc: 82.23%, consuming tine: 4.59\n",
      "batch: 174, batch train loss: 0.47, train acc: 82.25%, consuming tine: 4.57\n",
      "batch: 175, batch train loss: 0.46, train acc: 82.28%, consuming tine: 4.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 176, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.68\n",
      "batch: 177, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.68\n",
      "batch: 178, batch train loss: 0.46, train acc: 82.30%, consuming tine: 4.70\n",
      "batch: 179, batch train loss: 0.46, train acc: 82.30%, consuming tine: 4.49\n",
      "batch: 180, batch train loss: 0.46, train acc: 82.29%, consuming tine: 4.88\n",
      "batch: 181, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.48\n",
      "batch: 182, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.40\n",
      "batch: 183, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.79\n",
      "batch: 184, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.37\n",
      "batch: 185, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.49\n",
      "batch: 186, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.51\n",
      "batch: 187, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.56\n",
      "batch: 188, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.50\n",
      "batch: 189, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.99\n",
      "batch: 190, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.51\n",
      "batch: 191, batch train loss: 0.46, train acc: 82.35%, consuming tine: 4.59\n",
      "batch: 192, batch train loss: 0.46, train acc: 82.38%, consuming tine: 4.37\n",
      "batch: 193, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.80\n",
      "batch: 194, batch train loss: 0.46, train acc: 82.37%, consuming tine: 4.87\n",
      "batch: 195, batch train loss: 0.46, train acc: 82.35%, consuming tine: 4.60\n",
      "batch: 196, batch train loss: 0.46, train acc: 82.35%, consuming tine: 4.58\n",
      "batch: 197, batch train loss: 0.46, train acc: 82.35%, consuming tine: 4.59\n",
      "batch: 198, batch train loss: 0.46, train acc: 82.37%, consuming tine: 4.40\n",
      "batch: 199, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.69\n",
      "batch: 200, batch train loss: 0.46, train acc: 82.42%, consuming tine: 4.49\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 2.39, valid acc: 36.98%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.46, train acc: 82.42%, consuming tine: 4.41\n",
      "batch: 202, batch train loss: 0.46, train acc: 82.44%, consuming tine: 4.79\n",
      "batch: 203, batch train loss: 0.46, train acc: 82.48%, consuming tine: 4.68\n",
      "batch: 204, batch train loss: 0.46, train acc: 82.49%, consuming tine: 4.53\n",
      "batch: 205, batch train loss: 0.46, train acc: 82.51%, consuming tine: 5.26\n",
      "batch: 206, batch train loss: 0.46, train acc: 82.52%, consuming tine: 4.78\n",
      "batch: 207, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.59\n",
      "batch: 208, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.69\n",
      "batch: 209, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.48\n",
      "batch: 210, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.59\n",
      "batch: 211, batch train loss: 0.46, train acc: 82.54%, consuming tine: 4.59\n",
      "batch: 212, batch train loss: 0.46, train acc: 82.56%, consuming tine: 4.58\n",
      "batch: 213, batch train loss: 0.46, train acc: 82.57%, consuming tine: 4.59\n",
      "batch: 214, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.70\n",
      "batch: 215, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.50\n",
      "batch: 216, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.46\n",
      "batch: 217, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.40\n",
      "batch: 218, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.81\n",
      "batch: 219, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.58\n",
      "batch: 220, batch train loss: 0.46, train acc: 82.54%, consuming tine: 4.58\n",
      "batch: 221, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.90\n",
      "batch: 222, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.68\n",
      "batch: 223, batch train loss: 0.46, train acc: 82.53%, consuming tine: 4.48\n",
      "batch: 224, batch train loss: 0.46, train acc: 82.50%, consuming tine: 4.59\n",
      "batch: 225, batch train loss: 0.46, train acc: 82.49%, consuming tine: 4.68\n",
      "batch: 226, batch train loss: 0.46, train acc: 82.49%, consuming tine: 4.40\n",
      "batch: 227, batch train loss: 0.46, train acc: 82.46%, consuming tine: 4.68\n",
      "batch: 228, batch train loss: 0.46, train acc: 82.43%, consuming tine: 4.50\n",
      "batch: 229, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.77\n",
      "batch: 230, batch train loss: 0.46, train acc: 82.36%, consuming tine: 4.70\n",
      "batch: 231, batch train loss: 0.46, train acc: 82.34%, consuming tine: 4.57\n",
      "batch: 232, batch train loss: 0.46, train acc: 82.34%, consuming tine: 4.59\n",
      "batch: 233, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.48\n",
      "batch: 234, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.59\n",
      "batch: 235, batch train loss: 0.46, train acc: 82.30%, consuming tine: 4.78\n",
      "batch: 236, batch train loss: 0.47, train acc: 82.29%, consuming tine: 4.66\n",
      "batch: 237, batch train loss: 0.47, train acc: 82.28%, consuming tine: 4.53\n",
      "batch: 238, batch train loss: 0.47, train acc: 82.28%, consuming tine: 4.49\n",
      "batch: 239, batch train loss: 0.47, train acc: 82.28%, consuming tine: 5.00\n",
      "batch: 240, batch train loss: 0.47, train acc: 82.30%, consuming tine: 4.47\n",
      "batch: 241, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.54\n",
      "batch: 242, batch train loss: 0.46, train acc: 82.31%, consuming tine: 4.52\n",
      "batch: 243, batch train loss: 0.47, train acc: 82.30%, consuming tine: 4.30\n",
      "batch: 244, batch train loss: 0.47, train acc: 82.30%, consuming tine: 4.38\n",
      "batch: 245, batch train loss: 0.47, train acc: 82.31%, consuming tine: 4.97\n",
      "batch: 246, batch train loss: 0.47, train acc: 82.32%, consuming tine: 4.48\n",
      "batch: 247, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.50\n",
      "batch: 248, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.59\n",
      "batch: 249, batch train loss: 0.46, train acc: 82.34%, consuming tine: 4.60\n",
      "batch: 250, batch train loss: 0.46, train acc: 82.33%, consuming tine: 4.36\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 2.40, valid acc: 37.02%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.88\n",
      "batch: 252, batch train loss: 0.46, train acc: 82.32%, consuming tine: 4.40\n",
      "batch: 253, batch train loss: 0.46, train acc: 82.34%, consuming tine: 4.29\n",
      "batch: 254, batch train loss: 0.46, train acc: 82.35%, consuming tine: 4.91\n",
      "batch: 255, batch train loss: 0.46, train acc: 82.36%, consuming tine: 4.68\n",
      "batch: 256, batch train loss: 0.46, train acc: 82.36%, consuming tine: 5.27\n",
      "batch: 257, batch train loss: 0.46, train acc: 82.36%, consuming tine: 5.19\n",
      "batch: 258, batch train loss: 0.46, train acc: 82.37%, consuming tine: 4.98\n",
      "batch: 259, batch train loss: 0.46, train acc: 82.39%, consuming tine: 4.89\n",
      "batch: 260, batch train loss: 0.46, train acc: 82.41%, consuming tine: 4.59\n",
      "batch: 261, batch train loss: 0.46, train acc: 82.42%, consuming tine: 4.49\n",
      "batch: 262, batch train loss: 0.46, train acc: 82.44%, consuming tine: 4.60\n",
      "batch: 263, batch train loss: 0.46, train acc: 82.43%, consuming tine: 4.58\n",
      "batch: 264, batch train loss: 0.46, train acc: 82.44%, consuming tine: 4.78\n",
      "batch: 265, batch train loss: 0.46, train acc: 82.45%, consuming tine: 4.60\n",
      "batch: 266, batch train loss: 0.46, train acc: 82.47%, consuming tine: 4.58\n",
      "batch: 267, batch train loss: 0.46, train acc: 82.49%, consuming tine: 4.50\n",
      "batch: 268, batch train loss: 0.46, train acc: 82.51%, consuming tine: 4.47\n",
      "batch: 269, batch train loss: 0.46, train acc: 82.52%, consuming tine: 4.70\n",
      "batch: 270, batch train loss: 0.46, train acc: 82.54%, consuming tine: 4.47\n",
      "batch: 271, batch train loss: 0.46, train acc: 82.55%, consuming tine: 4.40\n",
      "batch: 272, batch train loss: 0.46, train acc: 82.56%, consuming tine: 4.67\n",
      "batch: 273, batch train loss: 0.46, train acc: 82.58%, consuming tine: 4.50\n",
      "batch: 274, batch train loss: 0.46, train acc: 82.60%, consuming tine: 4.29\n",
      "batch: 275, batch train loss: 0.46, train acc: 82.61%, consuming tine: 4.69\n",
      "batch: 276, batch train loss: 0.46, train acc: 82.62%, consuming tine: 4.59\n",
      "batch: 277, batch train loss: 0.46, train acc: 82.63%, consuming tine: 4.37\n",
      "batch: 278, batch train loss: 0.46, train acc: 82.64%, consuming tine: 4.67\n",
      "batch: 279, batch train loss: 0.46, train acc: 82.66%, consuming tine: 4.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 280, batch train loss: 0.46, train acc: 82.68%, consuming tine: 4.47\n",
      "batch: 281, batch train loss: 0.46, train acc: 82.70%, consuming tine: 4.60\n",
      "batch: 282, batch train loss: 0.46, train acc: 82.72%, consuming tine: 4.68\n",
      "batch: 283, batch train loss: 0.46, train acc: 82.73%, consuming tine: 4.38\n",
      "batch: 284, batch train loss: 0.46, train acc: 82.76%, consuming tine: 4.79\n",
      "batch: 285, batch train loss: 0.46, train acc: 82.77%, consuming tine: 4.39\n",
      "batch: 286, batch train loss: 0.46, train acc: 82.79%, consuming tine: 4.58\n",
      "batch: 287, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.70\n",
      "batch: 288, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.77\n",
      "batch: 289, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.70\n",
      "batch: 290, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.58\n",
      "batch: 291, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.39\n",
      "batch: 292, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.69\n",
      "batch: 293, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.68\n",
      "batch: 294, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.68\n",
      "batch: 295, batch train loss: 0.45, train acc: 82.84%, consuming tine: 4.56\n",
      "batch: 296, batch train loss: 0.45, train acc: 82.84%, consuming tine: 4.74\n",
      "batch: 297, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.68\n",
      "batch: 298, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.49\n",
      "batch: 299, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.68\n",
      "batch: 300, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.61\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 2.42, valid acc: 36.95%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.45, train acc: 82.84%, consuming tine: 4.60\n",
      "batch: 302, batch train loss: 0.45, train acc: 82.85%, consuming tine: 4.38\n",
      "batch: 303, batch train loss: 0.45, train acc: 82.85%, consuming tine: 5.08\n",
      "batch: 304, batch train loss: 0.45, train acc: 82.86%, consuming tine: 4.40\n",
      "batch: 305, batch train loss: 0.45, train acc: 82.86%, consuming tine: 4.59\n",
      "batch: 306, batch train loss: 0.45, train acc: 82.86%, consuming tine: 4.67\n",
      "batch: 307, batch train loss: 0.45, train acc: 82.84%, consuming tine: 4.51\n",
      "batch: 308, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.47\n",
      "batch: 309, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.69\n",
      "batch: 310, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.60\n",
      "batch: 311, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.48\n",
      "batch: 312, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.81\n",
      "batch: 313, batch train loss: 0.45, train acc: 82.80%, consuming tine: 4.49\n",
      "batch: 314, batch train loss: 0.45, train acc: 82.79%, consuming tine: 4.37\n",
      "batch: 315, batch train loss: 0.45, train acc: 82.78%, consuming tine: 4.48\n",
      "batch: 316, batch train loss: 0.45, train acc: 82.77%, consuming tine: 4.48\n",
      "batch: 317, batch train loss: 0.45, train acc: 82.76%, consuming tine: 4.59\n",
      "batch: 318, batch train loss: 0.45, train acc: 82.77%, consuming tine: 4.61\n",
      "batch: 319, batch train loss: 0.45, train acc: 82.78%, consuming tine: 4.57\n",
      "batch: 320, batch train loss: 0.45, train acc: 82.80%, consuming tine: 4.50\n",
      "batch: 321, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.78\n",
      "batch: 322, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.58\n",
      "batch: 323, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.51\n",
      "batch: 324, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.48\n",
      "batch: 325, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.67\n",
      "batch: 326, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.42\n",
      "batch: 327, batch train loss: 0.45, train acc: 82.81%, consuming tine: 4.69\n",
      "batch: 328, batch train loss: 0.45, train acc: 82.82%, consuming tine: 4.78\n",
      "batch: 329, batch train loss: 0.45, train acc: 82.83%, consuming tine: 4.74\n",
      "batch: 330, batch train loss: 0.45, train acc: 82.85%, consuming tine: 4.63\n",
      "batch: 331, batch train loss: 0.45, train acc: 82.87%, consuming tine: 4.68\n",
      "batch: 332, batch train loss: 0.45, train acc: 82.88%, consuming tine: 4.46\n",
      "batch: 333, batch train loss: 0.45, train acc: 82.90%, consuming tine: 4.73\n",
      "batch: 334, batch train loss: 0.45, train acc: 82.91%, consuming tine: 4.80\n",
      "batch: 335, batch train loss: 0.45, train acc: 82.92%, consuming tine: 4.58\n",
      "batch: 336, batch train loss: 0.45, train acc: 82.93%, consuming tine: 4.47\n",
      "batch: 337, batch train loss: 0.45, train acc: 82.94%, consuming tine: 4.69\n",
      "batch: 338, batch train loss: 0.45, train acc: 82.95%, consuming tine: 4.67\n",
      "batch: 339, batch train loss: 0.45, train acc: 82.97%, consuming tine: 4.69\n",
      "batch: 340, batch train loss: 0.45, train acc: 82.98%, consuming tine: 4.70\n",
      "batch: 341, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.71\n",
      "batch: 342, batch train loss: 0.45, train acc: 82.99%, consuming tine: 4.55\n",
      "batch: 343, batch train loss: 0.45, train acc: 83.01%, consuming tine: 4.70\n",
      "batch: 344, batch train loss: 0.45, train acc: 83.03%, consuming tine: 4.77\n",
      "batch: 345, batch train loss: 0.45, train acc: 83.04%, consuming tine: 4.50\n",
      "batch: 346, batch train loss: 0.45, train acc: 83.05%, consuming tine: 4.49\n",
      "batch: 347, batch train loss: 0.45, train acc: 83.07%, consuming tine: 4.79\n",
      "batch: 348, batch train loss: 0.45, train acc: 83.09%, consuming tine: 4.48\n",
      "batch: 349, batch train loss: 0.45, train acc: 83.11%, consuming tine: 4.68\n",
      "batch: 350, batch train loss: 0.45, train acc: 83.13%, consuming tine: 4.68\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 2.46, valid acc: 36.93%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.45, train acc: 83.15%, consuming tine: 4.64\n",
      "batch: 352, batch train loss: 0.45, train acc: 83.16%, consuming tine: 4.77\n",
      "batch: 353, batch train loss: 0.45, train acc: 83.18%, consuming tine: 4.94\n",
      "batch: 354, batch train loss: 0.44, train acc: 83.19%, consuming tine: 4.58\n",
      "batch: 355, batch train loss: 0.44, train acc: 83.19%, consuming tine: 4.71\n",
      "batch: 356, batch train loss: 0.44, train acc: 83.20%, consuming tine: 4.89\n",
      "batch: 357, batch train loss: 0.44, train acc: 83.21%, consuming tine: 4.76\n",
      "batch: 358, batch train loss: 0.44, train acc: 83.23%, consuming tine: 4.61\n",
      "batch: 359, batch train loss: 0.44, train acc: 83.24%, consuming tine: 4.68\n",
      "batch: 360, batch train loss: 0.44, train acc: 83.26%, consuming tine: 4.89\n",
      "batch: 361, batch train loss: 0.44, train acc: 83.27%, consuming tine: 4.59\n",
      "batch: 362, batch train loss: 0.44, train acc: 83.29%, consuming tine: 4.58\n",
      "batch: 363, batch train loss: 0.44, train acc: 83.30%, consuming tine: 4.59\n",
      "batch: 364, batch train loss: 0.44, train acc: 83.31%, consuming tine: 4.60\n",
      "batch: 365, batch train loss: 0.44, train acc: 83.33%, consuming tine: 4.61\n",
      "batch: 366, batch train loss: 0.44, train acc: 83.35%, consuming tine: 4.55\n",
      "batch: 367, batch train loss: 0.44, train acc: 83.37%, consuming tine: 4.74\n",
      "batch: 368, batch train loss: 0.44, train acc: 83.38%, consuming tine: 4.58\n",
      "batch: 369, batch train loss: 0.44, train acc: 83.40%, consuming tine: 4.56\n",
      "batch: 370, batch train loss: 0.44, train acc: 83.42%, consuming tine: 4.99\n",
      "batch: 371, batch train loss: 0.44, train acc: 83.45%, consuming tine: 4.48\n",
      "batch: 372, batch train loss: 0.44, train acc: 83.46%, consuming tine: 4.98\n",
      "batch: 373, batch train loss: 0.44, train acc: 83.48%, consuming tine: 4.80\n",
      "batch: 374, batch train loss: 0.44, train acc: 83.49%, consuming tine: 4.79\n",
      "batch: 375, batch train loss: 0.44, train acc: 83.51%, consuming tine: 4.59\n",
      "batch: 376, batch train loss: 0.44, train acc: 83.52%, consuming tine: 4.68\n",
      "batch: 377, batch train loss: 0.44, train acc: 83.53%, consuming tine: 5.28\n",
      "batch: 378, batch train loss: 0.44, train acc: 83.55%, consuming tine: 4.60\n",
      "batch: 379, batch train loss: 0.44, train acc: 83.56%, consuming tine: 4.69\n",
      "batch: 380, batch train loss: 0.44, train acc: 83.59%, consuming tine: 4.38\n",
      "batch: 381, batch train loss: 0.44, train acc: 83.60%, consuming tine: 4.39\n",
      "batch: 382, batch train loss: 0.44, train acc: 83.61%, consuming tine: 4.59\n",
      "batch: 383, batch train loss: 0.43, train acc: 83.63%, consuming tine: 4.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 384, batch train loss: 0.43, train acc: 83.64%, consuming tine: 4.49\n",
      "batch: 385, batch train loss: 0.43, train acc: 83.66%, consuming tine: 4.80\n",
      "batch: 386, batch train loss: 0.43, train acc: 83.68%, consuming tine: 4.38\n",
      "batch: 387, batch train loss: 0.43, train acc: 83.69%, consuming tine: 4.69\n",
      "batch: 388, batch train loss: 0.43, train acc: 83.71%, consuming tine: 4.58\n",
      "batch: 389, batch train loss: 0.43, train acc: 83.73%, consuming tine: 4.50\n",
      "batch: 390, batch train loss: 0.43, train acc: 83.74%, consuming tine: 4.68\n",
      "batch: 391, batch train loss: 0.43, train acc: 83.76%, consuming tine: 4.78\n",
      "batch: 392, batch train loss: 0.43, train acc: 83.78%, consuming tine: 4.71\n",
      "batch: 393, batch train loss: 0.43, train acc: 83.79%, consuming tine: 4.47\n",
      "batch: 394, batch train loss: 0.43, train acc: 83.80%, consuming tine: 4.57\n",
      "batch: 395, batch train loss: 0.43, train acc: 83.80%, consuming tine: 4.50\n",
      "batch: 396, batch train loss: 0.43, train acc: 83.82%, consuming tine: 4.59\n",
      "batch: 397, batch train loss: 0.43, train acc: 83.83%, consuming tine: 4.50\n",
      "batch: 398, batch train loss: 0.43, train acc: 83.84%, consuming tine: 4.58\n",
      "batch: 399, batch train loss: 0.43, train acc: 83.86%, consuming tine: 4.57\n",
      "batch: 400, batch train loss: 0.43, train acc: 83.87%, consuming tine: 4.88\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 2.50, valid acc: 36.81%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.43, train acc: 83.88%, consuming tine: 4.73\n",
      "batch: 402, batch train loss: 0.43, train acc: 83.89%, consuming tine: 4.79\n",
      "batch: 403, batch train loss: 0.43, train acc: 83.90%, consuming tine: 4.49\n",
      "batch: 404, batch train loss: 0.43, train acc: 83.92%, consuming tine: 4.59\n",
      "batch: 405, batch train loss: 0.43, train acc: 83.94%, consuming tine: 4.59\n",
      "batch: 406, batch train loss: 0.43, train acc: 83.96%, consuming tine: 4.60\n",
      "batch: 407, batch train loss: 0.43, train acc: 83.98%, consuming tine: 4.47\n",
      "batch: 408, batch train loss: 0.43, train acc: 83.99%, consuming tine: 4.89\n",
      "batch: 409, batch train loss: 0.43, train acc: 84.00%, consuming tine: 4.69\n",
      "batch: 410, batch train loss: 0.43, train acc: 84.01%, consuming tine: 4.50\n",
      "batch: 411, batch train loss: 0.43, train acc: 84.01%, consuming tine: 4.55\n",
      "batch: 412, batch train loss: 0.43, train acc: 84.03%, consuming tine: 4.33\n",
      "batch: 413, batch train loss: 0.43, train acc: 84.04%, consuming tine: 4.66\n",
      "batch: 414, batch train loss: 0.42, train acc: 84.06%, consuming tine: 4.79\n",
      "batch: 415, batch train loss: 0.42, train acc: 84.08%, consuming tine: 4.48\n",
      "batch: 416, batch train loss: 0.42, train acc: 84.09%, consuming tine: 4.70\n",
      "batch: 417, batch train loss: 0.42, train acc: 84.10%, consuming tine: 4.58\n",
      "batch: 418, batch train loss: 0.42, train acc: 84.11%, consuming tine: 4.58\n",
      "batch: 419, batch train loss: 0.42, train acc: 84.12%, consuming tine: 4.70\n",
      "batch: 420, batch train loss: 0.42, train acc: 84.14%, consuming tine: 4.70\n",
      "batch: 421, batch train loss: 0.42, train acc: 84.16%, consuming tine: 4.67\n",
      "batch: 422, batch train loss: 0.42, train acc: 84.17%, consuming tine: 4.60\n",
      "batch: 423, batch train loss: 0.42, train acc: 84.19%, consuming tine: 4.68\n",
      "batch: 424, batch train loss: 0.42, train acc: 84.21%, consuming tine: 4.78\n",
      "batch: 425, batch train loss: 0.42, train acc: 84.22%, consuming tine: 4.19\n",
      "batch: 426, batch train loss: 0.42, train acc: 84.23%, consuming tine: 4.60\n",
      "batch: 427, batch train loss: 0.42, train acc: 84.24%, consuming tine: 4.58\n",
      "batch: 428, batch train loss: 0.42, train acc: 84.25%, consuming tine: 4.59\n",
      "batch: 429, batch train loss: 0.42, train acc: 84.26%, consuming tine: 4.78\n",
      "batch: 430, batch train loss: 0.42, train acc: 84.28%, consuming tine: 4.70\n",
      "batch: 431, batch train loss: 0.42, train acc: 84.29%, consuming tine: 4.57\n",
      "batch: 432, batch train loss: 0.42, train acc: 84.30%, consuming tine: 4.40\n",
      "batch: 433, batch train loss: 0.42, train acc: 84.32%, consuming tine: 4.67\n",
      "batch: 434, batch train loss: 0.42, train acc: 84.33%, consuming tine: 4.71\n",
      "batch: 435, batch train loss: 0.42, train acc: 84.35%, consuming tine: 4.69\n",
      "batch: 436, batch train loss: 0.42, train acc: 84.35%, consuming tine: 4.47\n",
      "batch: 437, batch train loss: 0.42, train acc: 84.37%, consuming tine: 4.68\n",
      "batch: 438, batch train loss: 0.42, train acc: 84.37%, consuming tine: 4.80\n",
      "batch: 439, batch train loss: 0.42, train acc: 84.38%, consuming tine: 4.48\n",
      "batch: 440, batch train loss: 0.42, train acc: 84.39%, consuming tine: 4.49\n",
      "batch: 441, batch train loss: 0.42, train acc: 84.40%, consuming tine: 4.68\n",
      "batch: 442, batch train loss: 0.42, train acc: 84.42%, consuming tine: 4.69\n",
      "batch: 443, batch train loss: 0.42, train acc: 84.43%, consuming tine: 4.58\n",
      "batch: 444, batch train loss: 0.42, train acc: 84.45%, consuming tine: 4.59\n",
      "batch: 445, batch train loss: 0.41, train acc: 84.46%, consuming tine: 4.58\n",
      "batch: 446, batch train loss: 0.41, train acc: 84.48%, consuming tine: 4.69\n",
      "batch: 447, batch train loss: 0.41, train acc: 84.49%, consuming tine: 4.59\n",
      "batch: 448, batch train loss: 0.41, train acc: 84.50%, consuming tine: 5.00\n",
      "batch: 449, batch train loss: 0.41, train acc: 84.52%, consuming tine: 4.47\n",
      "batch: 450, batch train loss: 0.41, train acc: 84.53%, consuming tine: 4.49\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 2.53, valid acc: 36.79%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.41, train acc: 84.54%, consuming tine: 4.44\n",
      "batch: 452, batch train loss: 0.41, train acc: 84.55%, consuming tine: 4.79\n",
      "batch: 453, batch train loss: 0.41, train acc: 84.57%, consuming tine: 4.58\n",
      "batch: 454, batch train loss: 0.41, train acc: 84.58%, consuming tine: 4.61\n",
      "batch: 455, batch train loss: 0.41, train acc: 84.59%, consuming tine: 4.67\n",
      "batch: 456, batch train loss: 0.41, train acc: 84.61%, consuming tine: 4.69\n",
      "batch: 457, batch train loss: 0.41, train acc: 84.63%, consuming tine: 4.50\n",
      "batch: 458, batch train loss: 0.41, train acc: 84.64%, consuming tine: 4.78\n",
      "batch: 459, batch train loss: 0.41, train acc: 84.65%, consuming tine: 4.70\n",
      "batch: 460, batch train loss: 0.41, train acc: 84.67%, consuming tine: 4.87\n",
      "batch: 461, batch train loss: 0.41, train acc: 84.68%, consuming tine: 4.70\n",
      "batch: 462, batch train loss: 0.41, train acc: 84.70%, consuming tine: 4.58\n",
      "batch: 463, batch train loss: 0.41, train acc: 84.72%, consuming tine: 4.50\n",
      "batch: 464, batch train loss: 0.41, train acc: 84.73%, consuming tine: 4.88\n",
      "batch: 465, batch train loss: 0.41, train acc: 84.74%, consuming tine: 4.50\n",
      "batch: 466, batch train loss: 0.41, train acc: 84.76%, consuming tine: 4.78\n",
      "batch: 467, batch train loss: 0.41, train acc: 84.78%, consuming tine: 4.59\n",
      "batch: 468, batch train loss: 0.41, train acc: 84.79%, consuming tine: 4.79\n",
      "batch: 469, batch train loss: 0.41, train acc: 84.81%, consuming tine: 4.48\n",
      "batch: 470, batch train loss: 0.41, train acc: 84.82%, consuming tine: 4.61\n",
      "batch: 471, batch train loss: 0.41, train acc: 84.84%, consuming tine: 4.88\n",
      "batch: 472, batch train loss: 0.41, train acc: 84.85%, consuming tine: 4.68\n",
      "batch: 473, batch train loss: 0.41, train acc: 84.86%, consuming tine: 4.90\n",
      "batch: 474, batch train loss: 0.40, train acc: 84.88%, consuming tine: 4.58\n",
      "batch: 475, batch train loss: 0.40, train acc: 84.89%, consuming tine: 4.49\n",
      "batch: 476, batch train loss: 0.40, train acc: 84.90%, consuming tine: 4.60\n",
      "batch: 477, batch train loss: 0.40, train acc: 84.92%, consuming tine: 4.62\n",
      "batch: 478, batch train loss: 0.40, train acc: 84.93%, consuming tine: 4.54\n",
      "batch: 479, batch train loss: 0.40, train acc: 84.95%, consuming tine: 4.54\n",
      "batch: 480, batch train loss: 0.40, train acc: 84.96%, consuming tine: 4.62\n",
      "batch: 481, batch train loss: 0.40, train acc: 84.98%, consuming tine: 4.50\n",
      "batch: 482, batch train loss: 0.40, train acc: 84.99%, consuming tine: 4.49\n",
      "batch: 483, batch train loss: 0.40, train acc: 85.01%, consuming tine: 4.89\n",
      "batch: 484, batch train loss: 0.40, train acc: 85.03%, consuming tine: 4.60\n",
      "batch: 485, batch train loss: 0.40, train acc: 85.04%, consuming tine: 4.57\n",
      "batch: 486, batch train loss: 0.40, train acc: 85.06%, consuming tine: 4.49\n",
      "batch: 487, batch train loss: 0.40, train acc: 85.07%, consuming tine: 4.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 488, batch train loss: 0.40, train acc: 85.08%, consuming tine: 4.40\n",
      "batch: 489, batch train loss: 0.40, train acc: 85.09%, consuming tine: 4.39\n",
      "batch: 490, batch train loss: 0.40, train acc: 85.10%, consuming tine: 4.70\n",
      "batch: 491, batch train loss: 0.40, train acc: 85.12%, consuming tine: 4.41\n",
      "batch: 492, batch train loss: 0.40, train acc: 85.13%, consuming tine: 4.36\n",
      "batch: 493, batch train loss: 0.40, train acc: 85.14%, consuming tine: 4.61\n",
      "batch: 494, batch train loss: 0.40, train acc: 85.15%, consuming tine: 4.57\n",
      "batch: 495, batch train loss: 0.40, train acc: 85.17%, consuming tine: 4.59\n",
      "batch: 496, batch train loss: 0.40, train acc: 85.18%, consuming tine: 4.79\n",
      "batch: 497, batch train loss: 0.40, train acc: 85.19%, consuming tine: 4.67\n",
      "batch: 498, batch train loss: 0.40, train acc: 85.20%, consuming tine: 4.60\n",
      "batch: 499, batch train loss: 0.40, train acc: 85.21%, consuming tine: 4.80\n",
      "batch: 500, batch train loss: 0.40, train acc: 85.23%, consuming tine: 4.91\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 2.57, valid acc: 36.71%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.40, train acc: 85.24%, consuming tine: 4.65\n",
      "batch: 502, batch train loss: 0.40, train acc: 85.25%, consuming tine: 4.83\n",
      "batch: 503, batch train loss: 0.40, train acc: 85.26%, consuming tine: 4.85\n",
      "batch: 504, batch train loss: 0.40, train acc: 85.26%, consuming tine: 4.51\n",
      "batch: 505, batch train loss: 0.40, train acc: 85.27%, consuming tine: 5.01\n",
      "batch: 506, batch train loss: 0.39, train acc: 85.28%, consuming tine: 4.67\n",
      "batch: 507, batch train loss: 0.39, train acc: 85.30%, consuming tine: 4.41\n",
      "batch: 508, batch train loss: 0.39, train acc: 85.31%, consuming tine: 4.48\n",
      "batch: 509, batch train loss: 0.39, train acc: 85.33%, consuming tine: 4.53\n",
      "batch: 510, batch train loss: 0.39, train acc: 85.34%, consuming tine: 4.53\n",
      "batch: 511, batch train loss: 0.39, train acc: 85.35%, consuming tine: 4.59\n",
      "batch: 512, batch train loss: 0.39, train acc: 85.35%, consuming tine: 4.70\n",
      "batch: 513, batch train loss: 0.39, train acc: 85.35%, consuming tine: 4.78\n",
      "batch: 514, batch train loss: 0.39, train acc: 85.36%, consuming tine: 4.49\n",
      "batch: 515, batch train loss: 0.39, train acc: 85.36%, consuming tine: 4.78\n",
      "batch: 516, batch train loss: 0.39, train acc: 85.37%, consuming tine: 4.60\n",
      "batch: 517, batch train loss: 0.39, train acc: 85.38%, consuming tine: 4.48\n",
      "batch: 518, batch train loss: 0.39, train acc: 85.39%, consuming tine: 4.67\n",
      "batch: 519, batch train loss: 0.39, train acc: 85.39%, consuming tine: 4.61\n",
      "batch: 520, batch train loss: 0.39, train acc: 85.40%, consuming tine: 4.69\n",
      "batch: 521, batch train loss: 0.39, train acc: 85.41%, consuming tine: 4.61\n",
      "batch: 522, batch train loss: 0.39, train acc: 85.42%, consuming tine: 4.77\n",
      "batch: 523, batch train loss: 0.39, train acc: 85.42%, consuming tine: 4.71\n",
      "batch: 524, batch train loss: 0.39, train acc: 85.44%, consuming tine: 4.63\n",
      "batch: 525, batch train loss: 0.39, train acc: 85.44%, consuming tine: 4.44\n",
      "batch: 526, batch train loss: 0.39, train acc: 85.45%, consuming tine: 4.57\n",
      "batch: 527, batch train loss: 0.39, train acc: 85.47%, consuming tine: 4.79\n",
      "batch: 528, batch train loss: 0.39, train acc: 85.48%, consuming tine: 4.49\n",
      "batch: 529, batch train loss: 0.39, train acc: 85.49%, consuming tine: 4.60\n",
      "batch: 530, batch train loss: 0.39, train acc: 85.51%, consuming tine: 4.73\n",
      "batch: 531, batch train loss: 0.39, train acc: 85.52%, consuming tine: 4.65\n",
      "batch: 532, batch train loss: 0.39, train acc: 85.52%, consuming tine: 4.69\n",
      "batch: 533, batch train loss: 0.39, train acc: 85.53%, consuming tine: 4.57\n",
      "batch: 534, batch train loss: 0.39, train acc: 85.54%, consuming tine: 4.61\n",
      "batch: 535, batch train loss: 0.39, train acc: 85.56%, consuming tine: 4.73\n",
      "batch: 536, batch train loss: 0.39, train acc: 85.57%, consuming tine: 4.83\n",
      "batch: 537, batch train loss: 0.39, train acc: 85.58%, consuming tine: 4.59\n",
      "batch: 538, batch train loss: 0.39, train acc: 85.58%, consuming tine: 4.40\n",
      "batch: 539, batch train loss: 0.39, train acc: 85.59%, consuming tine: 4.78\n",
      "batch: 540, batch train loss: 0.39, train acc: 85.60%, consuming tine: 4.28\n",
      "batch: 541, batch train loss: 0.39, train acc: 85.62%, consuming tine: 4.80\n",
      "batch: 542, batch train loss: 0.39, train acc: 85.63%, consuming tine: 4.68\n",
      "batch: 543, batch train loss: 0.39, train acc: 85.64%, consuming tine: 4.47\n",
      "batch: 544, batch train loss: 0.39, train acc: 85.65%, consuming tine: 4.71\n",
      "batch: 545, batch train loss: 0.39, train acc: 85.66%, consuming tine: 4.68\n",
      "batch: 546, batch train loss: 0.39, train acc: 85.67%, consuming tine: 4.68\n",
      "batch: 547, batch train loss: 0.39, train acc: 85.68%, consuming tine: 4.59\n",
      "batch: 548, batch train loss: 0.38, train acc: 85.69%, consuming tine: 4.59\n",
      "batch: 549, batch train loss: 0.38, train acc: 85.71%, consuming tine: 4.68\n",
      "batch: 550, batch train loss: 0.38, train acc: 85.72%, consuming tine: 4.60\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 2.61, valid acc: 36.71%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.38, train acc: 85.72%, consuming tine: 4.89\n",
      "batch: 552, batch train loss: 0.38, train acc: 85.74%, consuming tine: 4.84\n",
      "batch: 553, batch train loss: 0.38, train acc: 85.75%, consuming tine: 4.55\n",
      "batch: 554, batch train loss: 0.38, train acc: 85.76%, consuming tine: 4.47\n",
      "batch: 555, batch train loss: 0.38, train acc: 85.77%, consuming tine: 4.68\n",
      "batch: 556, batch train loss: 0.38, train acc: 85.79%, consuming tine: 4.50\n",
      "batch: 557, batch train loss: 0.38, train acc: 85.80%, consuming tine: 4.51\n",
      "batch: 558, batch train loss: 0.38, train acc: 85.82%, consuming tine: 4.58\n",
      "batch: 559, batch train loss: 0.38, train acc: 85.83%, consuming tine: 4.61\n",
      "batch: 560, batch train loss: 0.38, train acc: 85.84%, consuming tine: 4.46\n",
      "batch: 561, batch train loss: 0.38, train acc: 85.85%, consuming tine: 4.50\n",
      "batch: 562, batch train loss: 0.38, train acc: 85.86%, consuming tine: 4.68\n",
      "batch: 563, batch train loss: 0.38, train acc: 85.88%, consuming tine: 4.39\n",
      "batch: 564, batch train loss: 0.38, train acc: 85.89%, consuming tine: 4.80\n",
      "batch: 565, batch train loss: 0.38, train acc: 85.90%, consuming tine: 4.50\n",
      "batch: 566, batch train loss: 0.38, train acc: 85.91%, consuming tine: 4.57\n",
      "batch: 567, batch train loss: 0.38, train acc: 85.92%, consuming tine: 4.58\n",
      "batch: 568, batch train loss: 0.38, train acc: 85.93%, consuming tine: 4.58\n",
      "batch: 569, batch train loss: 0.38, train acc: 85.94%, consuming tine: 4.50\n",
      "batch: 570, batch train loss: 0.38, train acc: 85.96%, consuming tine: 4.70\n",
      "batch: 571, batch train loss: 0.38, train acc: 85.96%, consuming tine: 4.57\n",
      "batch: 572, batch train loss: 0.38, train acc: 85.97%, consuming tine: 4.69\n",
      "batch: 573, batch train loss: 0.38, train acc: 85.98%, consuming tine: 4.99\n",
      "batch: 574, batch train loss: 0.38, train acc: 85.99%, consuming tine: 4.58\n",
      "batch: 575, batch train loss: 0.38, train acc: 86.00%, consuming tine: 4.51\n",
      "batch: 576, batch train loss: 0.38, train acc: 86.01%, consuming tine: 4.85\n",
      "batch: 577, batch train loss: 0.38, train acc: 86.02%, consuming tine: 4.72\n",
      "batch: 578, batch train loss: 0.38, train acc: 86.02%, consuming tine: 4.58\n",
      "batch: 579, batch train loss: 0.38, train acc: 86.02%, consuming tine: 4.68\n",
      "batch: 580, batch train loss: 0.38, train acc: 86.02%, consuming tine: 4.69\n",
      "batch: 581, batch train loss: 0.38, train acc: 86.03%, consuming tine: 4.69\n",
      "batch: 582, batch train loss: 0.38, train acc: 86.03%, consuming tine: 4.69\n",
      "batch: 583, batch train loss: 0.38, train acc: 86.04%, consuming tine: 4.59\n",
      "batch: 584, batch train loss: 0.38, train acc: 86.05%, consuming tine: 4.59\n",
      "batch: 585, batch train loss: 0.38, train acc: 86.06%, consuming tine: 4.69\n",
      "batch: 586, batch train loss: 0.38, train acc: 86.06%, consuming tine: 4.38\n",
      "batch: 587, batch train loss: 0.38, train acc: 86.06%, consuming tine: 4.58\n",
      "batch: 588, batch train loss: 0.38, train acc: 86.07%, consuming tine: 4.71\n",
      "batch: 589, batch train loss: 0.38, train acc: 86.07%, consuming tine: 4.67\n",
      "batch: 590, batch train loss: 0.38, train acc: 86.08%, consuming tine: 4.49\n",
      "batch: 591, batch train loss: 0.37, train acc: 86.10%, consuming tine: 4.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 592, batch train loss: 0.37, train acc: 86.11%, consuming tine: 4.62\n",
      "batch: 593, batch train loss: 0.37, train acc: 86.12%, consuming tine: 4.39\n",
      "batch: 594, batch train loss: 0.37, train acc: 86.12%, consuming tine: 4.80\n",
      "batch: 595, batch train loss: 0.37, train acc: 86.13%, consuming tine: 4.39\n",
      "batch: 596, batch train loss: 0.37, train acc: 86.14%, consuming tine: 4.78\n",
      "batch: 597, batch train loss: 0.37, train acc: 86.14%, consuming tine: 4.79\n",
      "batch: 598, batch train loss: 0.37, train acc: 86.15%, consuming tine: 4.78\n",
      "batch: 599, batch train loss: 0.37, train acc: 86.16%, consuming tine: 4.58\n",
      "batch: 600, batch train loss: 0.37, train acc: 86.17%, consuming tine: 4.69\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 2.63, valid acc: 36.67%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.37, train acc: 86.18%, consuming tine: 4.42\n",
      "batch: 602, batch train loss: 0.37, train acc: 86.19%, consuming tine: 4.65\n",
      "batch: 603, batch train loss: 0.37, train acc: 86.20%, consuming tine: 4.65\n",
      "batch: 604, batch train loss: 0.37, train acc: 86.21%, consuming tine: 4.49\n",
      "batch: 605, batch train loss: 0.37, train acc: 86.22%, consuming tine: 4.78\n",
      "batch: 606, batch train loss: 0.37, train acc: 86.22%, consuming tine: 4.70\n",
      "batch: 607, batch train loss: 0.37, train acc: 86.23%, consuming tine: 4.48\n",
      "batch: 608, batch train loss: 0.37, train acc: 86.23%, consuming tine: 4.67\n",
      "batch: 609, batch train loss: 0.37, train acc: 86.24%, consuming tine: 4.49\n",
      "batch: 610, batch train loss: 0.37, train acc: 86.24%, consuming tine: 4.90\n",
      "batch: 611, batch train loss: 0.37, train acc: 86.25%, consuming tine: 4.68\n",
      "batch: 612, batch train loss: 0.37, train acc: 86.26%, consuming tine: 4.40\n",
      "batch: 613, batch train loss: 0.37, train acc: 86.27%, consuming tine: 4.69\n",
      "batch: 614, batch train loss: 0.37, train acc: 86.27%, consuming tine: 4.58\n",
      "batch: 615, batch train loss: 0.37, train acc: 86.28%, consuming tine: 4.39\n",
      "batch: 616, batch train loss: 0.37, train acc: 86.28%, consuming tine: 4.79\n",
      "batch: 617, batch train loss: 0.37, train acc: 86.29%, consuming tine: 4.51\n",
      "batch: 618, batch train loss: 0.37, train acc: 86.29%, consuming tine: 4.57\n",
      "batch: 619, batch train loss: 0.37, train acc: 86.30%, consuming tine: 4.89\n",
      "batch: 620, batch train loss: 0.37, train acc: 86.31%, consuming tine: 4.48\n",
      "batch: 621, batch train loss: 0.37, train acc: 86.32%, consuming tine: 4.69\n",
      "batch: 622, batch train loss: 0.37, train acc: 86.33%, consuming tine: 4.79\n",
      "batch: 623, batch train loss: 0.37, train acc: 86.35%, consuming tine: 4.42\n",
      "batch: 624, batch train loss: 0.37, train acc: 86.36%, consuming tine: 4.46\n",
      "batch: 625, batch train loss: 0.37, train acc: 86.36%, consuming tine: 4.69\n",
      "batch: 626, batch train loss: 0.37, train acc: 86.37%, consuming tine: 4.70\n",
      "batch: 627, batch train loss: 0.37, train acc: 86.38%, consuming tine: 4.68\n",
      "batch: 628, batch train loss: 0.37, train acc: 86.39%, consuming tine: 5.07\n",
      "batch: 629, batch train loss: 0.37, train acc: 86.40%, consuming tine: 4.72\n",
      "batch: 630, batch train loss: 0.37, train acc: 86.41%, consuming tine: 4.61\n",
      "batch: 631, batch train loss: 0.37, train acc: 86.42%, consuming tine: 4.55\n",
      "batch: 632, batch train loss: 0.37, train acc: 86.43%, consuming tine: 5.02\n",
      "batch: 633, batch train loss: 0.37, train acc: 86.44%, consuming tine: 4.76\n",
      "batch: 634, batch train loss: 0.37, train acc: 86.45%, consuming tine: 4.68\n",
      "batch: 635, batch train loss: 0.37, train acc: 86.46%, consuming tine: 4.60\n",
      "batch: 636, batch train loss: 0.37, train acc: 86.47%, consuming tine: 4.69\n",
      "batch: 637, batch train loss: 0.36, train acc: 86.48%, consuming tine: 4.78\n",
      "batch: 638, batch train loss: 0.36, train acc: 86.49%, consuming tine: 4.59\n",
      "batch: 639, batch train loss: 0.36, train acc: 86.50%, consuming tine: 4.50\n",
      "batch: 640, batch train loss: 0.36, train acc: 86.51%, consuming tine: 4.70\n",
      "batch: 641, batch train loss: 0.36, train acc: 86.52%, consuming tine: 4.56\n",
      "batch: 642, batch train loss: 0.36, train acc: 86.52%, consuming tine: 4.60\n",
      "batch: 643, batch train loss: 0.36, train acc: 86.53%, consuming tine: 5.01\n",
      "batch: 644, batch train loss: 0.36, train acc: 86.54%, consuming tine: 4.57\n",
      "batch: 645, batch train loss: 0.36, train acc: 86.55%, consuming tine: 4.69\n",
      "batch: 646, batch train loss: 0.36, train acc: 86.57%, consuming tine: 4.90\n",
      "batch: 647, batch train loss: 0.36, train acc: 86.58%, consuming tine: 4.49\n",
      "batch: 648, batch train loss: 0.36, train acc: 86.59%, consuming tine: 4.57\n",
      "batch: 649, batch train loss: 0.36, train acc: 86.60%, consuming tine: 4.70\n",
      "batch: 650, batch train loss: 0.36, train acc: 86.61%, consuming tine: 4.86\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 2.67, valid acc: 36.66%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.36, train acc: 86.61%, consuming tine: 4.80\n",
      "batch: 652, batch train loss: 0.36, train acc: 86.62%, consuming tine: 4.69\n",
      "batch: 653, batch train loss: 0.36, train acc: 86.63%, consuming tine: 4.69\n",
      "batch: 654, batch train loss: 0.36, train acc: 86.64%, consuming tine: 4.88\n",
      "batch: 655, batch train loss: 0.36, train acc: 86.64%, consuming tine: 4.70\n",
      "batch: 656, batch train loss: 0.36, train acc: 86.65%, consuming tine: 4.68\n",
      "batch: 657, batch train loss: 0.36, train acc: 86.66%, consuming tine: 4.49\n",
      "batch: 658, batch train loss: 0.36, train acc: 86.67%, consuming tine: 4.79\n",
      "batch: 659, batch train loss: 0.36, train acc: 86.67%, consuming tine: 4.88\n",
      "batch: 660, batch train loss: 0.36, train acc: 86.68%, consuming tine: 4.40\n",
      "batch: 661, batch train loss: 0.36, train acc: 86.69%, consuming tine: 4.89\n",
      "batch: 662, batch train loss: 0.36, train acc: 86.70%, consuming tine: 4.78\n",
      "batch: 663, batch train loss: 0.36, train acc: 86.70%, consuming tine: 4.62\n",
      "batch: 664, batch train loss: 0.36, train acc: 86.71%, consuming tine: 4.67\n",
      "batch: 665, batch train loss: 0.36, train acc: 86.71%, consuming tine: 4.86\n",
      "batch: 666, batch train loss: 0.36, train acc: 86.72%, consuming tine: 4.52\n",
      "batch: 667, batch train loss: 0.36, train acc: 86.73%, consuming tine: 5.09\n",
      "batch: 668, batch train loss: 0.36, train acc: 86.74%, consuming tine: 4.48\n",
      "batch: 669, batch train loss: 0.36, train acc: 86.75%, consuming tine: 4.58\n",
      "batch: 670, batch train loss: 0.36, train acc: 86.76%, consuming tine: 4.71\n",
      "batch: 671, batch train loss: 0.36, train acc: 86.76%, consuming tine: 4.74\n",
      "batch: 672, batch train loss: 0.36, train acc: 86.77%, consuming tine: 4.72\n",
      "batch: 673, batch train loss: 0.36, train acc: 86.78%, consuming tine: 4.92\n",
      "batch: 674, batch train loss: 0.36, train acc: 86.79%, consuming tine: 4.56\n",
      "batch: 675, batch train loss: 0.36, train acc: 86.80%, consuming tine: 4.70\n",
      "batch: 676, batch train loss: 0.36, train acc: 86.81%, consuming tine: 4.78\n",
      "batch: 677, batch train loss: 0.36, train acc: 86.81%, consuming tine: 5.08\n",
      "batch: 678, batch train loss: 0.36, train acc: 86.82%, consuming tine: 4.61\n",
      "batch: 679, batch train loss: 0.36, train acc: 86.83%, consuming tine: 4.46\n",
      "batch: 680, batch train loss: 0.36, train acc: 86.84%, consuming tine: 4.68\n",
      "batch: 681, batch train loss: 0.36, train acc: 86.85%, consuming tine: 4.61\n",
      "batch: 682, batch train loss: 0.36, train acc: 86.86%, consuming tine: 4.61\n",
      "batch: 683, batch train loss: 0.36, train acc: 86.87%, consuming tine: 4.65\n",
      "batch: 684, batch train loss: 0.35, train acc: 86.88%, consuming tine: 4.59\n",
      "batch: 685, batch train loss: 0.35, train acc: 86.89%, consuming tine: 4.48\n",
      "batch: 686, batch train loss: 0.35, train acc: 86.90%, consuming tine: 4.49\n",
      "batch: 687, batch train loss: 0.35, train acc: 86.91%, consuming tine: 4.57\n",
      "batch: 688, batch train loss: 0.35, train acc: 86.92%, consuming tine: 4.69\n",
      "batch: 689, batch train loss: 0.35, train acc: 86.92%, consuming tine: 4.50\n",
      "batch: 690, batch train loss: 0.35, train acc: 86.93%, consuming tine: 4.38\n",
      "batch: 691, batch train loss: 0.35, train acc: 86.94%, consuming tine: 4.63\n",
      "batch: 692, batch train loss: 0.35, train acc: 86.95%, consuming tine: 4.45\n",
      "batch: 693, batch train loss: 0.35, train acc: 86.96%, consuming tine: 4.58\n",
      "batch: 694, batch train loss: 0.35, train acc: 86.97%, consuming tine: 4.60\n",
      "batch: 695, batch train loss: 0.35, train acc: 86.98%, consuming tine: 4.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 696, batch train loss: 0.35, train acc: 86.99%, consuming tine: 4.48\n",
      "batch: 697, batch train loss: 0.35, train acc: 87.00%, consuming tine: 4.69\n",
      "batch: 698, batch train loss: 0.35, train acc: 87.01%, consuming tine: 4.49\n",
      "batch: 699, batch train loss: 0.35, train acc: 87.01%, consuming tine: 4.68\n",
      "batch: 700, batch train loss: 0.35, train acc: 87.02%, consuming tine: 4.49\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 2.70, valid acc: 36.60%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.35, train acc: 87.03%, consuming tine: 4.79\n",
      "batch: 702, batch train loss: 0.35, train acc: 87.04%, consuming tine: 4.58\n",
      "batch: 703, batch train loss: 0.35, train acc: 87.05%, consuming tine: 4.69\n",
      "batch: 704, batch train loss: 0.35, train acc: 87.05%, consuming tine: 4.89\n",
      "Epoch 4, Loss: 0.35, Accuracy: 87.05%, Valid Loss: 2.70, Valid Accuracy: 36.60%\n",
      "batch: 1, batch train loss: 0.18, train acc: 93.65%, consuming tine: 4.53\n",
      "batch: 2, batch train loss: 0.19, train acc: 93.31%, consuming tine: 4.69\n",
      "batch: 3, batch train loss: 0.18, train acc: 93.75%, consuming tine: 4.58\n",
      "batch: 4, batch train loss: 0.19, train acc: 93.53%, consuming tine: 4.48\n",
      "batch: 5, batch train loss: 0.19, train acc: 93.44%, consuming tine: 4.60\n",
      "batch: 6, batch train loss: 0.20, train acc: 93.33%, consuming tine: 4.67\n",
      "batch: 7, batch train loss: 0.20, train acc: 93.22%, consuming tine: 4.71\n",
      "batch: 8, batch train loss: 0.20, train acc: 93.21%, consuming tine: 4.48\n",
      "batch: 9, batch train loss: 0.20, train acc: 92.98%, consuming tine: 4.66\n",
      "batch: 10, batch train loss: 0.20, train acc: 93.00%, consuming tine: 4.51\n",
      "batch: 11, batch train loss: 0.20, train acc: 93.16%, consuming tine: 4.59\n",
      "batch: 12, batch train loss: 0.19, train acc: 93.27%, consuming tine: 4.59\n",
      "batch: 13, batch train loss: 0.19, train acc: 93.32%, consuming tine: 4.59\n",
      "batch: 14, batch train loss: 0.20, train acc: 93.25%, consuming tine: 4.59\n",
      "batch: 15, batch train loss: 0.20, train acc: 93.24%, consuming tine: 4.37\n",
      "batch: 16, batch train loss: 0.20, train acc: 93.26%, consuming tine: 4.89\n",
      "batch: 17, batch train loss: 0.20, train acc: 93.16%, consuming tine: 4.58\n",
      "batch: 18, batch train loss: 0.20, train acc: 93.15%, consuming tine: 4.50\n",
      "batch: 19, batch train loss: 0.19, train acc: 93.27%, consuming tine: 4.87\n",
      "batch: 20, batch train loss: 0.19, train acc: 93.30%, consuming tine: 4.61\n",
      "batch: 21, batch train loss: 0.20, train acc: 93.23%, consuming tine: 4.40\n",
      "batch: 22, batch train loss: 0.20, train acc: 93.16%, consuming tine: 4.79\n",
      "batch: 23, batch train loss: 0.20, train acc: 93.02%, consuming tine: 4.57\n",
      "batch: 24, batch train loss: 0.21, train acc: 92.88%, consuming tine: 4.48\n",
      "batch: 25, batch train loss: 0.21, train acc: 92.81%, consuming tine: 4.48\n",
      "batch: 26, batch train loss: 0.21, train acc: 92.81%, consuming tine: 4.58\n",
      "batch: 27, batch train loss: 0.21, train acc: 92.82%, consuming tine: 4.69\n",
      "batch: 28, batch train loss: 0.21, train acc: 92.72%, consuming tine: 4.79\n",
      "batch: 29, batch train loss: 0.21, train acc: 92.66%, consuming tine: 4.70\n",
      "batch: 30, batch train loss: 0.21, train acc: 92.56%, consuming tine: 4.69\n",
      "batch: 31, batch train loss: 0.21, train acc: 92.54%, consuming tine: 4.59\n",
      "batch: 32, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.68\n",
      "batch: 33, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.57\n",
      "batch: 34, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.60\n",
      "batch: 35, batch train loss: 0.21, train acc: 92.58%, consuming tine: 4.66\n",
      "batch: 36, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.52\n",
      "batch: 37, batch train loss: 0.21, train acc: 92.53%, consuming tine: 4.63\n",
      "batch: 38, batch train loss: 0.21, train acc: 92.49%, consuming tine: 4.52\n",
      "batch: 39, batch train loss: 0.22, train acc: 92.38%, consuming tine: 4.48\n",
      "batch: 40, batch train loss: 0.21, train acc: 92.38%, consuming tine: 4.68\n",
      "batch: 41, batch train loss: 0.22, train acc: 92.40%, consuming tine: 4.60\n",
      "batch: 42, batch train loss: 0.22, train acc: 92.36%, consuming tine: 4.58\n",
      "batch: 43, batch train loss: 0.22, train acc: 92.33%, consuming tine: 4.59\n",
      "batch: 44, batch train loss: 0.22, train acc: 92.37%, consuming tine: 4.60\n",
      "batch: 45, batch train loss: 0.21, train acc: 92.43%, consuming tine: 4.58\n",
      "batch: 46, batch train loss: 0.21, train acc: 92.49%, consuming tine: 4.50\n",
      "batch: 47, batch train loss: 0.21, train acc: 92.50%, consuming tine: 4.79\n",
      "batch: 48, batch train loss: 0.21, train acc: 92.49%, consuming tine: 4.49\n",
      "batch: 49, batch train loss: 0.21, train acc: 92.49%, consuming tine: 4.59\n",
      "batch: 50, batch train loss: 0.21, train acc: 92.48%, consuming tine: 4.77\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 3.01, valid acc: 35.35%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.21, train acc: 92.47%, consuming tine: 4.78\n",
      "batch: 52, batch train loss: 0.21, train acc: 92.48%, consuming tine: 4.40\n",
      "batch: 53, batch train loss: 0.21, train acc: 92.50%, consuming tine: 4.58\n",
      "batch: 54, batch train loss: 0.21, train acc: 92.51%, consuming tine: 4.58\n",
      "batch: 55, batch train loss: 0.21, train acc: 92.52%, consuming tine: 4.49\n",
      "batch: 56, batch train loss: 0.21, train acc: 92.54%, consuming tine: 4.48\n",
      "batch: 57, batch train loss: 0.21, train acc: 92.56%, consuming tine: 4.58\n",
      "batch: 58, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.69\n",
      "batch: 59, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.38\n",
      "batch: 60, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.69\n",
      "batch: 61, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.62\n",
      "batch: 62, batch train loss: 0.21, train acc: 92.53%, consuming tine: 4.55\n",
      "batch: 63, batch train loss: 0.21, train acc: 92.51%, consuming tine: 4.88\n",
      "batch: 64, batch train loss: 0.21, train acc: 92.53%, consuming tine: 4.49\n",
      "batch: 65, batch train loss: 0.21, train acc: 92.54%, consuming tine: 4.69\n",
      "batch: 66, batch train loss: 0.21, train acc: 92.56%, consuming tine: 4.69\n",
      "batch: 67, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.90\n",
      "batch: 68, batch train loss: 0.21, train acc: 92.62%, consuming tine: 4.49\n",
      "batch: 69, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.48\n",
      "batch: 70, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.60\n",
      "batch: 71, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.78\n",
      "batch: 72, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.58\n",
      "batch: 73, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.50\n",
      "batch: 74, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.58\n",
      "batch: 75, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.20\n",
      "batch: 76, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.78\n",
      "batch: 77, batch train loss: 0.21, train acc: 92.64%, consuming tine: 4.51\n",
      "batch: 78, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.57\n",
      "batch: 79, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.57\n",
      "batch: 80, batch train loss: 0.21, train acc: 92.68%, consuming tine: 4.60\n",
      "batch: 81, batch train loss: 0.21, train acc: 92.70%, consuming tine: 4.59\n",
      "batch: 82, batch train loss: 0.21, train acc: 92.69%, consuming tine: 4.59\n",
      "batch: 83, batch train loss: 0.21, train acc: 92.71%, consuming tine: 4.59\n",
      "batch: 84, batch train loss: 0.21, train acc: 92.73%, consuming tine: 4.48\n",
      "batch: 85, batch train loss: 0.21, train acc: 92.75%, consuming tine: 4.72\n",
      "batch: 86, batch train loss: 0.21, train acc: 92.76%, consuming tine: 4.60\n",
      "batch: 87, batch train loss: 0.21, train acc: 92.77%, consuming tine: 4.75\n",
      "batch: 88, batch train loss: 0.21, train acc: 92.79%, consuming tine: 4.68\n",
      "batch: 89, batch train loss: 0.21, train acc: 92.81%, consuming tine: 4.72\n",
      "batch: 90, batch train loss: 0.21, train acc: 92.80%, consuming tine: 4.57\n",
      "batch: 91, batch train loss: 0.21, train acc: 92.81%, consuming tine: 4.57\n",
      "batch: 92, batch train loss: 0.21, train acc: 92.80%, consuming tine: 4.84\n",
      "batch: 93, batch train loss: 0.21, train acc: 92.79%, consuming tine: 4.64\n",
      "batch: 94, batch train loss: 0.21, train acc: 92.78%, consuming tine: 4.49\n",
      "batch: 95, batch train loss: 0.21, train acc: 92.79%, consuming tine: 4.69\n",
      "batch: 96, batch train loss: 0.21, train acc: 92.79%, consuming tine: 4.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 97, batch train loss: 0.21, train acc: 92.78%, consuming tine: 4.59\n",
      "batch: 98, batch train loss: 0.21, train acc: 92.76%, consuming tine: 4.79\n",
      "batch: 99, batch train loss: 0.21, train acc: 92.75%, consuming tine: 4.58\n",
      "batch: 100, batch train loss: 0.21, train acc: 92.73%, consuming tine: 4.98\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 3.06, valid acc: 35.89%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.21, train acc: 92.75%, consuming tine: 4.65\n",
      "batch: 102, batch train loss: 0.21, train acc: 92.74%, consuming tine: 4.67\n",
      "batch: 103, batch train loss: 0.21, train acc: 92.75%, consuming tine: 4.59\n",
      "batch: 104, batch train loss: 0.21, train acc: 92.72%, consuming tine: 4.59\n",
      "batch: 105, batch train loss: 0.21, train acc: 92.70%, consuming tine: 4.59\n",
      "batch: 106, batch train loss: 0.21, train acc: 92.67%, consuming tine: 4.60\n",
      "batch: 107, batch train loss: 0.21, train acc: 92.66%, consuming tine: 4.48\n",
      "batch: 108, batch train loss: 0.21, train acc: 92.66%, consuming tine: 4.65\n",
      "batch: 109, batch train loss: 0.21, train acc: 92.67%, consuming tine: 4.62\n",
      "batch: 110, batch train loss: 0.21, train acc: 92.66%, consuming tine: 4.60\n",
      "batch: 111, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.61\n",
      "batch: 112, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.76\n",
      "batch: 113, batch train loss: 0.21, train acc: 92.64%, consuming tine: 4.59\n",
      "batch: 114, batch train loss: 0.21, train acc: 92.64%, consuming tine: 4.39\n",
      "batch: 115, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.60\n",
      "batch: 116, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.58\n",
      "batch: 117, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.78\n",
      "batch: 118, batch train loss: 0.21, train acc: 92.65%, consuming tine: 4.78\n",
      "batch: 119, batch train loss: 0.21, train acc: 92.64%, consuming tine: 4.49\n",
      "batch: 120, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.49\n",
      "batch: 121, batch train loss: 0.21, train acc: 92.62%, consuming tine: 4.69\n",
      "batch: 122, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.48\n",
      "batch: 123, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.69\n",
      "batch: 124, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.49\n",
      "batch: 125, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.59\n",
      "batch: 126, batch train loss: 0.21, train acc: 92.62%, consuming tine: 4.60\n",
      "batch: 127, batch train loss: 0.21, train acc: 92.62%, consuming tine: 4.58\n",
      "batch: 128, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.69\n",
      "batch: 129, batch train loss: 0.21, train acc: 92.61%, consuming tine: 4.58\n",
      "batch: 130, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.51\n",
      "batch: 131, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.47\n",
      "batch: 132, batch train loss: 0.21, train acc: 92.58%, consuming tine: 4.40\n",
      "batch: 133, batch train loss: 0.21, train acc: 92.59%, consuming tine: 4.59\n",
      "batch: 134, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.60\n",
      "batch: 135, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.67\n",
      "batch: 136, batch train loss: 0.21, train acc: 92.56%, consuming tine: 4.48\n",
      "batch: 137, batch train loss: 0.21, train acc: 92.52%, consuming tine: 4.59\n",
      "batch: 138, batch train loss: 0.21, train acc: 92.48%, consuming tine: 4.64\n",
      "batch: 139, batch train loss: 0.21, train acc: 92.46%, consuming tine: 4.53\n",
      "batch: 140, batch train loss: 0.21, train acc: 92.43%, consuming tine: 4.58\n",
      "batch: 141, batch train loss: 0.21, train acc: 92.42%, consuming tine: 4.81\n",
      "batch: 142, batch train loss: 0.21, train acc: 92.42%, consuming tine: 4.46\n",
      "batch: 143, batch train loss: 0.22, train acc: 92.38%, consuming tine: 4.69\n",
      "batch: 144, batch train loss: 0.22, train acc: 92.32%, consuming tine: 4.60\n",
      "batch: 145, batch train loss: 0.22, train acc: 92.29%, consuming tine: 4.69\n",
      "batch: 146, batch train loss: 0.22, train acc: 92.27%, consuming tine: 4.59\n",
      "batch: 147, batch train loss: 0.22, train acc: 92.27%, consuming tine: 4.57\n",
      "batch: 148, batch train loss: 0.22, train acc: 92.27%, consuming tine: 4.57\n",
      "batch: 149, batch train loss: 0.22, train acc: 92.27%, consuming tine: 4.71\n",
      "batch: 150, batch train loss: 0.22, train acc: 92.23%, consuming tine: 4.73\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 3.05, valid acc: 35.28%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.22, train acc: 92.18%, consuming tine: 4.70\n",
      "batch: 152, batch train loss: 0.22, train acc: 92.12%, consuming tine: 4.57\n",
      "batch: 153, batch train loss: 0.22, train acc: 92.08%, consuming tine: 4.59\n",
      "batch: 154, batch train loss: 0.22, train acc: 92.06%, consuming tine: 4.71\n",
      "batch: 155, batch train loss: 0.22, train acc: 92.06%, consuming tine: 4.57\n",
      "batch: 156, batch train loss: 0.22, train acc: 92.07%, consuming tine: 4.50\n",
      "batch: 157, batch train loss: 0.22, train acc: 92.06%, consuming tine: 4.59\n",
      "batch: 158, batch train loss: 0.22, train acc: 92.04%, consuming tine: 4.58\n",
      "batch: 159, batch train loss: 0.22, train acc: 92.00%, consuming tine: 4.69\n",
      "batch: 160, batch train loss: 0.23, train acc: 91.95%, consuming tine: 6.48\n",
      "batch: 161, batch train loss: 0.23, train acc: 91.92%, consuming tine: 4.59\n",
      "batch: 162, batch train loss: 0.23, train acc: 91.91%, consuming tine: 4.60\n",
      "batch: 163, batch train loss: 0.23, train acc: 91.89%, consuming tine: 4.57\n",
      "batch: 164, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.58\n",
      "batch: 165, batch train loss: 0.23, train acc: 91.91%, consuming tine: 4.80\n",
      "batch: 166, batch train loss: 0.23, train acc: 91.92%, consuming tine: 4.58\n",
      "batch: 167, batch train loss: 0.23, train acc: 91.91%, consuming tine: 4.79\n",
      "batch: 168, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.54\n",
      "batch: 169, batch train loss: 0.23, train acc: 91.89%, consuming tine: 4.54\n",
      "batch: 170, batch train loss: 0.23, train acc: 91.88%, consuming tine: 4.75\n",
      "batch: 171, batch train loss: 0.23, train acc: 91.88%, consuming tine: 4.67\n",
      "batch: 172, batch train loss: 0.23, train acc: 91.88%, consuming tine: 4.73\n",
      "batch: 173, batch train loss: 0.23, train acc: 91.89%, consuming tine: 4.58\n",
      "batch: 174, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.48\n",
      "batch: 175, batch train loss: 0.23, train acc: 91.91%, consuming tine: 4.59\n",
      "batch: 176, batch train loss: 0.23, train acc: 91.92%, consuming tine: 4.69\n",
      "batch: 177, batch train loss: 0.23, train acc: 91.92%, consuming tine: 4.65\n",
      "batch: 178, batch train loss: 0.23, train acc: 91.93%, consuming tine: 4.43\n",
      "batch: 179, batch train loss: 0.23, train acc: 91.94%, consuming tine: 4.99\n",
      "batch: 180, batch train loss: 0.23, train acc: 91.95%, consuming tine: 4.67\n",
      "batch: 181, batch train loss: 0.23, train acc: 91.96%, consuming tine: 4.60\n",
      "batch: 182, batch train loss: 0.22, train acc: 91.96%, consuming tine: 4.88\n",
      "batch: 183, batch train loss: 0.22, train acc: 91.97%, consuming tine: 4.59\n",
      "batch: 184, batch train loss: 0.23, train acc: 91.96%, consuming tine: 4.59\n",
      "batch: 185, batch train loss: 0.23, train acc: 91.94%, consuming tine: 4.71\n",
      "batch: 186, batch train loss: 0.23, train acc: 91.92%, consuming tine: 4.67\n",
      "batch: 187, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.70\n",
      "batch: 188, batch train loss: 0.23, train acc: 91.89%, consuming tine: 4.79\n",
      "batch: 189, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.48\n",
      "batch: 190, batch train loss: 0.23, train acc: 91.90%, consuming tine: 4.59\n",
      "batch: 191, batch train loss: 0.23, train acc: 91.89%, consuming tine: 4.55\n",
      "batch: 192, batch train loss: 0.23, train acc: 91.87%, consuming tine: 4.42\n",
      "batch: 193, batch train loss: 0.23, train acc: 91.83%, consuming tine: 4.60\n",
      "batch: 194, batch train loss: 0.23, train acc: 91.81%, consuming tine: 4.59\n",
      "batch: 195, batch train loss: 0.23, train acc: 91.81%, consuming tine: 4.67\n",
      "batch: 196, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.49\n",
      "batch: 197, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.59\n",
      "batch: 198, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.61\n",
      "batch: 199, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.58\n",
      "batch: 200, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "batch: 200, batch valid loss: 3.06, valid acc: 35.33%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.23, train acc: 91.79%, consuming tine: 4.75\n",
      "batch: 202, batch train loss: 0.23, train acc: 91.78%, consuming tine: 4.68\n",
      "batch: 203, batch train loss: 0.23, train acc: 91.77%, consuming tine: 4.50\n",
      "batch: 204, batch train loss: 0.23, train acc: 91.76%, consuming tine: 4.78\n",
      "batch: 205, batch train loss: 0.23, train acc: 91.75%, consuming tine: 4.56\n",
      "batch: 206, batch train loss: 0.23, train acc: 91.73%, consuming tine: 4.63\n",
      "batch: 207, batch train loss: 0.23, train acc: 91.72%, consuming tine: 4.58\n",
      "batch: 208, batch train loss: 0.23, train acc: 91.72%, consuming tine: 4.60\n",
      "batch: 209, batch train loss: 0.23, train acc: 91.73%, consuming tine: 4.57\n",
      "batch: 210, batch train loss: 0.23, train acc: 91.73%, consuming tine: 4.39\n",
      "batch: 211, batch train loss: 0.23, train acc: 91.72%, consuming tine: 4.58\n",
      "batch: 212, batch train loss: 0.23, train acc: 91.72%, consuming tine: 4.81\n",
      "batch: 213, batch train loss: 0.23, train acc: 91.70%, consuming tine: 4.48\n",
      "batch: 214, batch train loss: 0.23, train acc: 91.68%, consuming tine: 4.59\n",
      "batch: 215, batch train loss: 0.23, train acc: 91.67%, consuming tine: 4.50\n",
      "batch: 216, batch train loss: 0.23, train acc: 91.66%, consuming tine: 4.57\n",
      "batch: 217, batch train loss: 0.23, train acc: 91.66%, consuming tine: 4.51\n",
      "batch: 218, batch train loss: 0.23, train acc: 91.67%, consuming tine: 4.78\n",
      "batch: 219, batch train loss: 0.23, train acc: 91.67%, consuming tine: 4.69\n",
      "batch: 220, batch train loss: 0.23, train acc: 91.66%, consuming tine: 4.59\n",
      "batch: 221, batch train loss: 0.23, train acc: 91.65%, consuming tine: 4.78\n",
      "batch: 222, batch train loss: 0.23, train acc: 91.65%, consuming tine: 4.67\n",
      "batch: 223, batch train loss: 0.23, train acc: 91.64%, consuming tine: 4.64\n",
      "batch: 224, batch train loss: 0.23, train acc: 91.63%, consuming tine: 4.79\n",
      "batch: 225, batch train loss: 0.23, train acc: 91.63%, consuming tine: 4.56\n",
      "batch: 226, batch train loss: 0.23, train acc: 91.62%, consuming tine: 4.58\n",
      "batch: 227, batch train loss: 0.23, train acc: 91.59%, consuming tine: 4.73\n",
      "batch: 228, batch train loss: 0.23, train acc: 91.58%, consuming tine: 4.59\n",
      "batch: 229, batch train loss: 0.23, train acc: 91.58%, consuming tine: 4.53\n",
      "batch: 230, batch train loss: 0.23, train acc: 91.57%, consuming tine: 4.80\n",
      "batch: 231, batch train loss: 0.23, train acc: 91.56%, consuming tine: 4.89\n",
      "batch: 232, batch train loss: 0.23, train acc: 91.55%, consuming tine: 4.77\n",
      "batch: 233, batch train loss: 0.24, train acc: 91.51%, consuming tine: 4.50\n",
      "batch: 234, batch train loss: 0.24, train acc: 91.48%, consuming tine: 4.49\n",
      "batch: 235, batch train loss: 0.24, train acc: 91.46%, consuming tine: 4.59\n",
      "batch: 236, batch train loss: 0.24, train acc: 91.46%, consuming tine: 5.18\n",
      "batch: 237, batch train loss: 0.24, train acc: 91.46%, consuming tine: 4.88\n",
      "batch: 238, batch train loss: 0.24, train acc: 91.46%, consuming tine: 5.11\n",
      "batch: 239, batch train loss: 0.24, train acc: 91.44%, consuming tine: 5.28\n",
      "batch: 240, batch train loss: 0.24, train acc: 91.43%, consuming tine: 4.88\n",
      "batch: 241, batch train loss: 0.24, train acc: 91.40%, consuming tine: 5.02\n",
      "batch: 242, batch train loss: 0.24, train acc: 91.38%, consuming tine: 4.86\n",
      "batch: 243, batch train loss: 0.24, train acc: 91.37%, consuming tine: 4.79\n",
      "batch: 244, batch train loss: 0.24, train acc: 91.37%, consuming tine: 5.37\n",
      "batch: 245, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.60\n",
      "batch: 246, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.90\n",
      "batch: 247, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.84\n",
      "batch: 248, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.72\n",
      "batch: 249, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.80\n",
      "batch: 250, batch train loss: 0.24, train acc: 91.37%, consuming tine: 4.77\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 3.05, valid acc: 35.41%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.24, train acc: 91.37%, consuming tine: 4.62\n",
      "batch: 252, batch train loss: 0.24, train acc: 91.36%, consuming tine: 5.07\n",
      "batch: 253, batch train loss: 0.24, train acc: 91.36%, consuming tine: 4.69\n",
      "batch: 254, batch train loss: 0.24, train acc: 91.35%, consuming tine: 5.18\n",
      "batch: 255, batch train loss: 0.24, train acc: 91.33%, consuming tine: 4.58\n",
      "batch: 256, batch train loss: 0.24, train acc: 91.31%, consuming tine: 4.69\n",
      "batch: 257, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.67\n",
      "batch: 258, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.49\n",
      "batch: 259, batch train loss: 0.24, train acc: 91.31%, consuming tine: 5.03\n",
      "batch: 260, batch train loss: 0.24, train acc: 91.31%, consuming tine: 4.63\n",
      "batch: 261, batch train loss: 0.24, train acc: 91.31%, consuming tine: 4.60\n",
      "batch: 262, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.60\n",
      "batch: 263, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.47\n",
      "batch: 264, batch train loss: 0.24, train acc: 91.28%, consuming tine: 4.59\n",
      "batch: 265, batch train loss: 0.24, train acc: 91.28%, consuming tine: 4.60\n",
      "batch: 266, batch train loss: 0.24, train acc: 91.28%, consuming tine: 4.50\n",
      "batch: 267, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.66\n",
      "batch: 268, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.68\n",
      "batch: 269, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.79\n",
      "batch: 270, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.79\n",
      "batch: 271, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.69\n",
      "batch: 272, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.85\n",
      "batch: 273, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.51\n",
      "batch: 274, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.49\n",
      "batch: 275, batch train loss: 0.24, train acc: 91.28%, consuming tine: 5.08\n",
      "batch: 276, batch train loss: 0.24, train acc: 91.27%, consuming tine: 4.59\n",
      "batch: 277, batch train loss: 0.24, train acc: 91.27%, consuming tine: 4.78\n",
      "batch: 278, batch train loss: 0.24, train acc: 91.28%, consuming tine: 4.70\n",
      "batch: 279, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.47\n",
      "batch: 280, batch train loss: 0.24, train acc: 91.29%, consuming tine: 4.58\n",
      "batch: 281, batch train loss: 0.24, train acc: 91.31%, consuming tine: 4.72\n",
      "batch: 282, batch train loss: 0.24, train acc: 91.32%, consuming tine: 5.25\n",
      "batch: 283, batch train loss: 0.24, train acc: 91.32%, consuming tine: 4.82\n",
      "batch: 284, batch train loss: 0.24, train acc: 91.32%, consuming tine: 4.77\n",
      "batch: 285, batch train loss: 0.24, train acc: 91.33%, consuming tine: 5.02\n",
      "batch: 286, batch train loss: 0.24, train acc: 91.33%, consuming tine: 5.06\n",
      "batch: 287, batch train loss: 0.24, train acc: 91.33%, consuming tine: 4.78\n",
      "batch: 288, batch train loss: 0.24, train acc: 91.33%, consuming tine: 5.20\n",
      "batch: 289, batch train loss: 0.24, train acc: 91.33%, consuming tine: 4.47\n",
      "batch: 290, batch train loss: 0.24, train acc: 91.33%, consuming tine: 4.89\n",
      "batch: 291, batch train loss: 0.24, train acc: 91.34%, consuming tine: 4.71\n",
      "batch: 292, batch train loss: 0.24, train acc: 91.35%, consuming tine: 5.28\n",
      "batch: 293, batch train loss: 0.24, train acc: 91.35%, consuming tine: 4.88\n",
      "batch: 294, batch train loss: 0.24, train acc: 91.35%, consuming tine: 4.79\n",
      "batch: 295, batch train loss: 0.24, train acc: 91.33%, consuming tine: 5.09\n",
      "batch: 296, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.89\n",
      "batch: 297, batch train loss: 0.24, train acc: 91.27%, consuming tine: 5.12\n",
      "batch: 298, batch train loss: 0.24, train acc: 91.26%, consuming tine: 4.98\n",
      "batch: 299, batch train loss: 0.24, train acc: 91.26%, consuming tine: 4.77\n",
      "batch: 300, batch train loss: 0.24, train acc: 91.27%, consuming tine: 5.10\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 3.05, valid acc: 35.45%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.24, train acc: 91.27%, consuming tine: 5.32\n",
      "batch: 302, batch train loss: 0.24, train acc: 91.26%, consuming tine: 4.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 303, batch train loss: 0.24, train acc: 91.24%, consuming tine: 5.46\n",
      "batch: 304, batch train loss: 0.24, train acc: 91.21%, consuming tine: 4.79\n",
      "batch: 305, batch train loss: 0.24, train acc: 91.18%, consuming tine: 4.62\n",
      "batch: 306, batch train loss: 0.24, train acc: 91.16%, consuming tine: 4.86\n",
      "batch: 307, batch train loss: 0.24, train acc: 91.16%, consuming tine: 4.98\n",
      "batch: 308, batch train loss: 0.24, train acc: 91.16%, consuming tine: 5.00\n",
      "batch: 309, batch train loss: 0.24, train acc: 91.16%, consuming tine: 5.00\n",
      "batch: 310, batch train loss: 0.24, train acc: 91.17%, consuming tine: 5.17\n",
      "batch: 311, batch train loss: 0.24, train acc: 91.17%, consuming tine: 5.29\n",
      "batch: 312, batch train loss: 0.24, train acc: 91.16%, consuming tine: 5.08\n",
      "batch: 313, batch train loss: 0.24, train acc: 91.15%, consuming tine: 4.91\n",
      "batch: 314, batch train loss: 0.24, train acc: 91.14%, consuming tine: 5.07\n",
      "batch: 315, batch train loss: 0.24, train acc: 91.12%, consuming tine: 4.90\n",
      "batch: 316, batch train loss: 0.24, train acc: 91.11%, consuming tine: 5.80\n",
      "batch: 317, batch train loss: 0.24, train acc: 91.11%, consuming tine: 5.10\n",
      "batch: 318, batch train loss: 0.24, train acc: 91.12%, consuming tine: 4.89\n",
      "batch: 319, batch train loss: 0.24, train acc: 91.12%, consuming tine: 4.98\n",
      "batch: 320, batch train loss: 0.24, train acc: 91.13%, consuming tine: 4.79\n",
      "batch: 321, batch train loss: 0.24, train acc: 91.14%, consuming tine: 5.18\n",
      "batch: 322, batch train loss: 0.24, train acc: 91.14%, consuming tine: 4.85\n",
      "batch: 323, batch train loss: 0.24, train acc: 91.15%, consuming tine: 4.93\n",
      "batch: 324, batch train loss: 0.24, train acc: 91.15%, consuming tine: 5.19\n",
      "batch: 325, batch train loss: 0.24, train acc: 91.15%, consuming tine: 5.38\n",
      "batch: 326, batch train loss: 0.24, train acc: 91.16%, consuming tine: 5.13\n",
      "batch: 327, batch train loss: 0.24, train acc: 91.17%, consuming tine: 5.03\n",
      "batch: 328, batch train loss: 0.24, train acc: 91.18%, consuming tine: 4.98\n",
      "batch: 329, batch train loss: 0.24, train acc: 91.19%, consuming tine: 5.20\n",
      "batch: 330, batch train loss: 0.24, train acc: 91.20%, consuming tine: 5.49\n",
      "batch: 331, batch train loss: 0.24, train acc: 91.20%, consuming tine: 4.60\n",
      "batch: 332, batch train loss: 0.24, train acc: 91.20%, consuming tine: 4.69\n",
      "batch: 333, batch train loss: 0.24, train acc: 91.20%, consuming tine: 5.17\n",
      "batch: 334, batch train loss: 0.24, train acc: 91.20%, consuming tine: 5.31\n",
      "batch: 335, batch train loss: 0.24, train acc: 91.21%, consuming tine: 4.91\n",
      "batch: 336, batch train loss: 0.24, train acc: 91.22%, consuming tine: 5.13\n",
      "batch: 337, batch train loss: 0.24, train acc: 91.24%, consuming tine: 5.11\n",
      "batch: 338, batch train loss: 0.24, train acc: 91.24%, consuming tine: 5.39\n",
      "batch: 339, batch train loss: 0.24, train acc: 91.25%, consuming tine: 4.87\n",
      "batch: 340, batch train loss: 0.24, train acc: 91.26%, consuming tine: 4.91\n",
      "batch: 341, batch train loss: 0.24, train acc: 91.27%, consuming tine: 5.38\n",
      "batch: 342, batch train loss: 0.24, train acc: 91.28%, consuming tine: 4.67\n",
      "batch: 343, batch train loss: 0.24, train acc: 91.29%, consuming tine: 5.30\n",
      "batch: 344, batch train loss: 0.24, train acc: 91.30%, consuming tine: 4.90\n",
      "batch: 345, batch train loss: 0.24, train acc: 91.31%, consuming tine: 5.19\n",
      "batch: 346, batch train loss: 0.24, train acc: 91.32%, consuming tine: 4.89\n",
      "batch: 347, batch train loss: 0.24, train acc: 91.33%, consuming tine: 5.06\n",
      "batch: 348, batch train loss: 0.24, train acc: 91.34%, consuming tine: 5.50\n",
      "batch: 349, batch train loss: 0.24, train acc: 91.35%, consuming tine: 4.80\n",
      "batch: 350, batch train loss: 0.24, train acc: 91.36%, consuming tine: 5.20\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 3.08, valid acc: 35.54%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.24, train acc: 91.38%, consuming tine: 5.43\n",
      "batch: 352, batch train loss: 0.24, train acc: 91.39%, consuming tine: 5.06\n",
      "batch: 353, batch train loss: 0.24, train acc: 91.40%, consuming tine: 5.31\n",
      "batch: 354, batch train loss: 0.24, train acc: 91.41%, consuming tine: 4.79\n",
      "batch: 355, batch train loss: 0.24, train acc: 91.43%, consuming tine: 4.79\n",
      "batch: 356, batch train loss: 0.24, train acc: 91.44%, consuming tine: 5.40\n",
      "batch: 357, batch train loss: 0.24, train acc: 91.45%, consuming tine: 5.07\n",
      "batch: 358, batch train loss: 0.24, train acc: 91.45%, consuming tine: 4.98\n",
      "batch: 359, batch train loss: 0.24, train acc: 91.46%, consuming tine: 5.19\n",
      "batch: 360, batch train loss: 0.24, train acc: 91.47%, consuming tine: 5.09\n",
      "batch: 361, batch train loss: 0.24, train acc: 91.47%, consuming tine: 5.20\n",
      "batch: 362, batch train loss: 0.24, train acc: 91.48%, consuming tine: 5.02\n",
      "batch: 363, batch train loss: 0.24, train acc: 91.48%, consuming tine: 4.85\n",
      "batch: 364, batch train loss: 0.24, train acc: 91.49%, consuming tine: 5.09\n",
      "batch: 365, batch train loss: 0.23, train acc: 91.50%, consuming tine: 4.98\n",
      "batch: 366, batch train loss: 0.23, train acc: 91.51%, consuming tine: 5.29\n",
      "batch: 367, batch train loss: 0.23, train acc: 91.52%, consuming tine: 5.09\n",
      "batch: 368, batch train loss: 0.23, train acc: 91.53%, consuming tine: 5.00\n",
      "batch: 369, batch train loss: 0.23, train acc: 91.54%, consuming tine: 4.80\n",
      "batch: 370, batch train loss: 0.23, train acc: 91.54%, consuming tine: 5.27\n",
      "batch: 371, batch train loss: 0.23, train acc: 91.55%, consuming tine: 5.03\n",
      "batch: 372, batch train loss: 0.23, train acc: 91.55%, consuming tine: 5.03\n",
      "batch: 373, batch train loss: 0.23, train acc: 91.56%, consuming tine: 4.71\n",
      "batch: 374, batch train loss: 0.23, train acc: 91.57%, consuming tine: 4.99\n",
      "batch: 375, batch train loss: 0.23, train acc: 91.57%, consuming tine: 5.08\n",
      "batch: 376, batch train loss: 0.23, train acc: 91.58%, consuming tine: 5.27\n",
      "batch: 377, batch train loss: 0.23, train acc: 91.60%, consuming tine: 4.89\n",
      "batch: 378, batch train loss: 0.23, train acc: 91.61%, consuming tine: 4.80\n",
      "batch: 379, batch train loss: 0.23, train acc: 91.62%, consuming tine: 5.06\n",
      "batch: 380, batch train loss: 0.23, train acc: 91.62%, consuming tine: 4.91\n",
      "batch: 381, batch train loss: 0.23, train acc: 91.63%, consuming tine: 5.09\n",
      "batch: 382, batch train loss: 0.23, train acc: 91.63%, consuming tine: 5.18\n",
      "batch: 383, batch train loss: 0.23, train acc: 91.64%, consuming tine: 5.01\n",
      "batch: 384, batch train loss: 0.23, train acc: 91.64%, consuming tine: 5.06\n",
      "batch: 385, batch train loss: 0.23, train acc: 91.65%, consuming tine: 4.68\n",
      "batch: 386, batch train loss: 0.23, train acc: 91.66%, consuming tine: 5.00\n",
      "batch: 387, batch train loss: 0.23, train acc: 91.67%, consuming tine: 4.99\n",
      "batch: 388, batch train loss: 0.23, train acc: 91.68%, consuming tine: 5.00\n",
      "batch: 389, batch train loss: 0.23, train acc: 91.69%, consuming tine: 5.29\n",
      "batch: 390, batch train loss: 0.23, train acc: 91.70%, consuming tine: 5.18\n",
      "batch: 391, batch train loss: 0.23, train acc: 91.70%, consuming tine: 5.08\n",
      "batch: 392, batch train loss: 0.23, train acc: 91.71%, consuming tine: 4.76\n",
      "batch: 393, batch train loss: 0.23, train acc: 91.72%, consuming tine: 4.73\n",
      "batch: 394, batch train loss: 0.23, train acc: 91.73%, consuming tine: 5.08\n",
      "batch: 395, batch train loss: 0.23, train acc: 91.74%, consuming tine: 4.69\n",
      "batch: 396, batch train loss: 0.23, train acc: 91.75%, consuming tine: 4.98\n",
      "batch: 397, batch train loss: 0.23, train acc: 91.75%, consuming tine: 4.77\n",
      "batch: 398, batch train loss: 0.23, train acc: 91.76%, consuming tine: 5.50\n",
      "batch: 399, batch train loss: 0.23, train acc: 91.77%, consuming tine: 5.00\n",
      "batch: 400, batch train loss: 0.23, train acc: 91.77%, consuming tine: 4.99\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 3.12, valid acc: 35.60%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.23, train acc: 91.78%, consuming tine: 5.37\n",
      "batch: 402, batch train loss: 0.23, train acc: 91.80%, consuming tine: 5.19\n",
      "batch: 403, batch train loss: 0.23, train acc: 91.80%, consuming tine: 5.38\n",
      "batch: 404, batch train loss: 0.23, train acc: 91.81%, consuming tine: 5.19\n",
      "batch: 405, batch train loss: 0.23, train acc: 91.82%, consuming tine: 4.87\n",
      "batch: 406, batch train loss: 0.23, train acc: 91.83%, consuming tine: 5.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 407, batch train loss: 0.23, train acc: 91.84%, consuming tine: 4.90\n",
      "batch: 408, batch train loss: 0.23, train acc: 91.84%, consuming tine: 4.60\n",
      "batch: 409, batch train loss: 0.23, train acc: 91.85%, consuming tine: 5.27\n",
      "batch: 410, batch train loss: 0.23, train acc: 91.86%, consuming tine: 5.02\n",
      "batch: 411, batch train loss: 0.23, train acc: 91.87%, consuming tine: 4.86\n",
      "batch: 412, batch train loss: 0.22, train acc: 91.88%, consuming tine: 4.91\n",
      "batch: 413, batch train loss: 0.22, train acc: 91.89%, consuming tine: 4.68\n",
      "batch: 414, batch train loss: 0.22, train acc: 91.89%, consuming tine: 5.24\n",
      "batch: 415, batch train loss: 0.22, train acc: 91.90%, consuming tine: 4.81\n",
      "batch: 416, batch train loss: 0.22, train acc: 91.90%, consuming tine: 4.97\n",
      "batch: 417, batch train loss: 0.22, train acc: 91.90%, consuming tine: 5.10\n",
      "batch: 418, batch train loss: 0.22, train acc: 91.90%, consuming tine: 4.36\n",
      "batch: 419, batch train loss: 0.22, train acc: 91.91%, consuming tine: 5.11\n",
      "batch: 420, batch train loss: 0.22, train acc: 91.91%, consuming tine: 4.85\n",
      "batch: 421, batch train loss: 0.22, train acc: 91.92%, consuming tine: 4.92\n",
      "batch: 422, batch train loss: 0.22, train acc: 91.93%, consuming tine: 5.09\n",
      "batch: 423, batch train loss: 0.22, train acc: 91.94%, consuming tine: 5.00\n",
      "batch: 424, batch train loss: 0.22, train acc: 91.94%, consuming tine: 4.85\n",
      "batch: 425, batch train loss: 0.22, train acc: 91.95%, consuming tine: 5.16\n",
      "batch: 426, batch train loss: 0.22, train acc: 91.95%, consuming tine: 4.95\n",
      "batch: 427, batch train loss: 0.22, train acc: 91.95%, consuming tine: 5.07\n",
      "batch: 428, batch train loss: 0.22, train acc: 91.96%, consuming tine: 5.51\n",
      "batch: 429, batch train loss: 0.22, train acc: 91.97%, consuming tine: 4.89\n",
      "batch: 430, batch train loss: 0.22, train acc: 91.98%, consuming tine: 5.19\n",
      "batch: 431, batch train loss: 0.22, train acc: 91.98%, consuming tine: 4.91\n",
      "batch: 432, batch train loss: 0.22, train acc: 91.99%, consuming tine: 4.96\n",
      "batch: 433, batch train loss: 0.22, train acc: 92.00%, consuming tine: 5.08\n",
      "batch: 434, batch train loss: 0.22, train acc: 92.00%, consuming tine: 5.08\n",
      "batch: 435, batch train loss: 0.22, train acc: 92.01%, consuming tine: 5.20\n",
      "batch: 436, batch train loss: 0.22, train acc: 92.01%, consuming tine: 5.71\n",
      "batch: 437, batch train loss: 0.22, train acc: 92.02%, consuming tine: 4.70\n",
      "batch: 438, batch train loss: 0.22, train acc: 92.02%, consuming tine: 4.96\n",
      "batch: 439, batch train loss: 0.22, train acc: 92.03%, consuming tine: 4.99\n",
      "batch: 440, batch train loss: 0.22, train acc: 92.03%, consuming tine: 5.52\n",
      "batch: 441, batch train loss: 0.22, train acc: 92.04%, consuming tine: 5.04\n",
      "batch: 442, batch train loss: 0.22, train acc: 92.05%, consuming tine: 5.02\n",
      "batch: 443, batch train loss: 0.22, train acc: 92.06%, consuming tine: 4.78\n",
      "batch: 444, batch train loss: 0.22, train acc: 92.06%, consuming tine: 4.87\n",
      "batch: 445, batch train loss: 0.22, train acc: 92.07%, consuming tine: 5.03\n",
      "batch: 446, batch train loss: 0.22, train acc: 92.07%, consuming tine: 4.75\n",
      "batch: 447, batch train loss: 0.22, train acc: 92.08%, consuming tine: 5.78\n",
      "batch: 448, batch train loss: 0.22, train acc: 92.09%, consuming tine: 5.01\n",
      "batch: 449, batch train loss: 0.22, train acc: 92.10%, consuming tine: 4.98\n",
      "batch: 450, batch train loss: 0.22, train acc: 92.11%, consuming tine: 5.48\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 3.15, valid acc: 35.64%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.22, train acc: 92.11%, consuming tine: 4.48\n",
      "batch: 452, batch train loss: 0.22, train acc: 92.12%, consuming tine: 4.78\n",
      "batch: 453, batch train loss: 0.22, train acc: 92.13%, consuming tine: 5.10\n",
      "batch: 454, batch train loss: 0.22, train acc: 92.14%, consuming tine: 4.39\n",
      "batch: 455, batch train loss: 0.22, train acc: 92.15%, consuming tine: 4.97\n",
      "batch: 456, batch train loss: 0.22, train acc: 92.15%, consuming tine: 4.91\n",
      "batch: 457, batch train loss: 0.22, train acc: 92.16%, consuming tine: 5.36\n",
      "batch: 458, batch train loss: 0.22, train acc: 92.16%, consuming tine: 5.00\n",
      "batch: 459, batch train loss: 0.22, train acc: 92.17%, consuming tine: 4.79\n",
      "batch: 460, batch train loss: 0.22, train acc: 92.17%, consuming tine: 5.08\n",
      "batch: 461, batch train loss: 0.22, train acc: 92.18%, consuming tine: 5.11\n",
      "batch: 462, batch train loss: 0.22, train acc: 92.18%, consuming tine: 4.87\n",
      "batch: 463, batch train loss: 0.22, train acc: 92.19%, consuming tine: 4.88\n",
      "batch: 464, batch train loss: 0.22, train acc: 92.20%, consuming tine: 4.88\n",
      "batch: 465, batch train loss: 0.22, train acc: 92.20%, consuming tine: 5.00\n",
      "batch: 466, batch train loss: 0.22, train acc: 92.21%, consuming tine: 5.21\n",
      "batch: 467, batch train loss: 0.22, train acc: 92.22%, consuming tine: 5.47\n",
      "batch: 468, batch train loss: 0.22, train acc: 92.23%, consuming tine: 5.06\n",
      "batch: 469, batch train loss: 0.22, train acc: 92.23%, consuming tine: 5.12\n",
      "batch: 470, batch train loss: 0.22, train acc: 92.24%, consuming tine: 4.88\n",
      "batch: 471, batch train loss: 0.22, train acc: 92.24%, consuming tine: 4.81\n",
      "batch: 472, batch train loss: 0.22, train acc: 92.25%, consuming tine: 5.09\n",
      "batch: 473, batch train loss: 0.21, train acc: 92.26%, consuming tine: 4.76\n",
      "batch: 474, batch train loss: 0.21, train acc: 92.26%, consuming tine: 5.27\n",
      "batch: 475, batch train loss: 0.21, train acc: 92.27%, consuming tine: 5.38\n",
      "batch: 476, batch train loss: 0.21, train acc: 92.28%, consuming tine: 5.40\n",
      "batch: 477, batch train loss: 0.21, train acc: 92.29%, consuming tine: 4.81\n",
      "batch: 478, batch train loss: 0.21, train acc: 92.30%, consuming tine: 4.89\n",
      "batch: 479, batch train loss: 0.21, train acc: 92.30%, consuming tine: 5.57\n",
      "batch: 480, batch train loss: 0.21, train acc: 92.31%, consuming tine: 5.61\n",
      "batch: 481, batch train loss: 0.21, train acc: 92.32%, consuming tine: 5.16\n",
      "batch: 482, batch train loss: 0.21, train acc: 92.32%, consuming tine: 5.11\n",
      "batch: 483, batch train loss: 0.21, train acc: 92.33%, consuming tine: 5.48\n",
      "batch: 484, batch train loss: 0.21, train acc: 92.34%, consuming tine: 5.21\n",
      "batch: 485, batch train loss: 0.21, train acc: 92.34%, consuming tine: 5.27\n",
      "batch: 486, batch train loss: 0.21, train acc: 92.35%, consuming tine: 4.98\n",
      "batch: 487, batch train loss: 0.21, train acc: 92.36%, consuming tine: 5.11\n",
      "batch: 488, batch train loss: 0.21, train acc: 92.36%, consuming tine: 5.26\n",
      "batch: 489, batch train loss: 0.21, train acc: 92.37%, consuming tine: 5.21\n",
      "batch: 490, batch train loss: 0.21, train acc: 92.38%, consuming tine: 5.16\n",
      "batch: 491, batch train loss: 0.21, train acc: 92.39%, consuming tine: 5.01\n",
      "batch: 492, batch train loss: 0.21, train acc: 92.40%, consuming tine: 5.08\n",
      "batch: 493, batch train loss: 0.21, train acc: 92.40%, consuming tine: 4.98\n",
      "batch: 494, batch train loss: 0.21, train acc: 92.41%, consuming tine: 4.99\n",
      "batch: 495, batch train loss: 0.21, train acc: 92.42%, consuming tine: 4.88\n",
      "batch: 496, batch train loss: 0.21, train acc: 92.42%, consuming tine: 4.89\n",
      "batch: 497, batch train loss: 0.21, train acc: 92.43%, consuming tine: 4.97\n",
      "batch: 498, batch train loss: 0.21, train acc: 92.44%, consuming tine: 4.83\n",
      "batch: 499, batch train loss: 0.21, train acc: 92.45%, consuming tine: 5.63\n",
      "batch: 500, batch train loss: 0.21, train acc: 92.45%, consuming tine: 4.95\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 3.19, valid acc: 35.69%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.21, train acc: 92.46%, consuming tine: 5.07\n",
      "batch: 502, batch train loss: 0.21, train acc: 92.46%, consuming tine: 5.17\n",
      "batch: 503, batch train loss: 0.21, train acc: 92.47%, consuming tine: 5.10\n",
      "batch: 504, batch train loss: 0.21, train acc: 92.48%, consuming tine: 5.36\n",
      "batch: 505, batch train loss: 0.21, train acc: 92.48%, consuming tine: 4.82\n",
      "batch: 506, batch train loss: 0.21, train acc: 92.49%, consuming tine: 5.08\n",
      "batch: 507, batch train loss: 0.21, train acc: 92.49%, consuming tine: 5.50\n",
      "batch: 508, batch train loss: 0.21, train acc: 92.50%, consuming tine: 5.06\n",
      "batch: 509, batch train loss: 0.21, train acc: 92.50%, consuming tine: 5.39\n",
      "batch: 510, batch train loss: 0.21, train acc: 92.51%, consuming tine: 5.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 511, batch train loss: 0.21, train acc: 92.51%, consuming tine: 5.16\n",
      "batch: 512, batch train loss: 0.21, train acc: 92.52%, consuming tine: 5.60\n",
      "batch: 513, batch train loss: 0.21, train acc: 92.53%, consuming tine: 5.04\n",
      "batch: 514, batch train loss: 0.21, train acc: 92.53%, consuming tine: 5.04\n",
      "batch: 515, batch train loss: 0.21, train acc: 92.54%, consuming tine: 4.98\n",
      "batch: 516, batch train loss: 0.21, train acc: 92.54%, consuming tine: 4.60\n",
      "batch: 517, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.98\n",
      "batch: 518, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.78\n",
      "batch: 519, batch train loss: 0.21, train acc: 92.55%, consuming tine: 4.78\n",
      "batch: 520, batch train loss: 0.21, train acc: 92.55%, consuming tine: 5.79\n",
      "batch: 521, batch train loss: 0.21, train acc: 92.55%, consuming tine: 5.00\n",
      "batch: 522, batch train loss: 0.21, train acc: 92.55%, consuming tine: 5.36\n",
      "batch: 523, batch train loss: 0.21, train acc: 92.56%, consuming tine: 5.09\n",
      "batch: 524, batch train loss: 0.21, train acc: 92.56%, consuming tine: 4.80\n",
      "batch: 525, batch train loss: 0.21, train acc: 92.57%, consuming tine: 5.27\n",
      "batch: 526, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.91\n",
      "batch: 527, batch train loss: 0.21, train acc: 92.57%, consuming tine: 4.99\n",
      "batch: 528, batch train loss: 0.21, train acc: 92.57%, consuming tine: 5.70\n",
      "batch: 529, batch train loss: 0.21, train acc: 92.58%, consuming tine: 5.30\n",
      "batch: 530, batch train loss: 0.21, train acc: 92.58%, consuming tine: 5.06\n",
      "batch: 531, batch train loss: 0.21, train acc: 92.58%, consuming tine: 4.89\n",
      "batch: 532, batch train loss: 0.21, train acc: 92.59%, consuming tine: 5.38\n",
      "batch: 533, batch train loss: 0.21, train acc: 92.59%, consuming tine: 5.19\n",
      "batch: 534, batch train loss: 0.21, train acc: 92.60%, consuming tine: 4.78\n",
      "batch: 535, batch train loss: 0.21, train acc: 92.61%, consuming tine: 5.53\n",
      "batch: 536, batch train loss: 0.21, train acc: 92.62%, consuming tine: 5.02\n",
      "batch: 537, batch train loss: 0.21, train acc: 92.62%, consuming tine: 5.31\n",
      "batch: 538, batch train loss: 0.21, train acc: 92.63%, consuming tine: 5.07\n",
      "batch: 539, batch train loss: 0.21, train acc: 92.63%, consuming tine: 5.17\n",
      "batch: 540, batch train loss: 0.21, train acc: 92.63%, consuming tine: 4.99\n",
      "batch: 541, batch train loss: 0.21, train acc: 92.64%, consuming tine: 5.22\n",
      "batch: 542, batch train loss: 0.20, train acc: 92.64%, consuming tine: 5.37\n",
      "batch: 543, batch train loss: 0.20, train acc: 92.65%, consuming tine: 4.79\n",
      "batch: 544, batch train loss: 0.20, train acc: 92.65%, consuming tine: 4.79\n",
      "batch: 545, batch train loss: 0.20, train acc: 92.65%, consuming tine: 5.63\n",
      "batch: 546, batch train loss: 0.20, train acc: 92.66%, consuming tine: 5.23\n",
      "batch: 547, batch train loss: 0.20, train acc: 92.66%, consuming tine: 5.09\n",
      "batch: 548, batch train loss: 0.20, train acc: 92.67%, consuming tine: 4.69\n",
      "batch: 549, batch train loss: 0.20, train acc: 92.67%, consuming tine: 5.08\n",
      "batch: 550, batch train loss: 0.20, train acc: 92.68%, consuming tine: 4.78\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 3.23, valid acc: 35.68%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.20, train acc: 92.68%, consuming tine: 4.84\n",
      "batch: 552, batch train loss: 0.20, train acc: 92.69%, consuming tine: 4.71\n",
      "batch: 553, batch train loss: 0.20, train acc: 92.70%, consuming tine: 4.95\n",
      "batch: 554, batch train loss: 0.20, train acc: 92.70%, consuming tine: 4.78\n",
      "batch: 555, batch train loss: 0.20, train acc: 92.71%, consuming tine: 5.55\n",
      "batch: 556, batch train loss: 0.20, train acc: 92.72%, consuming tine: 5.41\n",
      "batch: 557, batch train loss: 0.20, train acc: 92.72%, consuming tine: 5.17\n",
      "batch: 558, batch train loss: 0.20, train acc: 92.73%, consuming tine: 4.62\n",
      "batch: 559, batch train loss: 0.20, train acc: 92.73%, consuming tine: 5.37\n",
      "batch: 560, batch train loss: 0.20, train acc: 92.74%, consuming tine: 4.70\n",
      "batch: 561, batch train loss: 0.20, train acc: 92.74%, consuming tine: 5.56\n",
      "batch: 562, batch train loss: 0.20, train acc: 92.75%, consuming tine: 5.11\n",
      "batch: 563, batch train loss: 0.20, train acc: 92.76%, consuming tine: 4.98\n",
      "batch: 564, batch train loss: 0.20, train acc: 92.77%, consuming tine: 4.88\n",
      "batch: 565, batch train loss: 0.20, train acc: 92.77%, consuming tine: 4.78\n",
      "batch: 566, batch train loss: 0.20, train acc: 92.78%, consuming tine: 5.28\n",
      "batch: 567, batch train loss: 0.20, train acc: 92.79%, consuming tine: 5.30\n",
      "batch: 568, batch train loss: 0.20, train acc: 92.79%, consuming tine: 4.91\n",
      "batch: 569, batch train loss: 0.20, train acc: 92.80%, consuming tine: 5.11\n",
      "batch: 570, batch train loss: 0.20, train acc: 92.80%, consuming tine: 5.16\n",
      "batch: 571, batch train loss: 0.20, train acc: 92.81%, consuming tine: 5.28\n",
      "batch: 572, batch train loss: 0.20, train acc: 92.81%, consuming tine: 4.99\n",
      "batch: 573, batch train loss: 0.20, train acc: 92.82%, consuming tine: 5.37\n",
      "batch: 574, batch train loss: 0.20, train acc: 92.82%, consuming tine: 4.74\n",
      "batch: 575, batch train loss: 0.20, train acc: 92.83%, consuming tine: 4.95\n",
      "batch: 576, batch train loss: 0.20, train acc: 92.83%, consuming tine: 5.11\n",
      "batch: 577, batch train loss: 0.20, train acc: 92.83%, consuming tine: 4.87\n",
      "batch: 578, batch train loss: 0.20, train acc: 92.83%, consuming tine: 4.99\n",
      "batch: 579, batch train loss: 0.20, train acc: 92.84%, consuming tine: 4.78\n",
      "batch: 580, batch train loss: 0.20, train acc: 92.84%, consuming tine: 5.17\n",
      "batch: 581, batch train loss: 0.20, train acc: 92.85%, consuming tine: 5.08\n",
      "batch: 582, batch train loss: 0.20, train acc: 92.85%, consuming tine: 4.99\n",
      "batch: 583, batch train loss: 0.20, train acc: 92.86%, consuming tine: 5.11\n",
      "batch: 584, batch train loss: 0.20, train acc: 92.86%, consuming tine: 4.78\n",
      "batch: 585, batch train loss: 0.20, train acc: 92.86%, consuming tine: 5.17\n",
      "batch: 586, batch train loss: 0.20, train acc: 92.86%, consuming tine: 5.10\n",
      "batch: 587, batch train loss: 0.20, train acc: 92.87%, consuming tine: 4.99\n",
      "batch: 588, batch train loss: 0.20, train acc: 92.87%, consuming tine: 5.31\n",
      "batch: 589, batch train loss: 0.20, train acc: 92.88%, consuming tine: 5.25\n",
      "batch: 590, batch train loss: 0.20, train acc: 92.88%, consuming tine: 5.11\n",
      "batch: 591, batch train loss: 0.20, train acc: 92.89%, consuming tine: 5.27\n",
      "batch: 592, batch train loss: 0.20, train acc: 92.89%, consuming tine: 4.89\n",
      "batch: 593, batch train loss: 0.20, train acc: 92.90%, consuming tine: 5.20\n",
      "batch: 594, batch train loss: 0.20, train acc: 92.90%, consuming tine: 5.17\n",
      "batch: 595, batch train loss: 0.20, train acc: 92.90%, consuming tine: 4.60\n",
      "batch: 596, batch train loss: 0.20, train acc: 92.91%, consuming tine: 5.60\n",
      "batch: 597, batch train loss: 0.20, train acc: 92.91%, consuming tine: 5.07\n",
      "batch: 598, batch train loss: 0.20, train acc: 92.92%, consuming tine: 5.11\n",
      "batch: 599, batch train loss: 0.20, train acc: 92.92%, consuming tine: 4.96\n",
      "batch: 600, batch train loss: 0.20, train acc: 92.93%, consuming tine: 4.69\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 3.25, valid acc: 35.65%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.20, train acc: 92.93%, consuming tine: 4.64\n",
      "batch: 602, batch train loss: 0.20, train acc: 92.94%, consuming tine: 5.15\n",
      "batch: 603, batch train loss: 0.20, train acc: 92.94%, consuming tine: 4.99\n",
      "batch: 604, batch train loss: 0.20, train acc: 92.94%, consuming tine: 5.29\n",
      "batch: 605, batch train loss: 0.20, train acc: 92.95%, consuming tine: 4.76\n",
      "batch: 606, batch train loss: 0.20, train acc: 92.95%, consuming tine: 4.52\n",
      "batch: 607, batch train loss: 0.20, train acc: 92.95%, consuming tine: 5.01\n",
      "batch: 608, batch train loss: 0.20, train acc: 92.95%, consuming tine: 5.35\n",
      "batch: 609, batch train loss: 0.20, train acc: 92.96%, consuming tine: 4.42\n",
      "batch: 610, batch train loss: 0.20, train acc: 92.96%, consuming tine: 4.97\n",
      "batch: 611, batch train loss: 0.20, train acc: 92.96%, consuming tine: 4.90\n",
      "batch: 612, batch train loss: 0.20, train acc: 92.97%, consuming tine: 5.28\n",
      "batch: 613, batch train loss: 0.20, train acc: 92.97%, consuming tine: 5.28\n",
      "batch: 614, batch train loss: 0.20, train acc: 92.97%, consuming tine: 4.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 615, batch train loss: 0.20, train acc: 92.97%, consuming tine: 4.99\n",
      "batch: 616, batch train loss: 0.20, train acc: 92.98%, consuming tine: 5.08\n",
      "batch: 617, batch train loss: 0.20, train acc: 92.98%, consuming tine: 4.70\n",
      "batch: 618, batch train loss: 0.20, train acc: 92.99%, consuming tine: 4.78\n",
      "batch: 619, batch train loss: 0.20, train acc: 92.99%, consuming tine: 4.87\n",
      "batch: 620, batch train loss: 0.20, train acc: 93.00%, consuming tine: 5.50\n",
      "batch: 621, batch train loss: 0.20, train acc: 93.00%, consuming tine: 5.02\n",
      "batch: 622, batch train loss: 0.20, train acc: 93.01%, consuming tine: 5.08\n",
      "batch: 623, batch train loss: 0.20, train acc: 93.01%, consuming tine: 5.37\n",
      "batch: 624, batch train loss: 0.20, train acc: 93.01%, consuming tine: 4.89\n",
      "batch: 625, batch train loss: 0.20, train acc: 93.02%, consuming tine: 5.88\n",
      "batch: 626, batch train loss: 0.20, train acc: 93.02%, consuming tine: 5.00\n",
      "batch: 627, batch train loss: 0.20, train acc: 93.02%, consuming tine: 5.48\n",
      "batch: 628, batch train loss: 0.19, train acc: 93.02%, consuming tine: 4.89\n",
      "batch: 629, batch train loss: 0.19, train acc: 93.03%, consuming tine: 5.30\n",
      "batch: 630, batch train loss: 0.19, train acc: 93.04%, consuming tine: 4.78\n",
      "batch: 631, batch train loss: 0.19, train acc: 93.04%, consuming tine: 4.98\n",
      "batch: 632, batch train loss: 0.19, train acc: 93.04%, consuming tine: 5.59\n",
      "batch: 633, batch train loss: 0.19, train acc: 93.05%, consuming tine: 5.07\n",
      "batch: 634, batch train loss: 0.19, train acc: 93.06%, consuming tine: 5.11\n",
      "batch: 635, batch train loss: 0.19, train acc: 93.06%, consuming tine: 5.00\n",
      "batch: 636, batch train loss: 0.19, train acc: 93.07%, consuming tine: 4.97\n",
      "batch: 637, batch train loss: 0.19, train acc: 93.07%, consuming tine: 4.81\n",
      "batch: 638, batch train loss: 0.19, train acc: 93.07%, consuming tine: 5.67\n",
      "batch: 639, batch train loss: 0.19, train acc: 93.08%, consuming tine: 4.78\n",
      "batch: 640, batch train loss: 0.19, train acc: 93.08%, consuming tine: 5.30\n",
      "batch: 641, batch train loss: 0.19, train acc: 93.09%, consuming tine: 4.69\n",
      "batch: 642, batch train loss: 0.19, train acc: 93.09%, consuming tine: 4.88\n",
      "batch: 643, batch train loss: 0.19, train acc: 93.10%, consuming tine: 5.07\n",
      "batch: 644, batch train loss: 0.19, train acc: 93.10%, consuming tine: 4.69\n",
      "batch: 645, batch train loss: 0.19, train acc: 93.11%, consuming tine: 5.10\n",
      "batch: 646, batch train loss: 0.19, train acc: 93.11%, consuming tine: 5.20\n",
      "batch: 647, batch train loss: 0.19, train acc: 93.12%, consuming tine: 4.97\n",
      "batch: 648, batch train loss: 0.19, train acc: 93.12%, consuming tine: 5.28\n",
      "batch: 649, batch train loss: 0.19, train acc: 93.13%, consuming tine: 5.29\n",
      "batch: 650, batch train loss: 0.19, train acc: 93.13%, consuming tine: 4.91\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 3.29, valid acc: 35.65%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.19, train acc: 93.14%, consuming tine: 4.87\n",
      "batch: 652, batch train loss: 0.19, train acc: 93.14%, consuming tine: 4.71\n",
      "batch: 653, batch train loss: 0.19, train acc: 93.15%, consuming tine: 4.96\n",
      "batch: 654, batch train loss: 0.19, train acc: 93.15%, consuming tine: 5.00\n",
      "batch: 655, batch train loss: 0.19, train acc: 93.15%, consuming tine: 4.79\n",
      "batch: 656, batch train loss: 0.19, train acc: 93.16%, consuming tine: 4.78\n",
      "batch: 657, batch train loss: 0.19, train acc: 93.16%, consuming tine: 5.32\n",
      "batch: 658, batch train loss: 0.19, train acc: 93.17%, consuming tine: 4.87\n",
      "batch: 659, batch train loss: 0.19, train acc: 93.17%, consuming tine: 5.17\n",
      "batch: 660, batch train loss: 0.19, train acc: 93.17%, consuming tine: 5.12\n",
      "batch: 661, batch train loss: 0.19, train acc: 93.18%, consuming tine: 5.36\n",
      "batch: 662, batch train loss: 0.19, train acc: 93.18%, consuming tine: 4.92\n",
      "batch: 663, batch train loss: 0.19, train acc: 93.18%, consuming tine: 5.06\n",
      "batch: 664, batch train loss: 0.19, train acc: 93.19%, consuming tine: 4.92\n",
      "batch: 665, batch train loss: 0.19, train acc: 93.19%, consuming tine: 5.26\n",
      "batch: 666, batch train loss: 0.19, train acc: 93.19%, consuming tine: 5.08\n",
      "batch: 667, batch train loss: 0.19, train acc: 93.20%, consuming tine: 5.40\n",
      "batch: 668, batch train loss: 0.19, train acc: 93.21%, consuming tine: 5.18\n",
      "batch: 669, batch train loss: 0.19, train acc: 93.21%, consuming tine: 4.99\n",
      "batch: 670, batch train loss: 0.19, train acc: 93.21%, consuming tine: 4.79\n",
      "batch: 671, batch train loss: 0.19, train acc: 93.22%, consuming tine: 5.67\n",
      "batch: 672, batch train loss: 0.19, train acc: 93.22%, consuming tine: 4.91\n",
      "batch: 673, batch train loss: 0.19, train acc: 93.23%, consuming tine: 4.88\n",
      "batch: 674, batch train loss: 0.19, train acc: 93.23%, consuming tine: 5.20\n",
      "batch: 675, batch train loss: 0.19, train acc: 93.24%, consuming tine: 4.79\n",
      "batch: 676, batch train loss: 0.19, train acc: 93.24%, consuming tine: 4.97\n",
      "batch: 677, batch train loss: 0.19, train acc: 93.25%, consuming tine: 5.20\n",
      "batch: 678, batch train loss: 0.19, train acc: 93.25%, consuming tine: 4.38\n",
      "batch: 679, batch train loss: 0.19, train acc: 93.26%, consuming tine: 5.40\n",
      "batch: 680, batch train loss: 0.19, train acc: 93.26%, consuming tine: 5.06\n",
      "batch: 681, batch train loss: 0.19, train acc: 93.27%, consuming tine: 5.10\n",
      "batch: 682, batch train loss: 0.19, train acc: 93.27%, consuming tine: 4.78\n",
      "batch: 683, batch train loss: 0.19, train acc: 93.28%, consuming tine: 5.19\n",
      "batch: 684, batch train loss: 0.19, train acc: 93.28%, consuming tine: 5.40\n",
      "batch: 685, batch train loss: 0.19, train acc: 93.29%, consuming tine: 4.88\n",
      "batch: 686, batch train loss: 0.19, train acc: 93.29%, consuming tine: 4.79\n",
      "batch: 687, batch train loss: 0.19, train acc: 93.30%, consuming tine: 5.18\n",
      "batch: 688, batch train loss: 0.19, train acc: 93.30%, consuming tine: 4.69\n",
      "batch: 689, batch train loss: 0.19, train acc: 93.30%, consuming tine: 5.29\n",
      "batch: 690, batch train loss: 0.19, train acc: 93.31%, consuming tine: 4.68\n",
      "batch: 691, batch train loss: 0.19, train acc: 93.31%, consuming tine: 4.90\n",
      "batch: 692, batch train loss: 0.19, train acc: 93.32%, consuming tine: 4.79\n",
      "batch: 693, batch train loss: 0.19, train acc: 93.33%, consuming tine: 4.98\n",
      "batch: 694, batch train loss: 0.19, train acc: 93.33%, consuming tine: 5.21\n",
      "batch: 695, batch train loss: 0.19, train acc: 93.33%, consuming tine: 5.07\n",
      "batch: 696, batch train loss: 0.19, train acc: 93.34%, consuming tine: 4.69\n",
      "batch: 697, batch train loss: 0.19, train acc: 93.34%, consuming tine: 5.30\n",
      "batch: 698, batch train loss: 0.19, train acc: 93.35%, consuming tine: 4.98\n",
      "batch: 699, batch train loss: 0.19, train acc: 93.35%, consuming tine: 4.79\n",
      "batch: 700, batch train loss: 0.19, train acc: 93.36%, consuming tine: 4.89\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 3.33, valid acc: 35.65%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.19, train acc: 93.36%, consuming tine: 4.83\n",
      "batch: 702, batch train loss: 0.19, train acc: 93.37%, consuming tine: 4.90\n",
      "batch: 703, batch train loss: 0.19, train acc: 93.37%, consuming tine: 4.78\n",
      "batch: 704, batch train loss: 0.19, train acc: 93.37%, consuming tine: 5.08\n",
      "Epoch 5, Loss: 0.19, Accuracy: 93.37%, Valid Loss: 3.33, Valid Accuracy: 35.65%\n",
      "batch: 1, batch train loss: 0.12, train acc: 96.09%, consuming tine: 4.92\n",
      "batch: 2, batch train loss: 0.11, train acc: 96.58%, consuming tine: 5.66\n",
      "batch: 3, batch train loss: 0.12, train acc: 96.29%, consuming tine: 4.90\n",
      "batch: 4, batch train loss: 0.12, train acc: 96.14%, consuming tine: 5.11\n",
      "batch: 5, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.08\n",
      "batch: 6, batch train loss: 0.12, train acc: 96.16%, consuming tine: 5.07\n",
      "batch: 7, batch train loss: 0.11, train acc: 96.26%, consuming tine: 5.09\n",
      "batch: 8, batch train loss: 0.11, train acc: 96.35%, consuming tine: 4.82\n",
      "batch: 9, batch train loss: 0.11, train acc: 96.42%, consuming tine: 5.07\n",
      "batch: 10, batch train loss: 0.11, train acc: 96.41%, consuming tine: 5.08\n",
      "batch: 11, batch train loss: 0.11, train acc: 96.44%, consuming tine: 5.08\n",
      "batch: 12, batch train loss: 0.11, train acc: 96.41%, consuming tine: 5.00\n",
      "batch: 13, batch train loss: 0.11, train acc: 96.46%, consuming tine: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14, batch train loss: 0.11, train acc: 96.48%, consuming tine: 5.09\n",
      "batch: 15, batch train loss: 0.10, train acc: 96.54%, consuming tine: 4.76\n",
      "batch: 16, batch train loss: 0.10, train acc: 96.58%, consuming tine: 5.03\n",
      "batch: 17, batch train loss: 0.10, train acc: 96.59%, consuming tine: 5.47\n",
      "batch: 18, batch train loss: 0.10, train acc: 96.57%, consuming tine: 5.28\n",
      "batch: 19, batch train loss: 0.10, train acc: 96.59%, consuming tine: 4.81\n",
      "batch: 20, batch train loss: 0.11, train acc: 96.56%, consuming tine: 4.79\n",
      "batch: 21, batch train loss: 0.11, train acc: 96.54%, consuming tine: 5.08\n",
      "batch: 22, batch train loss: 0.11, train acc: 96.52%, consuming tine: 5.19\n",
      "batch: 23, batch train loss: 0.11, train acc: 96.51%, consuming tine: 4.71\n",
      "batch: 24, batch train loss: 0.11, train acc: 96.50%, consuming tine: 5.06\n",
      "batch: 25, batch train loss: 0.11, train acc: 96.49%, consuming tine: 4.89\n",
      "batch: 26, batch train loss: 0.11, train acc: 96.50%, consuming tine: 4.80\n",
      "batch: 27, batch train loss: 0.11, train acc: 96.45%, consuming tine: 4.79\n",
      "batch: 28, batch train loss: 0.11, train acc: 96.33%, consuming tine: 5.07\n",
      "batch: 29, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.00\n",
      "batch: 30, batch train loss: 0.12, train acc: 96.13%, consuming tine: 5.19\n",
      "batch: 31, batch train loss: 0.12, train acc: 96.10%, consuming tine: 4.79\n",
      "batch: 32, batch train loss: 0.12, train acc: 96.10%, consuming tine: 4.87\n",
      "batch: 33, batch train loss: 0.11, train acc: 96.14%, consuming tine: 5.49\n",
      "batch: 34, batch train loss: 0.11, train acc: 96.15%, consuming tine: 4.89\n",
      "batch: 35, batch train loss: 0.11, train acc: 96.15%, consuming tine: 5.58\n",
      "batch: 36, batch train loss: 0.12, train acc: 96.12%, consuming tine: 5.19\n",
      "batch: 37, batch train loss: 0.12, train acc: 96.03%, consuming tine: 4.89\n",
      "batch: 38, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.00\n",
      "batch: 39, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.19\n",
      "batch: 40, batch train loss: 0.12, train acc: 95.85%, consuming tine: 5.06\n",
      "batch: 41, batch train loss: 0.12, train acc: 95.84%, consuming tine: 4.79\n",
      "batch: 42, batch train loss: 0.12, train acc: 95.86%, consuming tine: 4.98\n",
      "batch: 43, batch train loss: 0.12, train acc: 95.87%, consuming tine: 5.60\n",
      "batch: 44, batch train loss: 0.12, train acc: 95.89%, consuming tine: 4.80\n",
      "batch: 45, batch train loss: 0.12, train acc: 95.92%, consuming tine: 4.60\n",
      "batch: 46, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.28\n",
      "batch: 47, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.00\n",
      "batch: 48, batch train loss: 0.12, train acc: 95.80%, consuming tine: 4.97\n",
      "batch: 49, batch train loss: 0.12, train acc: 95.74%, consuming tine: 4.90\n",
      "batch: 50, batch train loss: 0.12, train acc: 95.70%, consuming tine: 5.38\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 3.71, valid acc: 35.92%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.13, train acc: 95.69%, consuming tine: 5.15\n",
      "batch: 52, batch train loss: 0.13, train acc: 95.70%, consuming tine: 5.58\n",
      "batch: 53, batch train loss: 0.12, train acc: 95.72%, consuming tine: 4.60\n",
      "batch: 54, batch train loss: 0.12, train acc: 95.74%, consuming tine: 4.59\n",
      "batch: 55, batch train loss: 0.12, train acc: 95.75%, consuming tine: 5.19\n",
      "batch: 56, batch train loss: 0.12, train acc: 95.75%, consuming tine: 4.79\n",
      "batch: 57, batch train loss: 0.12, train acc: 95.77%, consuming tine: 4.98\n",
      "batch: 58, batch train loss: 0.12, train acc: 95.76%, consuming tine: 4.88\n",
      "batch: 59, batch train loss: 0.12, train acc: 95.74%, consuming tine: 5.12\n",
      "batch: 60, batch train loss: 0.12, train acc: 95.74%, consuming tine: 5.26\n",
      "batch: 61, batch train loss: 0.12, train acc: 95.73%, consuming tine: 5.08\n",
      "batch: 62, batch train loss: 0.12, train acc: 95.70%, consuming tine: 5.10\n",
      "batch: 63, batch train loss: 0.12, train acc: 95.70%, consuming tine: 4.88\n",
      "batch: 64, batch train loss: 0.12, train acc: 95.71%, consuming tine: 4.50\n",
      "batch: 65, batch train loss: 0.12, train acc: 95.73%, consuming tine: 5.08\n",
      "batch: 66, batch train loss: 0.12, train acc: 95.76%, consuming tine: 4.91\n",
      "batch: 67, batch train loss: 0.12, train acc: 95.78%, consuming tine: 5.28\n",
      "batch: 68, batch train loss: 0.12, train acc: 95.79%, consuming tine: 5.26\n",
      "batch: 69, batch train loss: 0.12, train acc: 95.79%, consuming tine: 4.64\n",
      "batch: 70, batch train loss: 0.12, train acc: 95.78%, consuming tine: 4.86\n",
      "batch: 71, batch train loss: 0.12, train acc: 95.78%, consuming tine: 5.00\n",
      "batch: 72, batch train loss: 0.12, train acc: 95.79%, consuming tine: 4.77\n",
      "batch: 73, batch train loss: 0.12, train acc: 95.81%, consuming tine: 5.11\n",
      "batch: 74, batch train loss: 0.12, train acc: 95.82%, consuming tine: 4.97\n",
      "batch: 75, batch train loss: 0.12, train acc: 95.84%, consuming tine: 5.26\n",
      "batch: 76, batch train loss: 0.12, train acc: 95.83%, consuming tine: 5.00\n",
      "batch: 77, batch train loss: 0.12, train acc: 95.85%, consuming tine: 5.81\n",
      "batch: 78, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.00\n",
      "batch: 79, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.27\n",
      "batch: 80, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.28\n",
      "batch: 81, batch train loss: 0.12, train acc: 95.89%, consuming tine: 4.88\n",
      "batch: 82, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.49\n",
      "batch: 83, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.28\n",
      "batch: 84, batch train loss: 0.12, train acc: 95.93%, consuming tine: 5.00\n",
      "batch: 85, batch train loss: 0.12, train acc: 95.95%, consuming tine: 5.09\n",
      "batch: 86, batch train loss: 0.12, train acc: 95.96%, consuming tine: 5.27\n",
      "batch: 87, batch train loss: 0.12, train acc: 95.98%, consuming tine: 5.40\n",
      "batch: 88, batch train loss: 0.12, train acc: 95.99%, consuming tine: 5.07\n",
      "batch: 89, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.10\n",
      "batch: 90, batch train loss: 0.12, train acc: 96.01%, consuming tine: 5.78\n",
      "batch: 91, batch train loss: 0.12, train acc: 96.02%, consuming tine: 4.97\n",
      "batch: 92, batch train loss: 0.12, train acc: 96.02%, consuming tine: 4.80\n",
      "batch: 93, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.71\n",
      "batch: 94, batch train loss: 0.11, train acc: 96.04%, consuming tine: 4.76\n",
      "batch: 95, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.29\n",
      "batch: 96, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.20\n",
      "batch: 97, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.00\n",
      "batch: 98, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.18\n",
      "batch: 99, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.19\n",
      "batch: 100, batch train loss: 0.11, train acc: 96.05%, consuming tine: 4.76\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 3.68, valid acc: 35.95%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.08\n",
      "batch: 102, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.79\n",
      "batch: 103, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.80\n",
      "batch: 104, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.19\n",
      "batch: 105, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.09\n",
      "batch: 106, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.17\n",
      "batch: 107, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.19\n",
      "batch: 108, batch train loss: 0.11, train acc: 96.07%, consuming tine: 5.20\n",
      "batch: 109, batch train loss: 0.11, train acc: 96.07%, consuming tine: 4.89\n",
      "batch: 110, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.48\n",
      "batch: 111, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.10\n",
      "batch: 112, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.32\n",
      "batch: 113, batch train loss: 0.11, train acc: 96.02%, consuming tine: 5.15\n",
      "batch: 114, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.00\n",
      "batch: 115, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.08\n",
      "batch: 116, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.09\n",
      "batch: 117, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.09\n",
      "batch: 118, batch train loss: 0.11, train acc: 96.05%, consuming tine: 4.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 119, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.19\n",
      "batch: 120, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.58\n",
      "batch: 121, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.09\n",
      "batch: 122, batch train loss: 0.11, train acc: 96.04%, consuming tine: 4.98\n",
      "batch: 123, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.21\n",
      "batch: 124, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.09\n",
      "batch: 125, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.91\n",
      "batch: 126, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.25\n",
      "batch: 127, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.19\n",
      "batch: 128, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.35\n",
      "batch: 129, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.91\n",
      "batch: 130, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.56\n",
      "batch: 131, batch train loss: 0.11, train acc: 96.07%, consuming tine: 4.81\n",
      "batch: 132, batch train loss: 0.11, train acc: 96.07%, consuming tine: 5.49\n",
      "batch: 133, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.58\n",
      "batch: 134, batch train loss: 0.11, train acc: 96.10%, consuming tine: 5.09\n",
      "batch: 135, batch train loss: 0.11, train acc: 96.10%, consuming tine: 4.89\n",
      "batch: 136, batch train loss: 0.11, train acc: 96.10%, consuming tine: 5.17\n",
      "batch: 137, batch train loss: 0.11, train acc: 96.10%, consuming tine: 4.80\n",
      "batch: 138, batch train loss: 0.11, train acc: 96.10%, consuming tine: 4.73\n",
      "batch: 139, batch train loss: 0.11, train acc: 96.11%, consuming tine: 4.95\n",
      "batch: 140, batch train loss: 0.11, train acc: 96.10%, consuming tine: 5.18\n",
      "batch: 141, batch train loss: 0.11, train acc: 96.09%, consuming tine: 4.49\n",
      "batch: 142, batch train loss: 0.11, train acc: 96.09%, consuming tine: 4.81\n",
      "batch: 143, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.77\n",
      "batch: 144, batch train loss: 0.11, train acc: 96.05%, consuming tine: 4.69\n",
      "batch: 145, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.17\n",
      "batch: 146, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.22\n",
      "batch: 147, batch train loss: 0.11, train acc: 96.07%, consuming tine: 5.06\n",
      "batch: 148, batch train loss: 0.11, train acc: 96.07%, consuming tine: 5.08\n",
      "batch: 149, batch train loss: 0.11, train acc: 96.07%, consuming tine: 4.99\n",
      "batch: 150, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.39\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 3.76, valid acc: 35.86%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.11, train acc: 96.04%, consuming tine: 4.93\n",
      "batch: 152, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.18\n",
      "batch: 153, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.27\n",
      "batch: 154, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.00\n",
      "batch: 155, batch train loss: 0.11, train acc: 96.05%, consuming tine: 4.97\n",
      "batch: 156, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.42\n",
      "batch: 157, batch train loss: 0.11, train acc: 96.04%, consuming tine: 4.87\n",
      "batch: 158, batch train loss: 0.11, train acc: 96.02%, consuming tine: 4.88\n",
      "batch: 159, batch train loss: 0.11, train acc: 96.01%, consuming tine: 4.99\n",
      "batch: 160, batch train loss: 0.11, train acc: 96.00%, consuming tine: 4.99\n",
      "batch: 161, batch train loss: 0.11, train acc: 96.00%, consuming tine: 4.68\n",
      "batch: 162, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.07\n",
      "batch: 163, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.70\n",
      "batch: 164, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.60\n",
      "batch: 165, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.19\n",
      "batch: 166, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.17\n",
      "batch: 167, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.20\n",
      "batch: 168, batch train loss: 0.12, train acc: 96.00%, consuming tine: 4.87\n",
      "batch: 169, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.11\n",
      "batch: 170, batch train loss: 0.12, train acc: 95.99%, consuming tine: 4.78\n",
      "batch: 171, batch train loss: 0.12, train acc: 95.99%, consuming tine: 4.87\n",
      "batch: 172, batch train loss: 0.12, train acc: 95.98%, consuming tine: 5.39\n",
      "batch: 173, batch train loss: 0.12, train acc: 95.99%, consuming tine: 5.34\n",
      "batch: 174, batch train loss: 0.12, train acc: 95.99%, consuming tine: 5.42\n",
      "batch: 175, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.10\n",
      "batch: 176, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.61\n",
      "batch: 177, batch train loss: 0.12, train acc: 96.00%, consuming tine: 4.99\n",
      "batch: 178, batch train loss: 0.11, train acc: 96.00%, consuming tine: 4.97\n",
      "batch: 179, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.60\n",
      "batch: 180, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.38\n",
      "batch: 181, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.41\n",
      "batch: 182, batch train loss: 0.12, train acc: 95.99%, consuming tine: 4.87\n",
      "batch: 183, batch train loss: 0.11, train acc: 95.99%, consuming tine: 5.22\n",
      "batch: 184, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.17\n",
      "batch: 185, batch train loss: 0.11, train acc: 96.00%, consuming tine: 5.06\n",
      "batch: 186, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.11\n",
      "batch: 187, batch train loss: 0.11, train acc: 96.01%, consuming tine: 4.87\n",
      "batch: 188, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.58\n",
      "batch: 189, batch train loss: 0.11, train acc: 96.02%, consuming tine: 4.91\n",
      "batch: 190, batch train loss: 0.11, train acc: 96.01%, consuming tine: 5.29\n",
      "batch: 191, batch train loss: 0.11, train acc: 95.99%, consuming tine: 4.79\n",
      "batch: 192, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.16\n",
      "batch: 193, batch train loss: 0.12, train acc: 95.96%, consuming tine: 5.29\n",
      "batch: 194, batch train loss: 0.12, train acc: 95.96%, consuming tine: 4.80\n",
      "batch: 195, batch train loss: 0.12, train acc: 95.97%, consuming tine: 4.79\n",
      "batch: 196, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.36\n",
      "batch: 197, batch train loss: 0.12, train acc: 95.95%, consuming tine: 4.70\n",
      "batch: 198, batch train loss: 0.12, train acc: 95.93%, consuming tine: 5.28\n",
      "batch: 199, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.30\n",
      "batch: 200, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.19\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 3.73, valid acc: 35.75%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.12, train acc: 95.87%, consuming tine: 5.60\n",
      "batch: 202, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.09\n",
      "batch: 203, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.38\n",
      "batch: 204, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.54\n",
      "batch: 205, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.14\n",
      "batch: 206, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.38\n",
      "batch: 207, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.09\n",
      "batch: 208, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.08\n",
      "batch: 209, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.92\n",
      "batch: 210, batch train loss: 0.12, train acc: 95.83%, consuming tine: 5.15\n",
      "batch: 211, batch train loss: 0.12, train acc: 95.80%, consuming tine: 5.10\n",
      "batch: 212, batch train loss: 0.12, train acc: 95.78%, consuming tine: 5.48\n",
      "batch: 213, batch train loss: 0.12, train acc: 95.77%, consuming tine: 5.37\n",
      "batch: 214, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.40\n",
      "batch: 215, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.39\n",
      "batch: 216, batch train loss: 0.12, train acc: 95.77%, consuming tine: 4.89\n",
      "batch: 217, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.29\n",
      "batch: 218, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.19\n",
      "batch: 219, batch train loss: 0.12, train acc: 95.75%, consuming tine: 5.18\n",
      "batch: 220, batch train loss: 0.12, train acc: 95.73%, consuming tine: 5.19\n",
      "batch: 221, batch train loss: 0.12, train acc: 95.72%, consuming tine: 4.70\n",
      "batch: 222, batch train loss: 0.12, train acc: 95.70%, consuming tine: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 223, batch train loss: 0.12, train acc: 95.68%, consuming tine: 5.06\n",
      "batch: 224, batch train loss: 0.12, train acc: 95.67%, consuming tine: 5.08\n",
      "batch: 225, batch train loss: 0.12, train acc: 95.67%, consuming tine: 5.39\n",
      "batch: 226, batch train loss: 0.12, train acc: 95.66%, consuming tine: 5.12\n",
      "batch: 227, batch train loss: 0.12, train acc: 95.66%, consuming tine: 4.90\n",
      "batch: 228, batch train loss: 0.12, train acc: 95.66%, consuming tine: 5.15\n",
      "batch: 229, batch train loss: 0.12, train acc: 95.65%, consuming tine: 5.04\n",
      "batch: 230, batch train loss: 0.12, train acc: 95.64%, consuming tine: 5.10\n",
      "batch: 231, batch train loss: 0.12, train acc: 95.64%, consuming tine: 5.23\n",
      "batch: 232, batch train loss: 0.12, train acc: 95.64%, consuming tine: 5.30\n",
      "batch: 233, batch train loss: 0.12, train acc: 95.63%, consuming tine: 6.19\n",
      "batch: 234, batch train loss: 0.12, train acc: 95.62%, consuming tine: 5.09\n",
      "batch: 235, batch train loss: 0.12, train acc: 95.62%, consuming tine: 5.50\n",
      "batch: 236, batch train loss: 0.12, train acc: 95.60%, consuming tine: 5.27\n",
      "batch: 237, batch train loss: 0.13, train acc: 95.58%, consuming tine: 5.63\n",
      "batch: 238, batch train loss: 0.13, train acc: 95.57%, consuming tine: 5.47\n",
      "batch: 239, batch train loss: 0.13, train acc: 95.55%, consuming tine: 5.17\n",
      "batch: 240, batch train loss: 0.13, train acc: 95.54%, consuming tine: 4.98\n",
      "batch: 241, batch train loss: 0.13, train acc: 95.54%, consuming tine: 4.99\n",
      "batch: 242, batch train loss: 0.13, train acc: 95.54%, consuming tine: 5.61\n",
      "batch: 243, batch train loss: 0.13, train acc: 95.53%, consuming tine: 5.27\n",
      "batch: 244, batch train loss: 0.13, train acc: 95.53%, consuming tine: 5.19\n",
      "batch: 245, batch train loss: 0.13, train acc: 95.52%, consuming tine: 5.60\n",
      "batch: 246, batch train loss: 0.13, train acc: 95.51%, consuming tine: 5.08\n",
      "batch: 247, batch train loss: 0.13, train acc: 95.49%, consuming tine: 5.38\n",
      "batch: 248, batch train loss: 0.13, train acc: 95.47%, consuming tine: 5.30\n",
      "batch: 249, batch train loss: 0.13, train acc: 95.46%, consuming tine: 4.87\n",
      "batch: 250, batch train loss: 0.13, train acc: 95.45%, consuming tine: 5.27\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 3.65, valid acc: 35.71%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.13, train acc: 95.44%, consuming tine: 5.01\n",
      "batch: 252, batch train loss: 0.13, train acc: 95.44%, consuming tine: 5.44\n",
      "batch: 253, batch train loss: 0.13, train acc: 95.43%, consuming tine: 4.93\n",
      "batch: 254, batch train loss: 0.13, train acc: 95.43%, consuming tine: 5.09\n",
      "batch: 255, batch train loss: 0.13, train acc: 95.42%, consuming tine: 5.42\n",
      "batch: 256, batch train loss: 0.13, train acc: 95.42%, consuming tine: 5.05\n",
      "batch: 257, batch train loss: 0.13, train acc: 95.41%, consuming tine: 4.89\n",
      "batch: 258, batch train loss: 0.13, train acc: 95.41%, consuming tine: 5.08\n",
      "batch: 259, batch train loss: 0.13, train acc: 95.40%, consuming tine: 5.51\n",
      "batch: 260, batch train loss: 0.13, train acc: 95.39%, consuming tine: 5.37\n",
      "batch: 261, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.31\n",
      "batch: 262, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.07\n",
      "batch: 263, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.00\n",
      "batch: 264, batch train loss: 0.13, train acc: 95.38%, consuming tine: 4.97\n",
      "batch: 265, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.01\n",
      "batch: 266, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.06\n",
      "batch: 267, batch train loss: 0.13, train acc: 95.38%, consuming tine: 4.89\n",
      "batch: 268, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.61\n",
      "batch: 269, batch train loss: 0.13, train acc: 95.36%, consuming tine: 5.07\n",
      "batch: 270, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.70\n",
      "batch: 271, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.22\n",
      "batch: 272, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.25\n",
      "batch: 273, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.39\n",
      "batch: 274, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.08\n",
      "batch: 275, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.08\n",
      "batch: 276, batch train loss: 0.13, train acc: 95.34%, consuming tine: 4.71\n",
      "batch: 277, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.27\n",
      "batch: 278, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.33\n",
      "batch: 279, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.05\n",
      "batch: 280, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.21\n",
      "batch: 281, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.15\n",
      "batch: 282, batch train loss: 0.13, train acc: 95.32%, consuming tine: 4.99\n",
      "batch: 283, batch train loss: 0.13, train acc: 95.31%, consuming tine: 4.99\n",
      "batch: 284, batch train loss: 0.13, train acc: 95.31%, consuming tine: 5.28\n",
      "batch: 285, batch train loss: 0.13, train acc: 95.31%, consuming tine: 5.09\n",
      "batch: 286, batch train loss: 0.13, train acc: 95.31%, consuming tine: 5.38\n",
      "batch: 287, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.31\n",
      "batch: 288, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.48\n",
      "batch: 289, batch train loss: 0.13, train acc: 95.33%, consuming tine: 4.78\n",
      "batch: 290, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.39\n",
      "batch: 291, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.08\n",
      "batch: 292, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.20\n",
      "batch: 293, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.29\n",
      "batch: 294, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.39\n",
      "batch: 295, batch train loss: 0.13, train acc: 95.32%, consuming tine: 4.96\n",
      "batch: 296, batch train loss: 0.13, train acc: 95.31%, consuming tine: 5.39\n",
      "batch: 297, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.31\n",
      "batch: 298, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.54\n",
      "batch: 299, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.22\n",
      "batch: 300, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.48\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 3.64, valid acc: 35.85%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.03\n",
      "batch: 302, batch train loss: 0.13, train acc: 95.32%, consuming tine: 4.97\n",
      "batch: 303, batch train loss: 0.13, train acc: 95.32%, consuming tine: 5.17\n",
      "batch: 304, batch train loss: 0.13, train acc: 95.32%, consuming tine: 4.90\n",
      "batch: 305, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.69\n",
      "batch: 306, batch train loss: 0.13, train acc: 95.33%, consuming tine: 4.79\n",
      "batch: 307, batch train loss: 0.13, train acc: 95.34%, consuming tine: 4.78\n",
      "batch: 308, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.40\n",
      "batch: 309, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.36\n",
      "batch: 310, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.41\n",
      "batch: 311, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.22\n",
      "batch: 312, batch train loss: 0.13, train acc: 95.33%, consuming tine: 5.34\n",
      "batch: 313, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.41\n",
      "batch: 314, batch train loss: 0.13, train acc: 95.34%, consuming tine: 5.26\n",
      "batch: 315, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.09\n",
      "batch: 316, batch train loss: 0.13, train acc: 95.35%, consuming tine: 5.38\n",
      "batch: 317, batch train loss: 0.13, train acc: 95.36%, consuming tine: 5.30\n",
      "batch: 318, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.18\n",
      "batch: 319, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.70\n",
      "batch: 320, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.60\n",
      "batch: 321, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.53\n",
      "batch: 322, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.23\n",
      "batch: 323, batch train loss: 0.13, train acc: 95.37%, consuming tine: 5.49\n",
      "batch: 324, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.25\n",
      "batch: 325, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.51\n",
      "batch: 326, batch train loss: 0.13, train acc: 95.38%, consuming tine: 5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 327, batch train loss: 0.13, train acc: 95.39%, consuming tine: 5.49\n",
      "batch: 328, batch train loss: 0.13, train acc: 95.40%, consuming tine: 5.61\n",
      "batch: 329, batch train loss: 0.13, train acc: 95.40%, consuming tine: 5.57\n",
      "batch: 330, batch train loss: 0.13, train acc: 95.41%, consuming tine: 4.78\n",
      "batch: 331, batch train loss: 0.13, train acc: 95.41%, consuming tine: 5.51\n",
      "batch: 332, batch train loss: 0.13, train acc: 95.42%, consuming tine: 5.17\n",
      "batch: 333, batch train loss: 0.13, train acc: 95.42%, consuming tine: 5.58\n",
      "batch: 334, batch train loss: 0.13, train acc: 95.42%, consuming tine: 5.26\n",
      "batch: 335, batch train loss: 0.13, train acc: 95.43%, consuming tine: 5.00\n",
      "batch: 336, batch train loss: 0.13, train acc: 95.44%, consuming tine: 5.00\n",
      "batch: 337, batch train loss: 0.13, train acc: 95.44%, consuming tine: 5.46\n",
      "batch: 338, batch train loss: 0.13, train acc: 95.45%, consuming tine: 5.09\n",
      "batch: 339, batch train loss: 0.13, train acc: 95.45%, consuming tine: 5.69\n",
      "batch: 340, batch train loss: 0.13, train acc: 95.46%, consuming tine: 4.80\n",
      "batch: 341, batch train loss: 0.13, train acc: 95.46%, consuming tine: 4.61\n",
      "batch: 342, batch train loss: 0.13, train acc: 95.47%, consuming tine: 5.37\n",
      "batch: 343, batch train loss: 0.13, train acc: 95.48%, consuming tine: 4.89\n",
      "batch: 344, batch train loss: 0.13, train acc: 95.48%, consuming tine: 5.18\n",
      "batch: 345, batch train loss: 0.13, train acc: 95.49%, consuming tine: 5.20\n",
      "batch: 346, batch train loss: 0.13, train acc: 95.49%, consuming tine: 5.05\n",
      "batch: 347, batch train loss: 0.13, train acc: 95.50%, consuming tine: 5.11\n",
      "batch: 348, batch train loss: 0.13, train acc: 95.50%, consuming tine: 5.17\n",
      "batch: 349, batch train loss: 0.13, train acc: 95.51%, consuming tine: 5.09\n",
      "batch: 350, batch train loss: 0.13, train acc: 95.51%, consuming tine: 5.29\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 3.66, valid acc: 35.80%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.13, train acc: 95.52%, consuming tine: 5.34\n",
      "batch: 352, batch train loss: 0.13, train acc: 95.52%, consuming tine: 5.27\n",
      "batch: 353, batch train loss: 0.13, train acc: 95.53%, consuming tine: 4.90\n",
      "batch: 354, batch train loss: 0.13, train acc: 95.53%, consuming tine: 5.60\n",
      "batch: 355, batch train loss: 0.13, train acc: 95.53%, consuming tine: 5.37\n",
      "batch: 356, batch train loss: 0.13, train acc: 95.54%, consuming tine: 4.89\n",
      "batch: 357, batch train loss: 0.13, train acc: 95.54%, consuming tine: 5.50\n",
      "batch: 358, batch train loss: 0.13, train acc: 95.55%, consuming tine: 5.16\n",
      "batch: 359, batch train loss: 0.13, train acc: 95.55%, consuming tine: 5.12\n",
      "batch: 360, batch train loss: 0.13, train acc: 95.56%, consuming tine: 5.25\n",
      "batch: 361, batch train loss: 0.13, train acc: 95.57%, consuming tine: 5.17\n",
      "batch: 362, batch train loss: 0.13, train acc: 95.58%, consuming tine: 5.30\n",
      "batch: 363, batch train loss: 0.13, train acc: 95.58%, consuming tine: 4.97\n",
      "batch: 364, batch train loss: 0.13, train acc: 95.58%, consuming tine: 5.29\n",
      "batch: 365, batch train loss: 0.13, train acc: 95.59%, consuming tine: 5.31\n",
      "batch: 366, batch train loss: 0.13, train acc: 95.59%, consuming tine: 5.17\n",
      "batch: 367, batch train loss: 0.13, train acc: 95.59%, consuming tine: 4.89\n",
      "batch: 368, batch train loss: 0.13, train acc: 95.60%, consuming tine: 5.18\n",
      "batch: 369, batch train loss: 0.13, train acc: 95.60%, consuming tine: 5.18\n",
      "batch: 370, batch train loss: 0.13, train acc: 95.61%, consuming tine: 5.18\n",
      "batch: 371, batch train loss: 0.13, train acc: 95.62%, consuming tine: 4.87\n",
      "batch: 372, batch train loss: 0.13, train acc: 95.62%, consuming tine: 5.50\n",
      "batch: 373, batch train loss: 0.13, train acc: 95.63%, consuming tine: 5.06\n",
      "batch: 374, batch train loss: 0.13, train acc: 95.63%, consuming tine: 4.89\n",
      "batch: 375, batch train loss: 0.13, train acc: 95.63%, consuming tine: 5.20\n",
      "batch: 376, batch train loss: 0.13, train acc: 95.64%, consuming tine: 5.78\n",
      "batch: 377, batch train loss: 0.13, train acc: 95.64%, consuming tine: 5.28\n",
      "batch: 378, batch train loss: 0.13, train acc: 95.64%, consuming tine: 5.58\n",
      "batch: 379, batch train loss: 0.12, train acc: 95.65%, consuming tine: 5.19\n",
      "batch: 380, batch train loss: 0.12, train acc: 95.65%, consuming tine: 5.19\n",
      "batch: 381, batch train loss: 0.12, train acc: 95.66%, consuming tine: 5.27\n",
      "batch: 382, batch train loss: 0.12, train acc: 95.66%, consuming tine: 5.09\n",
      "batch: 383, batch train loss: 0.12, train acc: 95.67%, consuming tine: 5.40\n",
      "batch: 384, batch train loss: 0.12, train acc: 95.67%, consuming tine: 5.69\n",
      "batch: 385, batch train loss: 0.12, train acc: 95.68%, consuming tine: 5.18\n",
      "batch: 386, batch train loss: 0.12, train acc: 95.68%, consuming tine: 5.16\n",
      "batch: 387, batch train loss: 0.12, train acc: 95.69%, consuming tine: 5.20\n",
      "batch: 388, batch train loss: 0.12, train acc: 95.69%, consuming tine: 5.01\n",
      "batch: 389, batch train loss: 0.12, train acc: 95.69%, consuming tine: 5.45\n",
      "batch: 390, batch train loss: 0.12, train acc: 95.70%, consuming tine: 5.31\n",
      "batch: 391, batch train loss: 0.12, train acc: 95.70%, consuming tine: 5.02\n",
      "batch: 392, batch train loss: 0.12, train acc: 95.71%, consuming tine: 5.16\n",
      "batch: 393, batch train loss: 0.12, train acc: 95.71%, consuming tine: 5.18\n",
      "batch: 394, batch train loss: 0.12, train acc: 95.72%, consuming tine: 5.60\n",
      "batch: 395, batch train loss: 0.12, train acc: 95.72%, consuming tine: 5.29\n",
      "batch: 396, batch train loss: 0.12, train acc: 95.73%, consuming tine: 5.19\n",
      "batch: 397, batch train loss: 0.12, train acc: 95.73%, consuming tine: 5.48\n",
      "batch: 398, batch train loss: 0.12, train acc: 95.74%, consuming tine: 5.38\n",
      "batch: 399, batch train loss: 0.12, train acc: 95.74%, consuming tine: 5.59\n",
      "batch: 400, batch train loss: 0.12, train acc: 95.75%, consuming tine: 5.32\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 3.70, valid acc: 35.76%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.12, train acc: 95.75%, consuming tine: 5.42\n",
      "batch: 402, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.19\n",
      "batch: 403, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.58\n",
      "batch: 404, batch train loss: 0.12, train acc: 95.76%, consuming tine: 5.11\n",
      "batch: 405, batch train loss: 0.12, train acc: 95.77%, consuming tine: 5.47\n",
      "batch: 406, batch train loss: 0.12, train acc: 95.77%, consuming tine: 5.37\n",
      "batch: 407, batch train loss: 0.12, train acc: 95.78%, consuming tine: 5.07\n",
      "batch: 408, batch train loss: 0.12, train acc: 95.78%, consuming tine: 5.19\n",
      "batch: 409, batch train loss: 0.12, train acc: 95.79%, consuming tine: 5.38\n",
      "batch: 410, batch train loss: 0.12, train acc: 95.79%, consuming tine: 5.20\n",
      "batch: 411, batch train loss: 0.12, train acc: 95.80%, consuming tine: 5.28\n",
      "batch: 412, batch train loss: 0.12, train acc: 95.80%, consuming tine: 5.08\n",
      "batch: 413, batch train loss: 0.12, train acc: 95.81%, consuming tine: 5.47\n",
      "batch: 414, batch train loss: 0.12, train acc: 95.81%, consuming tine: 5.59\n",
      "batch: 415, batch train loss: 0.12, train acc: 95.81%, consuming tine: 5.21\n",
      "batch: 416, batch train loss: 0.12, train acc: 95.82%, consuming tine: 4.58\n",
      "batch: 417, batch train loss: 0.12, train acc: 95.82%, consuming tine: 5.17\n",
      "batch: 418, batch train loss: 0.12, train acc: 95.83%, consuming tine: 4.87\n",
      "batch: 419, batch train loss: 0.12, train acc: 95.83%, consuming tine: 4.93\n",
      "batch: 420, batch train loss: 0.12, train acc: 95.84%, consuming tine: 4.96\n",
      "batch: 421, batch train loss: 0.12, train acc: 95.84%, consuming tine: 5.38\n",
      "batch: 422, batch train loss: 0.12, train acc: 95.84%, consuming tine: 4.87\n",
      "batch: 423, batch train loss: 0.12, train acc: 95.84%, consuming tine: 5.20\n",
      "batch: 424, batch train loss: 0.12, train acc: 95.84%, consuming tine: 5.57\n",
      "batch: 425, batch train loss: 0.12, train acc: 95.85%, consuming tine: 5.21\n",
      "batch: 426, batch train loss: 0.12, train acc: 95.85%, consuming tine: 5.68\n",
      "batch: 427, batch train loss: 0.12, train acc: 95.85%, consuming tine: 4.79\n",
      "batch: 428, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.40\n",
      "batch: 429, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.17\n",
      "batch: 430, batch train loss: 0.12, train acc: 95.86%, consuming tine: 5.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 431, batch train loss: 0.12, train acc: 95.87%, consuming tine: 5.02\n",
      "batch: 432, batch train loss: 0.12, train acc: 95.87%, consuming tine: 5.18\n",
      "batch: 433, batch train loss: 0.12, train acc: 95.87%, consuming tine: 4.89\n",
      "batch: 434, batch train loss: 0.12, train acc: 95.88%, consuming tine: 4.97\n",
      "batch: 435, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.10\n",
      "batch: 436, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.20\n",
      "batch: 437, batch train loss: 0.12, train acc: 95.88%, consuming tine: 5.25\n",
      "batch: 438, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.20\n",
      "batch: 439, batch train loss: 0.12, train acc: 95.89%, consuming tine: 5.46\n",
      "batch: 440, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.41\n",
      "batch: 441, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.48\n",
      "batch: 442, batch train loss: 0.12, train acc: 95.90%, consuming tine: 5.18\n",
      "batch: 443, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.52\n",
      "batch: 444, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.06\n",
      "batch: 445, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.39\n",
      "batch: 446, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.40\n",
      "batch: 447, batch train loss: 0.12, train acc: 95.91%, consuming tine: 5.07\n",
      "batch: 448, batch train loss: 0.12, train acc: 95.92%, consuming tine: 4.90\n",
      "batch: 449, batch train loss: 0.12, train acc: 95.92%, consuming tine: 5.38\n",
      "batch: 450, batch train loss: 0.12, train acc: 95.92%, consuming tine: 5.29\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 3.73, valid acc: 35.77%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.12, train acc: 95.93%, consuming tine: 5.08\n",
      "batch: 452, batch train loss: 0.12, train acc: 95.94%, consuming tine: 4.99\n",
      "batch: 453, batch train loss: 0.12, train acc: 95.94%, consuming tine: 5.17\n",
      "batch: 454, batch train loss: 0.12, train acc: 95.94%, consuming tine: 5.04\n",
      "batch: 455, batch train loss: 0.12, train acc: 95.95%, consuming tine: 5.33\n",
      "batch: 456, batch train loss: 0.12, train acc: 95.95%, consuming tine: 5.29\n",
      "batch: 457, batch train loss: 0.12, train acc: 95.96%, consuming tine: 5.11\n",
      "batch: 458, batch train loss: 0.12, train acc: 95.96%, consuming tine: 5.18\n",
      "batch: 459, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.66\n",
      "batch: 460, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.20\n",
      "batch: 461, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.50\n",
      "batch: 462, batch train loss: 0.12, train acc: 95.97%, consuming tine: 5.57\n",
      "batch: 463, batch train loss: 0.12, train acc: 95.98%, consuming tine: 4.99\n",
      "batch: 464, batch train loss: 0.12, train acc: 95.98%, consuming tine: 5.28\n",
      "batch: 465, batch train loss: 0.12, train acc: 95.99%, consuming tine: 5.20\n",
      "batch: 466, batch train loss: 0.12, train acc: 95.99%, consuming tine: 4.88\n",
      "batch: 467, batch train loss: 0.12, train acc: 95.99%, consuming tine: 5.37\n",
      "batch: 468, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.42\n",
      "batch: 469, batch train loss: 0.12, train acc: 96.00%, consuming tine: 5.58\n",
      "batch: 470, batch train loss: 0.12, train acc: 96.01%, consuming tine: 5.19\n",
      "batch: 471, batch train loss: 0.12, train acc: 96.01%, consuming tine: 4.98\n",
      "batch: 472, batch train loss: 0.12, train acc: 96.01%, consuming tine: 5.49\n",
      "batch: 473, batch train loss: 0.12, train acc: 96.02%, consuming tine: 5.47\n",
      "batch: 474, batch train loss: 0.11, train acc: 96.02%, consuming tine: 5.31\n",
      "batch: 475, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.28\n",
      "batch: 476, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.28\n",
      "batch: 477, batch train loss: 0.11, train acc: 96.03%, consuming tine: 5.28\n",
      "batch: 478, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.31\n",
      "batch: 479, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.27\n",
      "batch: 480, batch train loss: 0.11, train acc: 96.04%, consuming tine: 5.57\n",
      "batch: 481, batch train loss: 0.11, train acc: 96.05%, consuming tine: 4.99\n",
      "batch: 482, batch train loss: 0.11, train acc: 96.05%, consuming tine: 5.40\n",
      "batch: 483, batch train loss: 0.11, train acc: 96.06%, consuming tine: 4.98\n",
      "batch: 484, batch train loss: 0.11, train acc: 96.06%, consuming tine: 5.31\n",
      "batch: 485, batch train loss: 0.11, train acc: 96.07%, consuming tine: 4.89\n",
      "batch: 486, batch train loss: 0.11, train acc: 96.07%, consuming tine: 5.28\n",
      "batch: 487, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.20\n",
      "batch: 488, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.57\n",
      "batch: 489, batch train loss: 0.11, train acc: 96.08%, consuming tine: 5.09\n",
      "batch: 490, batch train loss: 0.11, train acc: 96.09%, consuming tine: 5.40\n",
      "batch: 491, batch train loss: 0.11, train acc: 96.09%, consuming tine: 5.68\n",
      "batch: 492, batch train loss: 0.11, train acc: 96.10%, consuming tine: 5.40\n",
      "batch: 493, batch train loss: 0.11, train acc: 96.10%, consuming tine: 4.79\n",
      "batch: 494, batch train loss: 0.11, train acc: 96.10%, consuming tine: 4.96\n",
      "batch: 495, batch train loss: 0.11, train acc: 96.11%, consuming tine: 5.07\n",
      "batch: 496, batch train loss: 0.11, train acc: 96.11%, consuming tine: 4.79\n",
      "batch: 497, batch train loss: 0.11, train acc: 96.12%, consuming tine: 5.27\n",
      "batch: 498, batch train loss: 0.11, train acc: 96.12%, consuming tine: 5.71\n",
      "batch: 499, batch train loss: 0.11, train acc: 96.12%, consuming tine: 5.11\n",
      "batch: 500, batch train loss: 0.11, train acc: 96.12%, consuming tine: 5.35\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 3.78, valid acc: 35.74%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.11, train acc: 96.12%, consuming tine: 5.17\n",
      "batch: 502, batch train loss: 0.11, train acc: 96.13%, consuming tine: 5.39\n",
      "batch: 503, batch train loss: 0.11, train acc: 96.13%, consuming tine: 5.09\n",
      "batch: 504, batch train loss: 0.11, train acc: 96.13%, consuming tine: 5.18\n",
      "batch: 505, batch train loss: 0.11, train acc: 96.13%, consuming tine: 5.37\n",
      "batch: 506, batch train loss: 0.11, train acc: 96.14%, consuming tine: 5.40\n",
      "batch: 507, batch train loss: 0.11, train acc: 96.14%, consuming tine: 5.30\n",
      "batch: 508, batch train loss: 0.11, train acc: 96.14%, consuming tine: 5.47\n",
      "batch: 509, batch train loss: 0.11, train acc: 96.14%, consuming tine: 5.19\n",
      "batch: 510, batch train loss: 0.11, train acc: 96.15%, consuming tine: 5.69\n",
      "batch: 511, batch train loss: 0.11, train acc: 96.15%, consuming tine: 5.58\n",
      "batch: 512, batch train loss: 0.11, train acc: 96.15%, consuming tine: 5.19\n",
      "batch: 513, batch train loss: 0.11, train acc: 96.16%, consuming tine: 5.28\n",
      "batch: 514, batch train loss: 0.11, train acc: 96.16%, consuming tine: 4.69\n",
      "batch: 515, batch train loss: 0.11, train acc: 96.16%, consuming tine: 4.77\n",
      "batch: 516, batch train loss: 0.11, train acc: 96.16%, consuming tine: 5.10\n",
      "batch: 517, batch train loss: 0.11, train acc: 96.16%, consuming tine: 5.17\n",
      "batch: 518, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.36\n",
      "batch: 519, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.31\n",
      "batch: 520, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.20\n",
      "batch: 521, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.38\n",
      "batch: 522, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.18\n",
      "batch: 523, batch train loss: 0.11, train acc: 96.17%, consuming tine: 5.39\n",
      "batch: 524, batch train loss: 0.11, train acc: 96.18%, consuming tine: 4.90\n",
      "batch: 525, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.40\n",
      "batch: 526, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.66\n",
      "batch: 527, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.18\n",
      "batch: 528, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.30\n",
      "batch: 529, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.37\n",
      "batch: 530, batch train loss: 0.11, train acc: 96.18%, consuming tine: 5.39\n",
      "batch: 531, batch train loss: 0.11, train acc: 96.19%, consuming tine: 5.28\n",
      "batch: 532, batch train loss: 0.11, train acc: 96.19%, consuming tine: 5.33\n",
      "batch: 533, batch train loss: 0.11, train acc: 96.19%, consuming tine: 5.15\n",
      "batch: 534, batch train loss: 0.11, train acc: 96.19%, consuming tine: 5.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 535, batch train loss: 0.11, train acc: 96.19%, consuming tine: 5.16\n",
      "batch: 536, batch train loss: 0.11, train acc: 96.20%, consuming tine: 5.80\n",
      "batch: 537, batch train loss: 0.11, train acc: 96.20%, consuming tine: 4.71\n",
      "batch: 538, batch train loss: 0.11, train acc: 96.20%, consuming tine: 5.54\n",
      "batch: 539, batch train loss: 0.11, train acc: 96.20%, consuming tine: 5.18\n",
      "batch: 540, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.30\n",
      "batch: 541, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.60\n",
      "batch: 542, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.18\n",
      "batch: 543, batch train loss: 0.11, train acc: 96.21%, consuming tine: 5.08\n",
      "batch: 544, batch train loss: 0.11, train acc: 96.22%, consuming tine: 6.08\n",
      "batch: 545, batch train loss: 0.11, train acc: 96.22%, consuming tine: 5.08\n",
      "batch: 546, batch train loss: 0.11, train acc: 96.22%, consuming tine: 5.60\n",
      "batch: 547, batch train loss: 0.11, train acc: 96.22%, consuming tine: 5.07\n",
      "batch: 548, batch train loss: 0.11, train acc: 96.22%, consuming tine: 5.18\n",
      "batch: 549, batch train loss: 0.11, train acc: 96.23%, consuming tine: 5.38\n",
      "batch: 550, batch train loss: 0.11, train acc: 96.23%, consuming tine: 4.81\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 3.81, valid acc: 35.70%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.11, train acc: 96.23%, consuming tine: 5.49\n",
      "batch: 552, batch train loss: 0.11, train acc: 96.23%, consuming tine: 4.86\n",
      "batch: 553, batch train loss: 0.11, train acc: 96.23%, consuming tine: 5.53\n",
      "batch: 554, batch train loss: 0.11, train acc: 96.23%, consuming tine: 5.28\n",
      "batch: 555, batch train loss: 0.11, train acc: 96.24%, consuming tine: 5.39\n",
      "batch: 556, batch train loss: 0.11, train acc: 96.24%, consuming tine: 5.66\n",
      "batch: 557, batch train loss: 0.11, train acc: 96.24%, consuming tine: 5.16\n",
      "batch: 558, batch train loss: 0.11, train acc: 96.25%, consuming tine: 5.30\n",
      "batch: 559, batch train loss: 0.11, train acc: 96.25%, consuming tine: 5.18\n",
      "batch: 560, batch train loss: 0.11, train acc: 96.25%, consuming tine: 5.59\n",
      "batch: 561, batch train loss: 0.11, train acc: 96.25%, consuming tine: 5.49\n",
      "batch: 562, batch train loss: 0.11, train acc: 96.25%, consuming tine: 5.39\n",
      "batch: 563, batch train loss: 0.11, train acc: 96.26%, consuming tine: 5.18\n",
      "batch: 564, batch train loss: 0.11, train acc: 96.26%, consuming tine: 5.27\n",
      "batch: 565, batch train loss: 0.11, train acc: 96.26%, consuming tine: 5.21\n",
      "batch: 566, batch train loss: 0.11, train acc: 96.26%, consuming tine: 5.44\n",
      "batch: 567, batch train loss: 0.11, train acc: 96.26%, consuming tine: 4.70\n",
      "batch: 568, batch train loss: 0.11, train acc: 96.27%, consuming tine: 5.19\n",
      "batch: 569, batch train loss: 0.11, train acc: 96.27%, consuming tine: 4.97\n",
      "batch: 570, batch train loss: 0.11, train acc: 96.27%, consuming tine: 4.80\n",
      "batch: 571, batch train loss: 0.11, train acc: 96.28%, consuming tine: 5.27\n",
      "batch: 572, batch train loss: 0.11, train acc: 96.28%, consuming tine: 5.47\n",
      "batch: 573, batch train loss: 0.11, train acc: 96.28%, consuming tine: 5.49\n",
      "batch: 574, batch train loss: 0.11, train acc: 96.28%, consuming tine: 5.19\n",
      "batch: 575, batch train loss: 0.11, train acc: 96.29%, consuming tine: 5.18\n",
      "batch: 576, batch train loss: 0.11, train acc: 96.29%, consuming tine: 5.44\n",
      "batch: 577, batch train loss: 0.11, train acc: 96.29%, consuming tine: 5.35\n",
      "batch: 578, batch train loss: 0.11, train acc: 96.29%, consuming tine: 5.37\n",
      "batch: 579, batch train loss: 0.11, train acc: 96.30%, consuming tine: 5.29\n",
      "batch: 580, batch train loss: 0.11, train acc: 96.30%, consuming tine: 5.37\n",
      "batch: 581, batch train loss: 0.11, train acc: 96.30%, consuming tine: 5.20\n",
      "batch: 582, batch train loss: 0.11, train acc: 96.31%, consuming tine: 5.38\n",
      "batch: 583, batch train loss: 0.11, train acc: 96.31%, consuming tine: 5.23\n",
      "batch: 584, batch train loss: 0.11, train acc: 96.31%, consuming tine: 5.44\n",
      "batch: 585, batch train loss: 0.11, train acc: 96.31%, consuming tine: 5.38\n",
      "batch: 586, batch train loss: 0.11, train acc: 96.31%, consuming tine: 5.61\n",
      "batch: 587, batch train loss: 0.11, train acc: 96.32%, consuming tine: 5.58\n",
      "batch: 588, batch train loss: 0.11, train acc: 96.32%, consuming tine: 5.08\n",
      "batch: 589, batch train loss: 0.11, train acc: 96.32%, consuming tine: 5.68\n",
      "batch: 590, batch train loss: 0.11, train acc: 96.32%, consuming tine: 5.11\n",
      "batch: 591, batch train loss: 0.11, train acc: 96.33%, consuming tine: 5.17\n",
      "batch: 592, batch train loss: 0.11, train acc: 96.33%, consuming tine: 5.17\n",
      "batch: 593, batch train loss: 0.11, train acc: 96.33%, consuming tine: 5.01\n",
      "batch: 594, batch train loss: 0.11, train acc: 96.33%, consuming tine: 5.39\n",
      "batch: 595, batch train loss: 0.11, train acc: 96.34%, consuming tine: 5.09\n",
      "batch: 596, batch train loss: 0.11, train acc: 96.34%, consuming tine: 5.20\n",
      "batch: 597, batch train loss: 0.11, train acc: 96.34%, consuming tine: 5.26\n",
      "batch: 598, batch train loss: 0.11, train acc: 96.34%, consuming tine: 5.09\n",
      "batch: 599, batch train loss: 0.11, train acc: 96.35%, consuming tine: 4.89\n",
      "batch: 600, batch train loss: 0.11, train acc: 96.35%, consuming tine: 4.99\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 3.84, valid acc: 35.60%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.11, train acc: 96.35%, consuming tine: 5.29\n",
      "batch: 602, batch train loss: 0.11, train acc: 96.35%, consuming tine: 5.08\n",
      "batch: 603, batch train loss: 0.11, train acc: 96.36%, consuming tine: 5.39\n",
      "batch: 604, batch train loss: 0.11, train acc: 96.36%, consuming tine: 5.07\n",
      "batch: 605, batch train loss: 0.11, train acc: 96.36%, consuming tine: 5.10\n",
      "batch: 606, batch train loss: 0.11, train acc: 96.36%, consuming tine: 4.70\n",
      "batch: 607, batch train loss: 0.11, train acc: 96.37%, consuming tine: 5.37\n",
      "batch: 608, batch train loss: 0.11, train acc: 96.37%, consuming tine: 5.38\n",
      "batch: 609, batch train loss: 0.11, train acc: 96.37%, consuming tine: 5.28\n",
      "batch: 610, batch train loss: 0.11, train acc: 96.37%, consuming tine: 5.37\n",
      "batch: 611, batch train loss: 0.11, train acc: 96.37%, consuming tine: 4.90\n",
      "batch: 612, batch train loss: 0.11, train acc: 96.38%, consuming tine: 5.56\n",
      "batch: 613, batch train loss: 0.11, train acc: 96.38%, consuming tine: 5.31\n",
      "batch: 614, batch train loss: 0.10, train acc: 96.38%, consuming tine: 5.29\n",
      "batch: 615, batch train loss: 0.10, train acc: 96.38%, consuming tine: 5.09\n",
      "batch: 616, batch train loss: 0.10, train acc: 96.38%, consuming tine: 5.16\n",
      "batch: 617, batch train loss: 0.10, train acc: 96.39%, consuming tine: 5.91\n",
      "batch: 618, batch train loss: 0.10, train acc: 96.39%, consuming tine: 5.27\n",
      "batch: 619, batch train loss: 0.10, train acc: 96.39%, consuming tine: 5.01\n",
      "batch: 620, batch train loss: 0.10, train acc: 96.40%, consuming tine: 5.49\n",
      "batch: 621, batch train loss: 0.10, train acc: 96.40%, consuming tine: 5.37\n",
      "batch: 622, batch train loss: 0.10, train acc: 96.40%, consuming tine: 5.39\n",
      "batch: 623, batch train loss: 0.10, train acc: 96.40%, consuming tine: 5.89\n",
      "batch: 624, batch train loss: 0.10, train acc: 96.41%, consuming tine: 5.17\n",
      "batch: 625, batch train loss: 0.10, train acc: 96.41%, consuming tine: 5.59\n",
      "batch: 626, batch train loss: 0.10, train acc: 96.41%, consuming tine: 5.09\n",
      "batch: 627, batch train loss: 0.10, train acc: 96.41%, consuming tine: 5.00\n",
      "batch: 628, batch train loss: 0.10, train acc: 96.42%, consuming tine: 5.69\n",
      "batch: 629, batch train loss: 0.10, train acc: 96.42%, consuming tine: 4.97\n",
      "batch: 630, batch train loss: 0.10, train acc: 96.42%, consuming tine: 4.99\n",
      "batch: 631, batch train loss: 0.10, train acc: 96.42%, consuming tine: 5.19\n",
      "batch: 632, batch train loss: 0.10, train acc: 96.43%, consuming tine: 5.06\n",
      "batch: 633, batch train loss: 0.10, train acc: 96.43%, consuming tine: 5.40\n",
      "batch: 634, batch train loss: 0.10, train acc: 96.43%, consuming tine: 4.98\n",
      "batch: 635, batch train loss: 0.10, train acc: 96.43%, consuming tine: 5.00\n",
      "batch: 636, batch train loss: 0.10, train acc: 96.44%, consuming tine: 5.67\n",
      "batch: 637, batch train loss: 0.10, train acc: 96.44%, consuming tine: 5.07\n",
      "batch: 638, batch train loss: 0.10, train acc: 96.44%, consuming tine: 5.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 639, batch train loss: 0.10, train acc: 96.44%, consuming tine: 5.36\n",
      "batch: 640, batch train loss: 0.10, train acc: 96.44%, consuming tine: 5.91\n",
      "batch: 641, batch train loss: 0.10, train acc: 96.45%, consuming tine: 5.51\n",
      "batch: 642, batch train loss: 0.10, train acc: 96.45%, consuming tine: 4.80\n",
      "batch: 643, batch train loss: 0.10, train acc: 96.45%, consuming tine: 5.48\n",
      "batch: 644, batch train loss: 0.10, train acc: 96.45%, consuming tine: 5.28\n",
      "batch: 645, batch train loss: 0.10, train acc: 96.46%, consuming tine: 5.08\n",
      "batch: 646, batch train loss: 0.10, train acc: 96.46%, consuming tine: 5.48\n",
      "batch: 647, batch train loss: 0.10, train acc: 96.46%, consuming tine: 5.49\n",
      "batch: 648, batch train loss: 0.10, train acc: 96.46%, consuming tine: 5.29\n",
      "batch: 649, batch train loss: 0.10, train acc: 96.46%, consuming tine: 5.08\n",
      "batch: 650, batch train loss: 0.10, train acc: 96.47%, consuming tine: 5.47\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 3.86, valid acc: 35.55%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.10, train acc: 96.47%, consuming tine: 5.60\n",
      "batch: 652, batch train loss: 0.10, train acc: 96.47%, consuming tine: 5.38\n",
      "batch: 653, batch train loss: 0.10, train acc: 96.47%, consuming tine: 4.77\n",
      "batch: 654, batch train loss: 0.10, train acc: 96.48%, consuming tine: 5.49\n",
      "batch: 655, batch train loss: 0.10, train acc: 96.48%, consuming tine: 4.98\n",
      "batch: 656, batch train loss: 0.10, train acc: 96.48%, consuming tine: 5.00\n",
      "batch: 657, batch train loss: 0.10, train acc: 96.48%, consuming tine: 5.57\n",
      "batch: 658, batch train loss: 0.10, train acc: 96.48%, consuming tine: 5.28\n",
      "batch: 659, batch train loss: 0.10, train acc: 96.48%, consuming tine: 5.90\n",
      "batch: 660, batch train loss: 0.10, train acc: 96.48%, consuming tine: 4.89\n",
      "batch: 661, batch train loss: 0.10, train acc: 96.49%, consuming tine: 4.78\n",
      "batch: 662, batch train loss: 0.10, train acc: 96.49%, consuming tine: 5.12\n",
      "batch: 663, batch train loss: 0.10, train acc: 96.49%, consuming tine: 5.05\n",
      "batch: 664, batch train loss: 0.10, train acc: 96.49%, consuming tine: 5.00\n",
      "batch: 665, batch train loss: 0.10, train acc: 96.49%, consuming tine: 5.09\n",
      "batch: 666, batch train loss: 0.10, train acc: 96.49%, consuming tine: 4.66\n",
      "batch: 667, batch train loss: 0.10, train acc: 96.50%, consuming tine: 5.01\n",
      "batch: 668, batch train loss: 0.10, train acc: 96.50%, consuming tine: 5.09\n",
      "batch: 669, batch train loss: 0.10, train acc: 96.50%, consuming tine: 4.97\n",
      "batch: 670, batch train loss: 0.10, train acc: 96.50%, consuming tine: 4.78\n",
      "batch: 671, batch train loss: 0.10, train acc: 96.50%, consuming tine: 4.88\n",
      "batch: 672, batch train loss: 0.10, train acc: 96.51%, consuming tine: 5.51\n",
      "batch: 673, batch train loss: 0.10, train acc: 96.51%, consuming tine: 5.46\n",
      "batch: 674, batch train loss: 0.10, train acc: 96.51%, consuming tine: 4.87\n",
      "batch: 675, batch train loss: 0.10, train acc: 96.51%, consuming tine: 5.51\n",
      "batch: 676, batch train loss: 0.10, train acc: 96.51%, consuming tine: 5.43\n",
      "batch: 677, batch train loss: 0.10, train acc: 96.52%, consuming tine: 4.94\n",
      "batch: 678, batch train loss: 0.10, train acc: 96.52%, consuming tine: 5.41\n",
      "batch: 679, batch train loss: 0.10, train acc: 96.52%, consuming tine: 4.77\n",
      "batch: 680, batch train loss: 0.10, train acc: 96.52%, consuming tine: 5.31\n",
      "batch: 681, batch train loss: 0.10, train acc: 96.52%, consuming tine: 5.16\n",
      "batch: 682, batch train loss: 0.10, train acc: 96.53%, consuming tine: 4.90\n",
      "batch: 683, batch train loss: 0.10, train acc: 96.53%, consuming tine: 5.59\n",
      "batch: 684, batch train loss: 0.10, train acc: 96.53%, consuming tine: 5.02\n",
      "batch: 685, batch train loss: 0.10, train acc: 96.53%, consuming tine: 5.77\n",
      "batch: 686, batch train loss: 0.10, train acc: 96.53%, consuming tine: 4.97\n",
      "batch: 687, batch train loss: 0.10, train acc: 96.54%, consuming tine: 5.30\n",
      "batch: 688, batch train loss: 0.10, train acc: 96.54%, consuming tine: 5.07\n",
      "batch: 689, batch train loss: 0.10, train acc: 96.54%, consuming tine: 5.11\n",
      "batch: 690, batch train loss: 0.10, train acc: 96.54%, consuming tine: 4.88\n",
      "batch: 691, batch train loss: 0.10, train acc: 96.55%, consuming tine: 5.48\n",
      "batch: 692, batch train loss: 0.10, train acc: 96.55%, consuming tine: 4.98\n",
      "batch: 693, batch train loss: 0.10, train acc: 96.55%, consuming tine: 5.19\n",
      "batch: 694, batch train loss: 0.10, train acc: 96.55%, consuming tine: 5.19\n",
      "batch: 695, batch train loss: 0.10, train acc: 96.55%, consuming tine: 4.88\n",
      "batch: 696, batch train loss: 0.10, train acc: 96.56%, consuming tine: 5.18\n",
      "batch: 697, batch train loss: 0.10, train acc: 96.56%, consuming tine: 5.13\n",
      "batch: 698, batch train loss: 0.10, train acc: 96.56%, consuming tine: 5.16\n",
      "batch: 699, batch train loss: 0.10, train acc: 96.56%, consuming tine: 5.30\n",
      "batch: 700, batch train loss: 0.10, train acc: 96.57%, consuming tine: 4.98\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 3.89, valid acc: 35.54%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.10, train acc: 96.57%, consuming tine: 5.30\n",
      "batch: 702, batch train loss: 0.10, train acc: 96.57%, consuming tine: 4.78\n",
      "batch: 703, batch train loss: 0.10, train acc: 96.57%, consuming tine: 5.06\n",
      "batch: 704, batch train loss: 0.10, train acc: 96.57%, consuming tine: 5.21\n",
      "Epoch 6, Loss: 0.10, Accuracy: 96.57%, Valid Loss: 3.89, Valid Accuracy: 35.54%\n",
      "batch: 1, batch train loss: 0.05, train acc: 98.73%, consuming tine: 5.14\n",
      "batch: 2, batch train loss: 0.06, train acc: 98.19%, consuming tine: 4.98\n",
      "batch: 3, batch train loss: 0.06, train acc: 98.24%, consuming tine: 5.47\n",
      "batch: 4, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.41\n",
      "batch: 5, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.36\n",
      "batch: 6, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.19\n",
      "batch: 7, batch train loss: 0.06, train acc: 98.16%, consuming tine: 5.49\n",
      "batch: 8, batch train loss: 0.06, train acc: 98.19%, consuming tine: 4.79\n",
      "batch: 9, batch train loss: 0.06, train acc: 98.19%, consuming tine: 5.09\n",
      "batch: 10, batch train loss: 0.06, train acc: 98.24%, consuming tine: 4.98\n",
      "batch: 11, batch train loss: 0.06, train acc: 98.24%, consuming tine: 4.98\n",
      "batch: 12, batch train loss: 0.05, train acc: 98.25%, consuming tine: 5.38\n",
      "batch: 13, batch train loss: 0.05, train acc: 98.28%, consuming tine: 4.90\n",
      "batch: 14, batch train loss: 0.05, train acc: 98.28%, consuming tine: 5.06\n",
      "batch: 15, batch train loss: 0.05, train acc: 98.28%, consuming tine: 5.11\n",
      "batch: 16, batch train loss: 0.05, train acc: 98.24%, consuming tine: 5.27\n",
      "batch: 17, batch train loss: 0.05, train acc: 98.24%, consuming tine: 5.22\n",
      "batch: 18, batch train loss: 0.05, train acc: 98.27%, consuming tine: 5.16\n",
      "batch: 19, batch train loss: 0.05, train acc: 98.29%, consuming tine: 5.09\n",
      "batch: 20, batch train loss: 0.05, train acc: 98.29%, consuming tine: 4.90\n",
      "batch: 21, batch train loss: 0.05, train acc: 98.30%, consuming tine: 5.08\n",
      "batch: 22, batch train loss: 0.05, train acc: 98.27%, consuming tine: 4.78\n",
      "batch: 23, batch train loss: 0.05, train acc: 98.27%, consuming tine: 4.88\n",
      "batch: 24, batch train loss: 0.05, train acc: 98.27%, consuming tine: 5.18\n",
      "batch: 25, batch train loss: 0.05, train acc: 98.28%, consuming tine: 5.00\n",
      "batch: 26, batch train loss: 0.06, train acc: 98.25%, consuming tine: 5.04\n",
      "batch: 27, batch train loss: 0.06, train acc: 98.26%, consuming tine: 5.05\n",
      "batch: 28, batch train loss: 0.06, train acc: 98.23%, consuming tine: 5.27\n",
      "batch: 29, batch train loss: 0.06, train acc: 98.22%, consuming tine: 5.08\n",
      "batch: 30, batch train loss: 0.06, train acc: 98.22%, consuming tine: 4.79\n",
      "batch: 31, batch train loss: 0.06, train acc: 98.24%, consuming tine: 5.09\n",
      "batch: 32, batch train loss: 0.06, train acc: 98.25%, consuming tine: 5.31\n",
      "batch: 33, batch train loss: 0.06, train acc: 98.24%, consuming tine: 4.97\n",
      "batch: 34, batch train loss: 0.06, train acc: 98.20%, consuming tine: 5.18\n",
      "batch: 35, batch train loss: 0.06, train acc: 98.15%, consuming tine: 4.82\n",
      "batch: 36, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.16\n",
      "batch: 37, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.00\n",
      "batch: 38, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 39, batch train loss: 0.06, train acc: 98.14%, consuming tine: 4.79\n",
      "batch: 40, batch train loss: 0.06, train acc: 98.14%, consuming tine: 4.68\n",
      "batch: 41, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.09\n",
      "batch: 42, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.29\n",
      "batch: 43, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.10\n",
      "batch: 44, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.24\n",
      "batch: 45, batch train loss: 0.06, train acc: 98.07%, consuming tine: 4.92\n",
      "batch: 46, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.50\n",
      "batch: 47, batch train loss: 0.06, train acc: 98.02%, consuming tine: 4.68\n",
      "batch: 48, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.30\n",
      "batch: 49, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.76\n",
      "batch: 50, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.31\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 4.20, valid acc: 35.56%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.39\n",
      "batch: 52, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.89\n",
      "batch: 53, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.39\n",
      "batch: 54, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.40\n",
      "batch: 55, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.59\n",
      "batch: 56, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.27\n",
      "batch: 57, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.59\n",
      "batch: 58, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.28\n",
      "batch: 59, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.07\n",
      "batch: 60, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.12\n",
      "batch: 61, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.29\n",
      "batch: 62, batch train loss: 0.06, train acc: 97.97%, consuming tine: 4.69\n",
      "batch: 63, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.17\n",
      "batch: 64, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.38\n",
      "batch: 65, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.11\n",
      "batch: 66, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.90\n",
      "batch: 67, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.76\n",
      "batch: 68, batch train loss: 0.06, train acc: 97.97%, consuming tine: 4.88\n",
      "batch: 69, batch train loss: 0.06, train acc: 97.95%, consuming tine: 5.09\n",
      "batch: 70, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.09\n",
      "batch: 71, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.60\n",
      "batch: 72, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.28\n",
      "batch: 73, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.10\n",
      "batch: 74, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.40\n",
      "batch: 75, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.66\n",
      "batch: 76, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.30\n",
      "batch: 77, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.20\n",
      "batch: 78, batch train loss: 0.06, train acc: 98.00%, consuming tine: 4.85\n",
      "batch: 79, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.40\n",
      "batch: 80, batch train loss: 0.06, train acc: 98.01%, consuming tine: 4.90\n",
      "batch: 81, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.07\n",
      "batch: 82, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.26\n",
      "batch: 83, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.22\n",
      "batch: 84, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.17\n",
      "batch: 85, batch train loss: 0.06, train acc: 97.99%, consuming tine: 4.80\n",
      "batch: 86, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.08\n",
      "batch: 87, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.78\n",
      "batch: 88, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.11\n",
      "batch: 89, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.57\n",
      "batch: 90, batch train loss: 0.06, train acc: 98.01%, consuming tine: 4.60\n",
      "batch: 91, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.09\n",
      "batch: 92, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.19\n",
      "batch: 93, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.28\n",
      "batch: 94, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.03\n",
      "batch: 95, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.16\n",
      "batch: 96, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.47\n",
      "batch: 97, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.20\n",
      "batch: 98, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.17\n",
      "batch: 99, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.02\n",
      "batch: 100, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.27\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 4.20, valid acc: 35.46%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.00\n",
      "batch: 102, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.40\n",
      "batch: 103, batch train loss: 0.06, train acc: 98.03%, consuming tine: 4.89\n",
      "batch: 104, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.80\n",
      "batch: 105, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.87\n",
      "batch: 106, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.41\n",
      "batch: 107, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.06\n",
      "batch: 108, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.93\n",
      "batch: 109, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.15\n",
      "batch: 110, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.08\n",
      "batch: 111, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.51\n",
      "batch: 112, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.97\n",
      "batch: 113, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.09\n",
      "batch: 114, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.59\n",
      "batch: 115, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.29\n",
      "batch: 116, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.31\n",
      "batch: 117, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.07\n",
      "batch: 118, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.89\n",
      "batch: 119, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.29\n",
      "batch: 120, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.97\n",
      "batch: 121, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.20\n",
      "batch: 122, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.49\n",
      "batch: 123, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.91\n",
      "batch: 124, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.87\n",
      "batch: 125, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.98\n",
      "batch: 126, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.10\n",
      "batch: 127, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.08\n",
      "batch: 128, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.71\n",
      "batch: 129, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.86\n",
      "batch: 130, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.20\n",
      "batch: 131, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.30\n",
      "batch: 132, batch train loss: 0.06, train acc: 98.07%, consuming tine: 4.87\n",
      "batch: 133, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.01\n",
      "batch: 134, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.48\n",
      "batch: 135, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.90\n",
      "batch: 136, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.19\n",
      "batch: 137, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.17\n",
      "batch: 138, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.17\n",
      "batch: 139, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.92\n",
      "batch: 140, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.37\n",
      "batch: 141, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.88\n",
      "batch: 142, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.29\n",
      "batch: 143, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 144, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.39\n",
      "batch: 145, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.39\n",
      "batch: 146, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.67\n",
      "batch: 147, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.59\n",
      "batch: 148, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.48\n",
      "batch: 149, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.60\n",
      "batch: 150, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.40\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 4.28, valid acc: 35.36%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.03\n",
      "batch: 152, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.37\n",
      "batch: 153, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.10\n",
      "batch: 154, batch train loss: 0.06, train acc: 98.13%, consuming tine: 4.99\n",
      "batch: 155, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.09\n",
      "batch: 156, batch train loss: 0.06, train acc: 98.14%, consuming tine: 4.87\n",
      "batch: 157, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.34\n",
      "batch: 158, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.18\n",
      "batch: 159, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.06\n",
      "batch: 160, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.38\n",
      "batch: 161, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.19\n",
      "batch: 162, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.02\n",
      "batch: 163, batch train loss: 0.06, train acc: 98.13%, consuming tine: 4.68\n",
      "batch: 164, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.08\n",
      "batch: 165, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.48\n",
      "batch: 166, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.30\n",
      "batch: 167, batch train loss: 0.06, train acc: 98.12%, consuming tine: 4.69\n",
      "batch: 168, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.23\n",
      "batch: 169, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.05\n",
      "batch: 170, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.16\n",
      "batch: 171, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.40\n",
      "batch: 172, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.73\n",
      "batch: 173, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.26\n",
      "batch: 174, batch train loss: 0.06, train acc: 98.12%, consuming tine: 4.89\n",
      "batch: 175, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.07\n",
      "batch: 176, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.00\n",
      "batch: 177, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.51\n",
      "batch: 178, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.28\n",
      "batch: 179, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.89\n",
      "batch: 180, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.57\n",
      "batch: 181, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.41\n",
      "batch: 182, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.28\n",
      "batch: 183, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.28\n",
      "batch: 184, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.59\n",
      "batch: 185, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.08\n",
      "batch: 186, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.69\n",
      "batch: 187, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.98\n",
      "batch: 188, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.09\n",
      "batch: 189, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.89\n",
      "batch: 190, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.99\n",
      "batch: 191, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.00\n",
      "batch: 192, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.16\n",
      "batch: 193, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.02\n",
      "batch: 194, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.07\n",
      "batch: 195, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.92\n",
      "batch: 196, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.25\n",
      "batch: 197, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.24\n",
      "batch: 198, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.06\n",
      "batch: 199, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.78\n",
      "batch: 200, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.19\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 4.31, valid acc: 35.45%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.13\n",
      "batch: 202, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.25\n",
      "batch: 203, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.30\n",
      "batch: 204, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.28\n",
      "batch: 205, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.01\n",
      "batch: 206, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.40\n",
      "batch: 207, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.52\n",
      "batch: 208, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.24\n",
      "batch: 209, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.01\n",
      "batch: 210, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.58\n",
      "batch: 211, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.29\n",
      "batch: 212, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.37\n",
      "batch: 213, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.39\n",
      "batch: 214, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.69\n",
      "batch: 215, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.19\n",
      "batch: 216, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.11\n",
      "batch: 217, batch train loss: 0.06, train acc: 98.02%, consuming tine: 4.89\n",
      "batch: 218, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.27\n",
      "batch: 219, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.09\n",
      "batch: 220, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.09\n",
      "batch: 221, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.47\n",
      "batch: 222, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.59\n",
      "batch: 223, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.19\n",
      "batch: 224, batch train loss: 0.06, train acc: 97.97%, consuming tine: 4.89\n",
      "batch: 225, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.79\n",
      "batch: 226, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.57\n",
      "batch: 227, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.20\n",
      "batch: 228, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.40\n",
      "batch: 229, batch train loss: 0.06, train acc: 97.95%, consuming tine: 5.08\n",
      "batch: 230, batch train loss: 0.06, train acc: 97.94%, consuming tine: 5.27\n",
      "batch: 231, batch train loss: 0.06, train acc: 97.92%, consuming tine: 5.01\n",
      "batch: 232, batch train loss: 0.06, train acc: 97.92%, consuming tine: 4.97\n",
      "batch: 233, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.28\n",
      "batch: 234, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.50\n",
      "batch: 235, batch train loss: 0.06, train acc: 97.90%, consuming tine: 4.88\n",
      "batch: 236, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.20\n",
      "batch: 237, batch train loss: 0.06, train acc: 97.90%, consuming tine: 4.89\n",
      "batch: 238, batch train loss: 0.06, train acc: 97.90%, consuming tine: 4.88\n",
      "batch: 239, batch train loss: 0.06, train acc: 97.90%, consuming tine: 4.70\n",
      "batch: 240, batch train loss: 0.06, train acc: 97.89%, consuming tine: 5.17\n",
      "batch: 241, batch train loss: 0.06, train acc: 97.88%, consuming tine: 5.40\n",
      "batch: 242, batch train loss: 0.06, train acc: 97.87%, consuming tine: 4.89\n",
      "batch: 243, batch train loss: 0.06, train acc: 97.85%, consuming tine: 5.29\n",
      "batch: 244, batch train loss: 0.06, train acc: 97.83%, consuming tine: 5.18\n",
      "batch: 245, batch train loss: 0.06, train acc: 97.82%, consuming tine: 5.19\n",
      "batch: 246, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.51\n",
      "batch: 247, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 248, batch train loss: 0.07, train acc: 97.80%, consuming tine: 5.19\n",
      "batch: 249, batch train loss: 0.07, train acc: 97.79%, consuming tine: 4.98\n",
      "batch: 250, batch train loss: 0.07, train acc: 97.78%, consuming tine: 5.20\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 4.26, valid acc: 35.35%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.07, train acc: 97.78%, consuming tine: 4.99\n",
      "batch: 252, batch train loss: 0.07, train acc: 97.77%, consuming tine: 5.51\n",
      "batch: 253, batch train loss: 0.07, train acc: 97.77%, consuming tine: 4.77\n",
      "batch: 254, batch train loss: 0.07, train acc: 97.76%, consuming tine: 5.17\n",
      "batch: 255, batch train loss: 0.07, train acc: 97.75%, consuming tine: 4.79\n",
      "batch: 256, batch train loss: 0.07, train acc: 97.74%, consuming tine: 4.91\n",
      "batch: 257, batch train loss: 0.07, train acc: 97.73%, consuming tine: 5.57\n",
      "batch: 258, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.10\n",
      "batch: 259, batch train loss: 0.07, train acc: 97.71%, consuming tine: 5.09\n",
      "batch: 260, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.77\n",
      "batch: 261, batch train loss: 0.07, train acc: 97.69%, consuming tine: 4.79\n",
      "batch: 262, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.18\n",
      "batch: 263, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.19\n",
      "batch: 264, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.38\n",
      "batch: 265, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.58\n",
      "batch: 266, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.10\n",
      "batch: 267, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.08\n",
      "batch: 268, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.51\n",
      "batch: 269, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.26\n",
      "batch: 270, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.50\n",
      "batch: 271, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.50\n",
      "batch: 272, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.89\n",
      "batch: 273, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.10\n",
      "batch: 274, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.37\n",
      "batch: 275, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.59\n",
      "batch: 276, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.60\n",
      "batch: 277, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.08\n",
      "batch: 278, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.18\n",
      "batch: 279, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.40\n",
      "batch: 280, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.19\n",
      "batch: 281, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.69\n",
      "batch: 282, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.39\n",
      "batch: 283, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.36\n",
      "batch: 284, batch train loss: 0.07, train acc: 97.62%, consuming tine: 4.71\n",
      "batch: 285, batch train loss: 0.07, train acc: 97.62%, consuming tine: 5.08\n",
      "batch: 286, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.88\n",
      "batch: 287, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.19\n",
      "batch: 288, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.49\n",
      "batch: 289, batch train loss: 0.07, train acc: 97.63%, consuming tine: 4.92\n",
      "batch: 290, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.44\n",
      "batch: 291, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.20\n",
      "batch: 292, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.68\n",
      "batch: 293, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.39\n",
      "batch: 294, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.11\n",
      "batch: 295, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.07\n",
      "batch: 296, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.29\n",
      "batch: 297, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.48\n",
      "batch: 298, batch train loss: 0.07, train acc: 97.63%, consuming tine: 4.89\n",
      "batch: 299, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.49\n",
      "batch: 300, batch train loss: 0.07, train acc: 97.63%, consuming tine: 4.98\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 4.20, valid acc: 35.43%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.07, train acc: 97.63%, consuming tine: 4.97\n",
      "batch: 302, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.28\n",
      "batch: 303, batch train loss: 0.07, train acc: 97.63%, consuming tine: 4.81\n",
      "batch: 304, batch train loss: 0.07, train acc: 97.63%, consuming tine: 5.07\n",
      "batch: 305, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.91\n",
      "batch: 306, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.98\n",
      "batch: 307, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.49\n",
      "batch: 308, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.80\n",
      "batch: 309, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.96\n",
      "batch: 310, batch train loss: 0.07, train acc: 97.64%, consuming tine: 5.91\n",
      "batch: 311, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.87\n",
      "batch: 312, batch train loss: 0.07, train acc: 97.64%, consuming tine: 4.71\n",
      "batch: 313, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.29\n",
      "batch: 314, batch train loss: 0.07, train acc: 97.65%, consuming tine: 4.97\n",
      "batch: 315, batch train loss: 0.07, train acc: 97.65%, consuming tine: 4.99\n",
      "batch: 316, batch train loss: 0.07, train acc: 97.65%, consuming tine: 4.79\n",
      "batch: 317, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.18\n",
      "batch: 318, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.19\n",
      "batch: 319, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.53\n",
      "batch: 320, batch train loss: 0.07, train acc: 97.65%, consuming tine: 4.85\n",
      "batch: 321, batch train loss: 0.07, train acc: 97.65%, consuming tine: 5.49\n",
      "batch: 322, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.20\n",
      "batch: 323, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.26\n",
      "batch: 324, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.61\n",
      "batch: 325, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.21\n",
      "batch: 326, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.25\n",
      "batch: 327, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.18\n",
      "batch: 328, batch train loss: 0.07, train acc: 97.66%, consuming tine: 4.92\n",
      "batch: 329, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.16\n",
      "batch: 330, batch train loss: 0.07, train acc: 97.66%, consuming tine: 5.11\n",
      "batch: 331, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.75\n",
      "batch: 332, batch train loss: 0.07, train acc: 97.67%, consuming tine: 4.91\n",
      "batch: 333, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.17\n",
      "batch: 334, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.19\n",
      "batch: 335, batch train loss: 0.07, train acc: 97.67%, consuming tine: 5.24\n",
      "batch: 336, batch train loss: 0.07, train acc: 97.67%, consuming tine: 4.96\n",
      "batch: 337, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.49\n",
      "batch: 338, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.15\n",
      "batch: 339, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.10\n",
      "batch: 340, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.09\n",
      "batch: 341, batch train loss: 0.07, train acc: 97.68%, consuming tine: 4.89\n",
      "batch: 342, batch train loss: 0.07, train acc: 97.68%, consuming tine: 4.89\n",
      "batch: 343, batch train loss: 0.07, train acc: 97.68%, consuming tine: 5.69\n",
      "batch: 344, batch train loss: 0.07, train acc: 97.69%, consuming tine: 4.96\n",
      "batch: 345, batch train loss: 0.07, train acc: 97.69%, consuming tine: 4.99\n",
      "batch: 346, batch train loss: 0.07, train acc: 97.69%, consuming tine: 5.59\n",
      "batch: 347, batch train loss: 0.07, train acc: 97.69%, consuming tine: 5.57\n",
      "batch: 348, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.11\n",
      "batch: 349, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.17\n",
      "batch: 350, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.31\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 4.21, valid acc: 35.45%\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 351, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.65\n",
      "batch: 352, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.28\n",
      "batch: 353, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.08\n",
      "batch: 354, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.39\n",
      "batch: 355, batch train loss: 0.07, train acc: 97.70%, consuming tine: 4.96\n",
      "batch: 356, batch train loss: 0.07, train acc: 97.70%, consuming tine: 5.31\n",
      "batch: 357, batch train loss: 0.07, train acc: 97.71%, consuming tine: 5.18\n",
      "batch: 358, batch train loss: 0.07, train acc: 97.71%, consuming tine: 5.49\n",
      "batch: 359, batch train loss: 0.07, train acc: 97.71%, consuming tine: 5.29\n",
      "batch: 360, batch train loss: 0.07, train acc: 97.71%, consuming tine: 5.38\n",
      "batch: 361, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.13\n",
      "batch: 362, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.76\n",
      "batch: 363, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.38\n",
      "batch: 364, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.43\n",
      "batch: 365, batch train loss: 0.07, train acc: 97.72%, consuming tine: 5.15\n",
      "batch: 366, batch train loss: 0.07, train acc: 97.72%, consuming tine: 4.98\n",
      "batch: 367, batch train loss: 0.07, train acc: 97.73%, consuming tine: 5.28\n",
      "batch: 368, batch train loss: 0.07, train acc: 97.73%, consuming tine: 5.10\n",
      "batch: 369, batch train loss: 0.07, train acc: 97.73%, consuming tine: 4.80\n",
      "batch: 370, batch train loss: 0.07, train acc: 97.73%, consuming tine: 5.27\n",
      "batch: 371, batch train loss: 0.07, train acc: 97.74%, consuming tine: 5.59\n",
      "batch: 372, batch train loss: 0.07, train acc: 97.74%, consuming tine: 5.59\n",
      "batch: 373, batch train loss: 0.07, train acc: 97.74%, consuming tine: 4.82\n",
      "batch: 374, batch train loss: 0.07, train acc: 97.74%, consuming tine: 5.05\n",
      "batch: 375, batch train loss: 0.07, train acc: 97.74%, consuming tine: 5.79\n",
      "batch: 376, batch train loss: 0.07, train acc: 97.74%, consuming tine: 4.89\n",
      "batch: 377, batch train loss: 0.07, train acc: 97.74%, consuming tine: 4.99\n",
      "batch: 378, batch train loss: 0.07, train acc: 97.74%, consuming tine: 5.28\n",
      "batch: 379, batch train loss: 0.07, train acc: 97.75%, consuming tine: 5.01\n",
      "batch: 380, batch train loss: 0.07, train acc: 97.75%, consuming tine: 5.17\n",
      "batch: 381, batch train loss: 0.07, train acc: 97.75%, consuming tine: 5.18\n",
      "batch: 382, batch train loss: 0.07, train acc: 97.75%, consuming tine: 4.89\n",
      "batch: 383, batch train loss: 0.07, train acc: 97.75%, consuming tine: 5.23\n",
      "batch: 384, batch train loss: 0.07, train acc: 97.75%, consuming tine: 4.56\n",
      "batch: 385, batch train loss: 0.07, train acc: 97.75%, consuming tine: 5.38\n",
      "batch: 386, batch train loss: 0.07, train acc: 97.76%, consuming tine: 4.59\n",
      "batch: 387, batch train loss: 0.07, train acc: 97.76%, consuming tine: 4.90\n",
      "batch: 388, batch train loss: 0.07, train acc: 97.76%, consuming tine: 4.78\n",
      "batch: 389, batch train loss: 0.07, train acc: 97.76%, consuming tine: 5.10\n",
      "batch: 390, batch train loss: 0.07, train acc: 97.76%, consuming tine: 5.16\n",
      "batch: 391, batch train loss: 0.07, train acc: 97.76%, consuming tine: 5.20\n",
      "batch: 392, batch train loss: 0.07, train acc: 97.76%, consuming tine: 4.69\n",
      "batch: 393, batch train loss: 0.07, train acc: 97.77%, consuming tine: 5.70\n",
      "batch: 394, batch train loss: 0.07, train acc: 97.77%, consuming tine: 4.98\n",
      "batch: 395, batch train loss: 0.07, train acc: 97.77%, consuming tine: 5.08\n",
      "batch: 396, batch train loss: 0.07, train acc: 97.77%, consuming tine: 5.62\n",
      "batch: 397, batch train loss: 0.07, train acc: 97.78%, consuming tine: 4.87\n",
      "batch: 398, batch train loss: 0.07, train acc: 97.78%, consuming tine: 5.20\n",
      "batch: 399, batch train loss: 0.07, train acc: 97.78%, consuming tine: 5.47\n",
      "batch: 400, batch train loss: 0.07, train acc: 97.78%, consuming tine: 5.09\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 4.23, valid acc: 35.42%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.07, train acc: 97.78%, consuming tine: 5.00\n",
      "batch: 402, batch train loss: 0.07, train acc: 97.79%, consuming tine: 5.10\n",
      "batch: 403, batch train loss: 0.07, train acc: 97.79%, consuming tine: 4.97\n",
      "batch: 404, batch train loss: 0.07, train acc: 97.79%, consuming tine: 5.20\n",
      "batch: 405, batch train loss: 0.07, train acc: 97.79%, consuming tine: 4.99\n",
      "batch: 406, batch train loss: 0.07, train acc: 97.79%, consuming tine: 5.28\n",
      "batch: 407, batch train loss: 0.07, train acc: 97.79%, consuming tine: 5.07\n",
      "batch: 408, batch train loss: 0.07, train acc: 97.80%, consuming tine: 5.01\n",
      "batch: 409, batch train loss: 0.07, train acc: 97.80%, consuming tine: 5.20\n",
      "batch: 410, batch train loss: 0.07, train acc: 97.80%, consuming tine: 5.26\n",
      "batch: 411, batch train loss: 0.07, train acc: 97.80%, consuming tine: 4.91\n",
      "batch: 412, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.37\n",
      "batch: 413, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.27\n",
      "batch: 414, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.41\n",
      "batch: 415, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.18\n",
      "batch: 416, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.19\n",
      "batch: 417, batch train loss: 0.07, train acc: 97.81%, consuming tine: 5.08\n",
      "batch: 418, batch train loss: 0.07, train acc: 97.82%, consuming tine: 5.00\n",
      "batch: 419, batch train loss: 0.07, train acc: 97.82%, consuming tine: 5.19\n",
      "batch: 420, batch train loss: 0.07, train acc: 97.82%, consuming tine: 5.28\n",
      "batch: 421, batch train loss: 0.07, train acc: 97.82%, consuming tine: 4.97\n",
      "batch: 422, batch train loss: 0.07, train acc: 97.82%, consuming tine: 5.10\n",
      "batch: 423, batch train loss: 0.07, train acc: 97.83%, consuming tine: 5.27\n",
      "batch: 424, batch train loss: 0.07, train acc: 97.83%, consuming tine: 5.18\n",
      "batch: 425, batch train loss: 0.07, train acc: 97.83%, consuming tine: 5.31\n",
      "batch: 426, batch train loss: 0.07, train acc: 97.83%, consuming tine: 4.70\n",
      "batch: 427, batch train loss: 0.07, train acc: 97.83%, consuming tine: 5.17\n",
      "batch: 428, batch train loss: 0.07, train acc: 97.83%, consuming tine: 5.01\n",
      "batch: 429, batch train loss: 0.06, train acc: 97.83%, consuming tine: 5.28\n",
      "batch: 430, batch train loss: 0.06, train acc: 97.84%, consuming tine: 5.17\n",
      "batch: 431, batch train loss: 0.06, train acc: 97.84%, consuming tine: 5.40\n",
      "batch: 432, batch train loss: 0.06, train acc: 97.84%, consuming tine: 5.17\n",
      "batch: 433, batch train loss: 0.06, train acc: 97.84%, consuming tine: 5.01\n",
      "batch: 434, batch train loss: 0.06, train acc: 97.84%, consuming tine: 5.38\n",
      "batch: 435, batch train loss: 0.06, train acc: 97.85%, consuming tine: 5.57\n",
      "batch: 436, batch train loss: 0.06, train acc: 97.85%, consuming tine: 4.92\n",
      "batch: 437, batch train loss: 0.06, train acc: 97.85%, consuming tine: 4.94\n",
      "batch: 438, batch train loss: 0.06, train acc: 97.85%, consuming tine: 5.23\n",
      "batch: 439, batch train loss: 0.06, train acc: 97.85%, consuming tine: 5.29\n",
      "batch: 440, batch train loss: 0.06, train acc: 97.86%, consuming tine: 5.07\n",
      "batch: 441, batch train loss: 0.06, train acc: 97.86%, consuming tine: 5.39\n",
      "batch: 442, batch train loss: 0.06, train acc: 97.86%, consuming tine: 5.07\n",
      "batch: 443, batch train loss: 0.06, train acc: 97.86%, consuming tine: 5.41\n",
      "batch: 444, batch train loss: 0.06, train acc: 97.86%, consuming tine: 5.32\n",
      "batch: 445, batch train loss: 0.06, train acc: 97.87%, consuming tine: 5.36\n",
      "batch: 446, batch train loss: 0.06, train acc: 97.87%, consuming tine: 4.87\n",
      "batch: 447, batch train loss: 0.06, train acc: 97.87%, consuming tine: 5.72\n",
      "batch: 448, batch train loss: 0.06, train acc: 97.87%, consuming tine: 5.37\n",
      "batch: 449, batch train loss: 0.06, train acc: 97.87%, consuming tine: 4.68\n",
      "batch: 450, batch train loss: 0.06, train acc: 97.87%, consuming tine: 5.17\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 4.26, valid acc: 35.41%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.06, train acc: 97.88%, consuming tine: 4.90\n",
      "batch: 452, batch train loss: 0.06, train acc: 97.88%, consuming tine: 5.28\n",
      "batch: 453, batch train loss: 0.06, train acc: 97.88%, consuming tine: 5.07\n",
      "batch: 454, batch train loss: 0.06, train acc: 97.88%, consuming tine: 5.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 455, batch train loss: 0.06, train acc: 97.88%, consuming tine: 5.30\n",
      "batch: 456, batch train loss: 0.06, train acc: 97.89%, consuming tine: 4.79\n",
      "batch: 457, batch train loss: 0.06, train acc: 97.89%, consuming tine: 4.98\n",
      "batch: 458, batch train loss: 0.06, train acc: 97.89%, consuming tine: 4.98\n",
      "batch: 459, batch train loss: 0.06, train acc: 97.89%, consuming tine: 5.03\n",
      "batch: 460, batch train loss: 0.06, train acc: 97.89%, consuming tine: 5.36\n",
      "batch: 461, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.00\n",
      "batch: 462, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.00\n",
      "batch: 463, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.47\n",
      "batch: 464, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.08\n",
      "batch: 465, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.63\n",
      "batch: 466, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.13\n",
      "batch: 467, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.12\n",
      "batch: 468, batch train loss: 0.06, train acc: 97.90%, consuming tine: 5.07\n",
      "batch: 469, batch train loss: 0.06, train acc: 97.90%, consuming tine: 4.71\n",
      "batch: 470, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.17\n",
      "batch: 471, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.11\n",
      "batch: 472, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.06\n",
      "batch: 473, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.02\n",
      "batch: 474, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.17\n",
      "batch: 475, batch train loss: 0.06, train acc: 97.91%, consuming tine: 5.29\n",
      "batch: 476, batch train loss: 0.06, train acc: 97.92%, consuming tine: 5.07\n",
      "batch: 477, batch train loss: 0.06, train acc: 97.92%, consuming tine: 5.39\n",
      "batch: 478, batch train loss: 0.06, train acc: 97.92%, consuming tine: 4.88\n",
      "batch: 479, batch train loss: 0.06, train acc: 97.92%, consuming tine: 4.99\n",
      "batch: 480, batch train loss: 0.06, train acc: 97.92%, consuming tine: 5.17\n",
      "batch: 481, batch train loss: 0.06, train acc: 97.93%, consuming tine: 5.02\n",
      "batch: 482, batch train loss: 0.06, train acc: 97.93%, consuming tine: 5.46\n",
      "batch: 483, batch train loss: 0.06, train acc: 97.93%, consuming tine: 5.00\n",
      "batch: 484, batch train loss: 0.06, train acc: 97.93%, consuming tine: 5.51\n",
      "batch: 485, batch train loss: 0.06, train acc: 97.93%, consuming tine: 5.67\n",
      "batch: 486, batch train loss: 0.06, train acc: 97.94%, consuming tine: 5.01\n",
      "batch: 487, batch train loss: 0.06, train acc: 97.94%, consuming tine: 4.96\n",
      "batch: 488, batch train loss: 0.06, train acc: 97.94%, consuming tine: 4.99\n",
      "batch: 489, batch train loss: 0.06, train acc: 97.94%, consuming tine: 5.50\n",
      "batch: 490, batch train loss: 0.06, train acc: 97.94%, consuming tine: 5.10\n",
      "batch: 491, batch train loss: 0.06, train acc: 97.94%, consuming tine: 4.96\n",
      "batch: 492, batch train loss: 0.06, train acc: 97.95%, consuming tine: 4.92\n",
      "batch: 493, batch train loss: 0.06, train acc: 97.95%, consuming tine: 4.96\n",
      "batch: 494, batch train loss: 0.06, train acc: 97.95%, consuming tine: 5.99\n",
      "batch: 495, batch train loss: 0.06, train acc: 97.95%, consuming tine: 4.79\n",
      "batch: 496, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.40\n",
      "batch: 497, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.11\n",
      "batch: 498, batch train loss: 0.06, train acc: 97.96%, consuming tine: 4.90\n",
      "batch: 499, batch train loss: 0.06, train acc: 97.96%, consuming tine: 4.96\n",
      "batch: 500, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.18\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 4.30, valid acc: 35.39%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.36\n",
      "batch: 502, batch train loss: 0.06, train acc: 97.96%, consuming tine: 4.80\n",
      "batch: 503, batch train loss: 0.06, train acc: 97.96%, consuming tine: 5.40\n",
      "batch: 504, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.07\n",
      "batch: 505, batch train loss: 0.06, train acc: 97.97%, consuming tine: 4.98\n",
      "batch: 506, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.40\n",
      "batch: 507, batch train loss: 0.06, train acc: 97.97%, consuming tine: 4.96\n",
      "batch: 508, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.29\n",
      "batch: 509, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.40\n",
      "batch: 510, batch train loss: 0.06, train acc: 97.97%, consuming tine: 5.29\n",
      "batch: 511, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.39\n",
      "batch: 512, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.98\n",
      "batch: 513, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.30\n",
      "batch: 514, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.28\n",
      "batch: 515, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.21\n",
      "batch: 516, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.39\n",
      "batch: 517, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.29\n",
      "batch: 518, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.09\n",
      "batch: 519, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.18\n",
      "batch: 520, batch train loss: 0.06, train acc: 97.98%, consuming tine: 5.18\n",
      "batch: 521, batch train loss: 0.06, train acc: 97.98%, consuming tine: 4.88\n",
      "batch: 522, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.17\n",
      "batch: 523, batch train loss: 0.06, train acc: 97.99%, consuming tine: 4.91\n",
      "batch: 524, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.29\n",
      "batch: 525, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.20\n",
      "batch: 526, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.28\n",
      "batch: 527, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.26\n",
      "batch: 528, batch train loss: 0.06, train acc: 97.99%, consuming tine: 5.12\n",
      "batch: 529, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.09\n",
      "batch: 530, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.39\n",
      "batch: 531, batch train loss: 0.06, train acc: 98.00%, consuming tine: 4.99\n",
      "batch: 532, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.07\n",
      "batch: 533, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.01\n",
      "batch: 534, batch train loss: 0.06, train acc: 98.00%, consuming tine: 4.58\n",
      "batch: 535, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.16\n",
      "batch: 536, batch train loss: 0.06, train acc: 98.00%, consuming tine: 5.39\n",
      "batch: 537, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.29\n",
      "batch: 538, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.01\n",
      "batch: 539, batch train loss: 0.06, train acc: 98.01%, consuming tine: 4.98\n",
      "batch: 540, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.39\n",
      "batch: 541, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.00\n",
      "batch: 542, batch train loss: 0.06, train acc: 98.01%, consuming tine: 4.99\n",
      "batch: 543, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.38\n",
      "batch: 544, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.01\n",
      "batch: 545, batch train loss: 0.06, train acc: 98.01%, consuming tine: 4.79\n",
      "batch: 546, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.30\n",
      "batch: 547, batch train loss: 0.06, train acc: 98.01%, consuming tine: 5.25\n",
      "batch: 548, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.08\n",
      "batch: 549, batch train loss: 0.06, train acc: 98.02%, consuming tine: 4.99\n",
      "batch: 550, batch train loss: 0.06, train acc: 98.02%, consuming tine: 4.71\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 4.33, valid acc: 35.34%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.22\n",
      "batch: 552, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.21\n",
      "batch: 553, batch train loss: 0.06, train acc: 98.02%, consuming tine: 5.16\n",
      "batch: 554, batch train loss: 0.06, train acc: 98.03%, consuming tine: 4.79\n",
      "batch: 555, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.51\n",
      "batch: 556, batch train loss: 0.06, train acc: 98.03%, consuming tine: 4.97\n",
      "batch: 557, batch train loss: 0.06, train acc: 98.03%, consuming tine: 4.87\n",
      "batch: 558, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 559, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.09\n",
      "batch: 560, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.18\n",
      "batch: 561, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.20\n",
      "batch: 562, batch train loss: 0.06, train acc: 98.03%, consuming tine: 4.78\n",
      "batch: 563, batch train loss: 0.06, train acc: 98.03%, consuming tine: 5.69\n",
      "batch: 564, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.19\n",
      "batch: 565, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.20\n",
      "batch: 566, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.08\n",
      "batch: 567, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.89\n",
      "batch: 568, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.30\n",
      "batch: 569, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.07\n",
      "batch: 570, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.40\n",
      "batch: 571, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.60\n",
      "batch: 572, batch train loss: 0.06, train acc: 98.04%, consuming tine: 4.89\n",
      "batch: 573, batch train loss: 0.06, train acc: 98.04%, consuming tine: 5.42\n",
      "batch: 574, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.38\n",
      "batch: 575, batch train loss: 0.06, train acc: 98.05%, consuming tine: 4.87\n",
      "batch: 576, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.59\n",
      "batch: 577, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.07\n",
      "batch: 578, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.19\n",
      "batch: 579, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.08\n",
      "batch: 580, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.41\n",
      "batch: 581, batch train loss: 0.06, train acc: 98.05%, consuming tine: 5.21\n",
      "batch: 582, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.16\n",
      "batch: 583, batch train loss: 0.06, train acc: 98.06%, consuming tine: 4.88\n",
      "batch: 584, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.08\n",
      "batch: 585, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.38\n",
      "batch: 586, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.43\n",
      "batch: 587, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.09\n",
      "batch: 588, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.16\n",
      "batch: 589, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.04\n",
      "batch: 590, batch train loss: 0.06, train acc: 98.06%, consuming tine: 5.05\n",
      "batch: 591, batch train loss: 0.06, train acc: 98.06%, consuming tine: 4.96\n",
      "batch: 592, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.30\n",
      "batch: 593, batch train loss: 0.06, train acc: 98.07%, consuming tine: 4.92\n",
      "batch: 594, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.35\n",
      "batch: 595, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.49\n",
      "batch: 596, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.27\n",
      "batch: 597, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.23\n",
      "batch: 598, batch train loss: 0.06, train acc: 98.07%, consuming tine: 4.78\n",
      "batch: 599, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.17\n",
      "batch: 600, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.40\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 4.37, valid acc: 35.30%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.02\n",
      "batch: 602, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.38\n",
      "batch: 603, batch train loss: 0.06, train acc: 98.07%, consuming tine: 5.28\n",
      "batch: 604, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.32\n",
      "batch: 605, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.44\n",
      "batch: 606, batch train loss: 0.06, train acc: 98.08%, consuming tine: 4.71\n",
      "batch: 607, batch train loss: 0.06, train acc: 98.08%, consuming tine: 4.69\n",
      "batch: 608, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.79\n",
      "batch: 609, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.01\n",
      "batch: 610, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.27\n",
      "batch: 611, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.20\n",
      "batch: 612, batch train loss: 0.06, train acc: 98.08%, consuming tine: 4.79\n",
      "batch: 613, batch train loss: 0.06, train acc: 98.08%, consuming tine: 4.98\n",
      "batch: 614, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.26\n",
      "batch: 615, batch train loss: 0.06, train acc: 98.08%, consuming tine: 5.22\n",
      "batch: 616, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.17\n",
      "batch: 617, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.80\n",
      "batch: 618, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.28\n",
      "batch: 619, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.27\n",
      "batch: 620, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.19\n",
      "batch: 621, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.82\n",
      "batch: 622, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.85\n",
      "batch: 623, batch train loss: 0.06, train acc: 98.09%, consuming tine: 4.70\n",
      "batch: 624, batch train loss: 0.06, train acc: 98.09%, consuming tine: 5.08\n",
      "batch: 625, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.01\n",
      "batch: 626, batch train loss: 0.06, train acc: 98.10%, consuming tine: 4.26\n",
      "batch: 627, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.09\n",
      "batch: 628, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.08\n",
      "batch: 629, batch train loss: 0.06, train acc: 98.10%, consuming tine: 4.60\n",
      "batch: 630, batch train loss: 0.06, train acc: 98.10%, consuming tine: 5.58\n",
      "batch: 631, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.39\n",
      "batch: 632, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.27\n",
      "batch: 633, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.29\n",
      "batch: 634, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.92\n",
      "batch: 635, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.27\n",
      "batch: 636, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.48\n",
      "batch: 637, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.28\n",
      "batch: 638, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.29\n",
      "batch: 639, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.89\n",
      "batch: 640, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.28\n",
      "batch: 641, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.89\n",
      "batch: 642, batch train loss: 0.06, train acc: 98.11%, consuming tine: 4.98\n",
      "batch: 643, batch train loss: 0.06, train acc: 98.11%, consuming tine: 5.06\n",
      "batch: 644, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.31\n",
      "batch: 645, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.38\n",
      "batch: 646, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.23\n",
      "batch: 647, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.29\n",
      "batch: 648, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.27\n",
      "batch: 649, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.09\n",
      "batch: 650, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.48\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 4.39, valid acc: 35.25%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.43\n",
      "batch: 652, batch train loss: 0.06, train acc: 98.12%, consuming tine: 5.16\n",
      "batch: 653, batch train loss: 0.06, train acc: 98.12%, consuming tine: 4.91\n",
      "batch: 654, batch train loss: 0.06, train acc: 98.13%, consuming tine: 4.98\n",
      "batch: 655, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.18\n",
      "batch: 656, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.43\n",
      "batch: 657, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.06\n",
      "batch: 658, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.38\n",
      "batch: 659, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.42\n",
      "batch: 660, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.24\n",
      "batch: 661, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.71\n",
      "batch: 662, batch train loss: 0.06, train acc: 98.13%, consuming tine: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 663, batch train loss: 0.06, train acc: 98.13%, consuming tine: 4.89\n",
      "batch: 664, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.27\n",
      "batch: 665, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.30\n",
      "batch: 666, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.29\n",
      "batch: 667, batch train loss: 0.06, train acc: 98.13%, consuming tine: 5.07\n",
      "batch: 668, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.00\n",
      "batch: 669, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.39\n",
      "batch: 670, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.02\n",
      "batch: 671, batch train loss: 0.06, train acc: 98.14%, consuming tine: 4.96\n",
      "batch: 672, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.10\n",
      "batch: 673, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.25\n",
      "batch: 674, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.60\n",
      "batch: 675, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.08\n",
      "batch: 676, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.29\n",
      "batch: 677, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.48\n",
      "batch: 678, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.52\n",
      "batch: 679, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.57\n",
      "batch: 680, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.11\n",
      "batch: 681, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.17\n",
      "batch: 682, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.18\n",
      "batch: 683, batch train loss: 0.06, train acc: 98.14%, consuming tine: 4.80\n",
      "batch: 684, batch train loss: 0.06, train acc: 98.14%, consuming tine: 5.30\n",
      "batch: 685, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.19\n",
      "batch: 686, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.28\n",
      "batch: 687, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.28\n",
      "batch: 688, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.00\n",
      "batch: 689, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.20\n",
      "batch: 690, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.18\n",
      "batch: 691, batch train loss: 0.06, train acc: 98.15%, consuming tine: 4.68\n",
      "batch: 692, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.44\n",
      "batch: 693, batch train loss: 0.06, train acc: 98.15%, consuming tine: 4.92\n",
      "batch: 694, batch train loss: 0.06, train acc: 98.15%, consuming tine: 4.78\n",
      "batch: 695, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.29\n",
      "batch: 696, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.28\n",
      "batch: 697, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.60\n",
      "batch: 698, batch train loss: 0.06, train acc: 98.15%, consuming tine: 4.98\n",
      "batch: 699, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.29\n",
      "batch: 700, batch train loss: 0.06, train acc: 98.15%, consuming tine: 5.39\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 4.41, valid acc: 35.21%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.06, train acc: 98.16%, consuming tine: 5.10\n",
      "batch: 702, batch train loss: 0.06, train acc: 98.16%, consuming tine: 5.67\n",
      "batch: 703, batch train loss: 0.06, train acc: 98.16%, consuming tine: 5.00\n",
      "batch: 704, batch train loss: 0.06, train acc: 98.16%, consuming tine: 4.89\n",
      "Epoch 7, Loss: 0.06, Accuracy: 98.16%, Valid Loss: 4.41, Valid Accuracy: 35.21%\n",
      "batch: 1, batch train loss: 0.03, train acc: 98.73%, consuming tine: 5.63\n",
      "batch: 2, batch train loss: 0.04, train acc: 98.49%, consuming tine: 4.97\n",
      "batch: 3, batch train loss: 0.04, train acc: 98.44%, consuming tine: 5.30\n",
      "batch: 4, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.46\n",
      "batch: 5, batch train loss: 0.04, train acc: 98.57%, consuming tine: 4.90\n",
      "batch: 6, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.40\n",
      "batch: 7, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.07\n",
      "batch: 8, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.50\n",
      "batch: 9, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.30\n",
      "batch: 10, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.78\n",
      "batch: 11, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.78\n",
      "batch: 12, batch train loss: 0.04, train acc: 98.62%, consuming tine: 4.88\n",
      "batch: 13, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.09\n",
      "batch: 14, batch train loss: 0.04, train acc: 98.68%, consuming tine: 4.88\n",
      "batch: 15, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.62\n",
      "batch: 16, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.05\n",
      "batch: 17, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.80\n",
      "batch: 18, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.71\n",
      "batch: 19, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.17\n",
      "batch: 20, batch train loss: 0.04, train acc: 98.78%, consuming tine: 4.98\n",
      "batch: 21, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.20\n",
      "batch: 22, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.08\n",
      "batch: 23, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.92\n",
      "batch: 24, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.48\n",
      "batch: 25, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.67\n",
      "batch: 26, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.20\n",
      "batch: 27, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.14\n",
      "batch: 28, batch train loss: 0.04, train acc: 98.77%, consuming tine: 4.80\n",
      "batch: 29, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.99\n",
      "batch: 30, batch train loss: 0.04, train acc: 98.78%, consuming tine: 4.61\n",
      "batch: 31, batch train loss: 0.04, train acc: 98.77%, consuming tine: 4.88\n",
      "batch: 32, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.46\n",
      "batch: 33, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.91\n",
      "batch: 34, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.56\n",
      "batch: 35, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.01\n",
      "batch: 36, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.19\n",
      "batch: 37, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.19\n",
      "batch: 38, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.47\n",
      "batch: 39, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.81\n",
      "batch: 40, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.88\n",
      "batch: 41, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.18\n",
      "batch: 42, batch train loss: 0.04, train acc: 98.80%, consuming tine: 4.93\n",
      "batch: 43, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.25\n",
      "batch: 44, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.59\n",
      "batch: 45, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.20\n",
      "batch: 46, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.38\n",
      "batch: 47, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.69\n",
      "batch: 48, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.17\n",
      "batch: 49, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.00\n",
      "batch: 50, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.18\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 4.60, valid acc: 34.62%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.09\n",
      "batch: 52, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.13\n",
      "batch: 53, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.99\n",
      "batch: 54, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.15\n",
      "batch: 55, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.54\n",
      "batch: 56, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.97\n",
      "batch: 57, batch train loss: 0.04, train acc: 98.80%, consuming tine: 4.98\n",
      "batch: 58, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.12\n",
      "batch: 59, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.98\n",
      "batch: 60, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.34\n",
      "batch: 61, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.14\n",
      "batch: 62, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 63, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.70\n",
      "batch: 64, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.37\n",
      "batch: 65, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.17\n",
      "batch: 66, batch train loss: 0.04, train acc: 98.80%, consuming tine: 4.80\n",
      "batch: 67, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.38\n",
      "batch: 68, batch train loss: 0.04, train acc: 98.80%, consuming tine: 4.88\n",
      "batch: 69, batch train loss: 0.04, train acc: 98.80%, consuming tine: 6.81\n",
      "batch: 70, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.99\n",
      "batch: 71, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.80\n",
      "batch: 72, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.86\n",
      "batch: 73, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.87\n",
      "batch: 74, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.99\n",
      "batch: 75, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.99\n",
      "batch: 76, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.09\n",
      "batch: 77, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.88\n",
      "batch: 78, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.73\n",
      "batch: 79, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.06\n",
      "batch: 80, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.39\n",
      "batch: 81, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.49\n",
      "batch: 82, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.68\n",
      "batch: 83, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.91\n",
      "batch: 84, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.07\n",
      "batch: 85, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.09\n",
      "batch: 86, batch train loss: 0.04, train acc: 98.83%, consuming tine: 4.91\n",
      "batch: 87, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.38\n",
      "batch: 88, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.07\n",
      "batch: 89, batch train loss: 0.04, train acc: 98.83%, consuming tine: 4.80\n",
      "batch: 90, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.97\n",
      "batch: 91, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.51\n",
      "batch: 92, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.89\n",
      "batch: 93, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.88\n",
      "batch: 94, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.99\n",
      "batch: 95, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.98\n",
      "batch: 96, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.11\n",
      "batch: 97, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.39\n",
      "batch: 98, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.28\n",
      "batch: 99, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.21\n",
      "batch: 100, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.07\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 4.68, valid acc: 34.97%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.26\n",
      "batch: 102, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.67\n",
      "batch: 103, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.25\n",
      "batch: 104, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.53\n",
      "batch: 105, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.15\n",
      "batch: 106, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.00\n",
      "batch: 107, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.88\n",
      "batch: 108, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.28\n",
      "batch: 109, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.09\n",
      "batch: 110, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.99\n",
      "batch: 111, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.87\n",
      "batch: 112, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.17\n",
      "batch: 113, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.18\n",
      "batch: 114, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.99\n",
      "batch: 115, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.50\n",
      "batch: 116, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.07\n",
      "batch: 117, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.91\n",
      "batch: 118, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.17\n",
      "batch: 119, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.91\n",
      "batch: 120, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.29\n",
      "batch: 121, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.27\n",
      "batch: 122, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.60\n",
      "batch: 123, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.08\n",
      "batch: 124, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.00\n",
      "batch: 125, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.28\n",
      "batch: 126, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.17\n",
      "batch: 127, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.23\n",
      "batch: 128, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.47\n",
      "batch: 129, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.78\n",
      "batch: 130, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.18\n",
      "batch: 131, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.88\n",
      "batch: 132, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.58\n",
      "batch: 133, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.80\n",
      "batch: 134, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.18\n",
      "batch: 135, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.47\n",
      "batch: 136, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.09\n",
      "batch: 137, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.29\n",
      "batch: 138, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.91\n",
      "batch: 139, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.08\n",
      "batch: 140, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.26\n",
      "batch: 141, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.91\n",
      "batch: 142, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.38\n",
      "batch: 143, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.09\n",
      "batch: 144, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.97\n",
      "batch: 145, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.29\n",
      "batch: 146, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.79\n",
      "batch: 147, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.20\n",
      "batch: 148, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.26\n",
      "batch: 149, batch train loss: 0.04, train acc: 98.88%, consuming tine: 4.69\n",
      "batch: 150, batch train loss: 0.04, train acc: 98.88%, consuming tine: 5.05\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 4.69, valid acc: 35.02%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.04, train acc: 98.88%, consuming tine: 4.72\n",
      "batch: 152, batch train loss: 0.04, train acc: 98.88%, consuming tine: 5.27\n",
      "batch: 153, batch train loss: 0.04, train acc: 98.88%, consuming tine: 5.30\n",
      "batch: 154, batch train loss: 0.04, train acc: 98.88%, consuming tine: 5.00\n",
      "batch: 155, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.79\n",
      "batch: 156, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.08\n",
      "batch: 157, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.90\n",
      "batch: 158, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.18\n",
      "batch: 159, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.88\n",
      "batch: 160, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.90\n",
      "batch: 161, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.68\n",
      "batch: 162, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.20\n",
      "batch: 163, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.17\n",
      "batch: 164, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.21\n",
      "batch: 165, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.28\n",
      "batch: 166, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.98\n",
      "batch: 167, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 168, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.11\n",
      "batch: 169, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.26\n",
      "batch: 170, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.99\n",
      "batch: 171, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.49\n",
      "batch: 172, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.20\n",
      "batch: 173, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.35\n",
      "batch: 174, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.33\n",
      "batch: 175, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.28\n",
      "batch: 176, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.50\n",
      "batch: 177, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.09\n",
      "batch: 178, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.25\n",
      "batch: 179, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.21\n",
      "batch: 180, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.07\n",
      "batch: 181, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.39\n",
      "batch: 182, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.99\n",
      "batch: 183, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.98\n",
      "batch: 184, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.30\n",
      "batch: 185, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.38\n",
      "batch: 186, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.59\n",
      "batch: 187, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.98\n",
      "batch: 188, batch train loss: 0.03, train acc: 98.89%, consuming tine: 4.88\n",
      "batch: 189, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.46\n",
      "batch: 190, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.70\n",
      "batch: 191, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.14\n",
      "batch: 192, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.06\n",
      "batch: 193, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.25\n",
      "batch: 194, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.20\n",
      "batch: 195, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.81\n",
      "batch: 196, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.99\n",
      "batch: 197, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.86\n",
      "batch: 198, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.77\n",
      "batch: 199, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.91\n",
      "batch: 200, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.19\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 4.70, valid acc: 35.18%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.99\n",
      "batch: 202, batch train loss: 0.03, train acc: 98.88%, consuming tine: 5.10\n",
      "batch: 203, batch train loss: 0.03, train acc: 98.88%, consuming tine: 4.97\n",
      "batch: 204, batch train loss: 0.03, train acc: 98.87%, consuming tine: 4.98\n",
      "batch: 205, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.11\n",
      "batch: 206, batch train loss: 0.04, train acc: 98.87%, consuming tine: 4.88\n",
      "batch: 207, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.07\n",
      "batch: 208, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.29\n",
      "batch: 209, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.77\n",
      "batch: 210, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.28\n",
      "batch: 211, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.94\n",
      "batch: 212, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.75\n",
      "batch: 213, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.22\n",
      "batch: 214, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.24\n",
      "batch: 215, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.00\n",
      "batch: 216, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.79\n",
      "batch: 217, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.97\n",
      "batch: 218, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.89\n",
      "batch: 219, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.18\n",
      "batch: 220, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.58\n",
      "batch: 221, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.91\n",
      "batch: 222, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.88\n",
      "batch: 223, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.47\n",
      "batch: 224, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.01\n",
      "batch: 225, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.27\n",
      "batch: 226, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.38\n",
      "batch: 227, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.32\n",
      "batch: 228, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.26\n",
      "batch: 229, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.49\n",
      "batch: 230, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.13\n",
      "batch: 231, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.83\n",
      "batch: 232, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.19\n",
      "batch: 233, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.09\n",
      "batch: 234, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.08\n",
      "batch: 235, batch train loss: 0.04, train acc: 98.77%, consuming tine: 4.78\n",
      "batch: 236, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.02\n",
      "batch: 237, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.27\n",
      "batch: 238, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.79\n",
      "batch: 239, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.17\n",
      "batch: 240, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.29\n",
      "batch: 241, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.88\n",
      "batch: 242, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.16\n",
      "batch: 243, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.52\n",
      "batch: 244, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.32\n",
      "batch: 245, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.78\n",
      "batch: 246, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.06\n",
      "batch: 247, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.25\n",
      "batch: 248, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.03\n",
      "batch: 249, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.10\n",
      "batch: 250, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.77\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 4.67, valid acc: 35.21%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.28\n",
      "batch: 252, batch train loss: 0.04, train acc: 98.72%, consuming tine: 4.90\n",
      "batch: 253, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.90\n",
      "batch: 254, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.10\n",
      "batch: 255, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.36\n",
      "batch: 256, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.09\n",
      "batch: 257, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.79\n",
      "batch: 258, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.08\n",
      "batch: 259, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.08\n",
      "batch: 260, batch train loss: 0.04, train acc: 98.69%, consuming tine: 4.59\n",
      "batch: 261, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.40\n",
      "batch: 262, batch train loss: 0.04, train acc: 98.69%, consuming tine: 4.88\n",
      "batch: 263, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.51\n",
      "batch: 264, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.07\n",
      "batch: 265, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.17\n",
      "batch: 266, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.37\n",
      "batch: 267, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.21\n",
      "batch: 268, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.88\n",
      "batch: 269, batch train loss: 0.04, train acc: 98.66%, consuming tine: 4.98\n",
      "batch: 270, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.78\n",
      "batch: 271, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 272, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.18\n",
      "batch: 273, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.09\n",
      "batch: 274, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.17\n",
      "batch: 275, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.60\n",
      "batch: 276, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.08\n",
      "batch: 277, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.91\n",
      "batch: 278, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.26\n",
      "batch: 279, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.97\n",
      "batch: 280, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.96\n",
      "batch: 281, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.40\n",
      "batch: 282, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.17\n",
      "batch: 283, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.39\n",
      "batch: 284, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.99\n",
      "batch: 285, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.09\n",
      "batch: 286, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.28\n",
      "batch: 287, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.70\n",
      "batch: 288, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.39\n",
      "batch: 289, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.90\n",
      "batch: 290, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.17\n",
      "batch: 291, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.89\n",
      "batch: 292, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.78\n",
      "batch: 293, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.30\n",
      "batch: 294, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.88\n",
      "batch: 295, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.30\n",
      "batch: 296, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.17\n",
      "batch: 297, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.93\n",
      "batch: 298, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.96\n",
      "batch: 299, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.00\n",
      "batch: 300, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.18\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 4.62, valid acc: 35.25%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.22\n",
      "batch: 302, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.99\n",
      "batch: 303, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.89\n",
      "batch: 304, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.19\n",
      "batch: 305, batch train loss: 0.04, train acc: 98.63%, consuming tine: 4.85\n",
      "batch: 306, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.22\n",
      "batch: 307, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.29\n",
      "batch: 308, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.07\n",
      "batch: 309, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.29\n",
      "batch: 310, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.30\n",
      "batch: 311, batch train loss: 0.04, train acc: 98.63%, consuming tine: 5.19\n",
      "batch: 312, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.60\n",
      "batch: 313, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.17\n",
      "batch: 314, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.79\n",
      "batch: 315, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.41\n",
      "batch: 316, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.17\n",
      "batch: 317, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.29\n",
      "batch: 318, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.00\n",
      "batch: 319, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.21\n",
      "batch: 320, batch train loss: 0.04, train acc: 98.64%, consuming tine: 4.87\n",
      "batch: 321, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.08\n",
      "batch: 322, batch train loss: 0.04, train acc: 98.64%, consuming tine: 5.37\n",
      "batch: 323, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.00\n",
      "batch: 324, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.20\n",
      "batch: 325, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.28\n",
      "batch: 326, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.61\n",
      "batch: 327, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.30\n",
      "batch: 328, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.39\n",
      "batch: 329, batch train loss: 0.04, train acc: 98.65%, consuming tine: 4.86\n",
      "batch: 330, batch train loss: 0.04, train acc: 98.65%, consuming tine: 5.06\n",
      "batch: 331, batch train loss: 0.04, train acc: 98.66%, consuming tine: 4.92\n",
      "batch: 332, batch train loss: 0.04, train acc: 98.66%, consuming tine: 4.89\n",
      "batch: 333, batch train loss: 0.04, train acc: 98.66%, consuming tine: 4.77\n",
      "batch: 334, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.08\n",
      "batch: 335, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.32\n",
      "batch: 336, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.08\n",
      "batch: 337, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.27\n",
      "batch: 338, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.19\n",
      "batch: 339, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.21\n",
      "batch: 340, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.45\n",
      "batch: 341, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.21\n",
      "batch: 342, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.50\n",
      "batch: 343, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.26\n",
      "batch: 344, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.42\n",
      "batch: 345, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.16\n",
      "batch: 346, batch train loss: 0.04, train acc: 98.66%, consuming tine: 5.31\n",
      "batch: 347, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.89\n",
      "batch: 348, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.68\n",
      "batch: 349, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.37\n",
      "batch: 350, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.18\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 4.63, valid acc: 35.24%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.27\n",
      "batch: 352, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.10\n",
      "batch: 353, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.78\n",
      "batch: 354, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.09\n",
      "batch: 355, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.49\n",
      "batch: 356, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.89\n",
      "batch: 357, batch train loss: 0.04, train acc: 98.67%, consuming tine: 4.94\n",
      "batch: 358, batch train loss: 0.04, train acc: 98.67%, consuming tine: 5.34\n",
      "batch: 359, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.26\n",
      "batch: 360, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.20\n",
      "batch: 361, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.28\n",
      "batch: 362, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.22\n",
      "batch: 363, batch train loss: 0.04, train acc: 98.68%, consuming tine: 4.88\n",
      "batch: 364, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.26\n",
      "batch: 365, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.02\n",
      "batch: 366, batch train loss: 0.04, train acc: 98.68%, consuming tine: 4.79\n",
      "batch: 367, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.08\n",
      "batch: 368, batch train loss: 0.04, train acc: 98.68%, consuming tine: 5.39\n",
      "batch: 369, batch train loss: 0.04, train acc: 98.68%, consuming tine: 4.97\n",
      "batch: 370, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.02\n",
      "batch: 371, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.16\n",
      "batch: 372, batch train loss: 0.04, train acc: 98.69%, consuming tine: 4.61\n",
      "batch: 373, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.19\n",
      "batch: 374, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.30\n",
      "batch: 375, batch train loss: 0.04, train acc: 98.69%, consuming tine: 4.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 376, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.20\n",
      "batch: 377, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.12\n",
      "batch: 378, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.18\n",
      "batch: 379, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.10\n",
      "batch: 380, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.08\n",
      "batch: 381, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.39\n",
      "batch: 382, batch train loss: 0.04, train acc: 98.69%, consuming tine: 5.08\n",
      "batch: 383, batch train loss: 0.04, train acc: 98.69%, consuming tine: 4.93\n",
      "batch: 384, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.55\n",
      "batch: 385, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.89\n",
      "batch: 386, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.90\n",
      "batch: 387, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.98\n",
      "batch: 388, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.88\n",
      "batch: 389, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.77\n",
      "batch: 390, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.30\n",
      "batch: 391, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.51\n",
      "batch: 392, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.27\n",
      "batch: 393, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.92\n",
      "batch: 394, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.36\n",
      "batch: 395, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.89\n",
      "batch: 396, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.50\n",
      "batch: 397, batch train loss: 0.04, train acc: 98.70%, consuming tine: 4.99\n",
      "batch: 398, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.57\n",
      "batch: 399, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.10\n",
      "batch: 400, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.50\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 4.65, valid acc: 35.20%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.04, train acc: 98.70%, consuming tine: 5.06\n",
      "batch: 402, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.76\n",
      "batch: 403, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.08\n",
      "batch: 404, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.95\n",
      "batch: 405, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.49\n",
      "batch: 406, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.91\n",
      "batch: 407, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.38\n",
      "batch: 408, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.89\n",
      "batch: 409, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.08\n",
      "batch: 410, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.97\n",
      "batch: 411, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.02\n",
      "batch: 412, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.97\n",
      "batch: 413, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.88\n",
      "batch: 414, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.99\n",
      "batch: 415, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.30\n",
      "batch: 416, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.87\n",
      "batch: 417, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.40\n",
      "batch: 418, batch train loss: 0.04, train acc: 98.71%, consuming tine: 5.08\n",
      "batch: 419, batch train loss: 0.04, train acc: 98.71%, consuming tine: 4.59\n",
      "batch: 420, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.30\n",
      "batch: 421, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.39\n",
      "batch: 422, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.18\n",
      "batch: 423, batch train loss: 0.04, train acc: 98.72%, consuming tine: 4.77\n",
      "batch: 424, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.45\n",
      "batch: 425, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.54\n",
      "batch: 426, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.18\n",
      "batch: 427, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.71\n",
      "batch: 428, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.18\n",
      "batch: 429, batch train loss: 0.04, train acc: 98.72%, consuming tine: 5.10\n",
      "batch: 430, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.97\n",
      "batch: 431, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.00\n",
      "batch: 432, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.26\n",
      "batch: 433, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.00\n",
      "batch: 434, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.29\n",
      "batch: 435, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.62\n",
      "batch: 436, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.17\n",
      "batch: 437, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.98\n",
      "batch: 438, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.99\n",
      "batch: 439, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.88\n",
      "batch: 440, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.89\n",
      "batch: 441, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.59\n",
      "batch: 442, batch train loss: 0.04, train acc: 98.73%, consuming tine: 5.15\n",
      "batch: 443, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.59\n",
      "batch: 444, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.20\n",
      "batch: 445, batch train loss: 0.04, train acc: 98.74%, consuming tine: 4.57\n",
      "batch: 446, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.00\n",
      "batch: 447, batch train loss: 0.04, train acc: 98.74%, consuming tine: 4.99\n",
      "batch: 448, batch train loss: 0.04, train acc: 98.74%, consuming tine: 4.49\n",
      "batch: 449, batch train loss: 0.04, train acc: 98.74%, consuming tine: 6.47\n",
      "batch: 450, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.28\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 4.68, valid acc: 35.17%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.42\n",
      "batch: 452, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.47\n",
      "batch: 453, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.21\n",
      "batch: 454, batch train loss: 0.04, train acc: 98.74%, consuming tine: 5.08\n",
      "batch: 455, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.19\n",
      "batch: 456, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.18\n",
      "batch: 457, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.01\n",
      "batch: 458, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.03\n",
      "batch: 459, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.42\n",
      "batch: 460, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.04\n",
      "batch: 461, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.45\n",
      "batch: 462, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.40\n",
      "batch: 463, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.06\n",
      "batch: 464, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.34\n",
      "batch: 465, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.91\n",
      "batch: 466, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.28\n",
      "batch: 467, batch train loss: 0.04, train acc: 98.75%, consuming tine: 5.28\n",
      "batch: 468, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.80\n",
      "batch: 469, batch train loss: 0.04, train acc: 98.75%, consuming tine: 4.90\n",
      "batch: 470, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.67\n",
      "batch: 471, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.10\n",
      "batch: 472, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.77\n",
      "batch: 473, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.09\n",
      "batch: 474, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.36\n",
      "batch: 475, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.91\n",
      "batch: 476, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.00\n",
      "batch: 477, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.30\n",
      "batch: 478, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.57\n",
      "batch: 479, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 480, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.59\n",
      "batch: 481, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.30\n",
      "batch: 482, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.20\n",
      "batch: 483, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.18\n",
      "batch: 484, batch train loss: 0.04, train acc: 98.76%, consuming tine: 4.87\n",
      "batch: 485, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.29\n",
      "batch: 486, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.18\n",
      "batch: 487, batch train loss: 0.04, train acc: 98.76%, consuming tine: 5.10\n",
      "batch: 488, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.19\n",
      "batch: 489, batch train loss: 0.04, train acc: 98.77%, consuming tine: 4.98\n",
      "batch: 490, batch train loss: 0.04, train acc: 98.77%, consuming tine: 4.99\n",
      "batch: 491, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.10\n",
      "batch: 492, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.18\n",
      "batch: 493, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.31\n",
      "batch: 494, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.18\n",
      "batch: 495, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.00\n",
      "batch: 496, batch train loss: 0.04, train acc: 98.77%, consuming tine: 5.49\n",
      "batch: 497, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.07\n",
      "batch: 498, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.39\n",
      "batch: 499, batch train loss: 0.04, train acc: 98.78%, consuming tine: 4.99\n",
      "batch: 500, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.28\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 4.71, valid acc: 35.20%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.19\n",
      "batch: 502, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.06\n",
      "batch: 503, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.29\n",
      "batch: 504, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.08\n",
      "batch: 505, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.17\n",
      "batch: 506, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.21\n",
      "batch: 507, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.49\n",
      "batch: 508, batch train loss: 0.04, train acc: 98.78%, consuming tine: 4.90\n",
      "batch: 509, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.37\n",
      "batch: 510, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.28\n",
      "batch: 511, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.20\n",
      "batch: 512, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.63\n",
      "batch: 513, batch train loss: 0.04, train acc: 98.78%, consuming tine: 5.14\n",
      "batch: 514, batch train loss: 0.04, train acc: 98.78%, consuming tine: 4.79\n",
      "batch: 515, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.59\n",
      "batch: 516, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.38\n",
      "batch: 517, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.40\n",
      "batch: 518, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.19\n",
      "batch: 519, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.68\n",
      "batch: 520, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.29\n",
      "batch: 521, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.30\n",
      "batch: 522, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.98\n",
      "batch: 523, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.39\n",
      "batch: 524, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.38\n",
      "batch: 525, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.20\n",
      "batch: 526, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.07\n",
      "batch: 527, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.19\n",
      "batch: 528, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.29\n",
      "batch: 529, batch train loss: 0.04, train acc: 98.79%, consuming tine: 5.21\n",
      "batch: 530, batch train loss: 0.04, train acc: 98.79%, consuming tine: 4.96\n",
      "batch: 531, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.11\n",
      "batch: 532, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.08\n",
      "batch: 533, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.41\n",
      "batch: 534, batch train loss: 0.04, train acc: 98.80%, consuming tine: 4.97\n",
      "batch: 535, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.28\n",
      "batch: 536, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.38\n",
      "batch: 537, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.08\n",
      "batch: 538, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.60\n",
      "batch: 539, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.16\n",
      "batch: 540, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.10\n",
      "batch: 541, batch train loss: 0.04, train acc: 98.80%, consuming tine: 5.29\n",
      "batch: 542, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.29\n",
      "batch: 543, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.40\n",
      "batch: 544, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.48\n",
      "batch: 545, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.58\n",
      "batch: 546, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.38\n",
      "batch: 547, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.42\n",
      "batch: 548, batch train loss: 0.04, train acc: 98.81%, consuming tine: 4.98\n",
      "batch: 549, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.88\n",
      "batch: 550, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.29\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 4.75, valid acc: 35.16%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.24\n",
      "batch: 552, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.50\n",
      "batch: 553, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.25\n",
      "batch: 554, batch train loss: 0.04, train acc: 98.81%, consuming tine: 5.19\n",
      "batch: 555, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.31\n",
      "batch: 556, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.06\n",
      "batch: 557, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.39\n",
      "batch: 558, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.62\n",
      "batch: 559, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.89\n",
      "batch: 560, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.08\n",
      "batch: 561, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.28\n",
      "batch: 562, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.39\n",
      "batch: 563, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.31\n",
      "batch: 564, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.26\n",
      "batch: 565, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.30\n",
      "batch: 566, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.58\n",
      "batch: 567, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.10\n",
      "batch: 568, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.81\n",
      "batch: 569, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.25\n",
      "batch: 570, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.12\n",
      "batch: 571, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.36\n",
      "batch: 572, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.20\n",
      "batch: 573, batch train loss: 0.04, train acc: 98.82%, consuming tine: 5.29\n",
      "batch: 574, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.98\n",
      "batch: 575, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.41\n",
      "batch: 576, batch train loss: 0.04, train acc: 98.82%, consuming tine: 4.88\n",
      "batch: 577, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.68\n",
      "batch: 578, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.17\n",
      "batch: 579, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.09\n",
      "batch: 580, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.48\n",
      "batch: 581, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.30\n",
      "batch: 582, batch train loss: 0.04, train acc: 98.83%, consuming tine: 4.98\n",
      "batch: 583, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 584, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.19\n",
      "batch: 585, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.70\n",
      "batch: 586, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.49\n",
      "batch: 587, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.69\n",
      "batch: 588, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.20\n",
      "batch: 589, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.26\n",
      "batch: 590, batch train loss: 0.04, train acc: 98.83%, consuming tine: 4.78\n",
      "batch: 591, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.35\n",
      "batch: 592, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.23\n",
      "batch: 593, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.08\n",
      "batch: 594, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.28\n",
      "batch: 595, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.20\n",
      "batch: 596, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.31\n",
      "batch: 597, batch train loss: 0.04, train acc: 98.83%, consuming tine: 5.28\n",
      "batch: 598, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.08\n",
      "batch: 599, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.69\n",
      "batch: 600, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.27\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 4.78, valid acc: 35.16%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.15\n",
      "batch: 602, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.30\n",
      "batch: 603, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.09\n",
      "batch: 604, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.79\n",
      "batch: 605, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.39\n",
      "batch: 606, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.38\n",
      "batch: 607, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.18\n",
      "batch: 608, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.29\n",
      "batch: 609, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.29\n",
      "batch: 610, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.08\n",
      "batch: 611, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.20\n",
      "batch: 612, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.20\n",
      "batch: 613, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.07\n",
      "batch: 614, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.89\n",
      "batch: 615, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.19\n",
      "batch: 616, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.26\n",
      "batch: 617, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.23\n",
      "batch: 618, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.49\n",
      "batch: 619, batch train loss: 0.04, train acc: 98.84%, consuming tine: 4.98\n",
      "batch: 620, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.00\n",
      "batch: 621, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.38\n",
      "batch: 622, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.38\n",
      "batch: 623, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.19\n",
      "batch: 624, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.09\n",
      "batch: 625, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.18\n",
      "batch: 626, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.19\n",
      "batch: 627, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.09\n",
      "batch: 628, batch train loss: 0.04, train acc: 98.84%, consuming tine: 5.15\n",
      "batch: 629, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.25\n",
      "batch: 630, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.12\n",
      "batch: 631, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.23\n",
      "batch: 632, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.99\n",
      "batch: 633, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.39\n",
      "batch: 634, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.48\n",
      "batch: 635, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.39\n",
      "batch: 636, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.18\n",
      "batch: 637, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.40\n",
      "batch: 638, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.20\n",
      "batch: 639, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.56\n",
      "batch: 640, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.31\n",
      "batch: 641, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.29\n",
      "batch: 642, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.39\n",
      "batch: 643, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.71\n",
      "batch: 644, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.28\n",
      "batch: 645, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.59\n",
      "batch: 646, batch train loss: 0.04, train acc: 98.85%, consuming tine: 4.87\n",
      "batch: 647, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.19\n",
      "batch: 648, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.39\n",
      "batch: 649, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.38\n",
      "batch: 650, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.60\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 4.81, valid acc: 35.13%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.40\n",
      "batch: 652, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.28\n",
      "batch: 653, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.09\n",
      "batch: 654, batch train loss: 0.04, train acc: 98.85%, consuming tine: 5.39\n",
      "batch: 655, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.68\n",
      "batch: 656, batch train loss: 0.04, train acc: 98.86%, consuming tine: 4.96\n",
      "batch: 657, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.40\n",
      "batch: 658, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.31\n",
      "batch: 659, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.37\n",
      "batch: 660, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.59\n",
      "batch: 661, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.10\n",
      "batch: 662, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.79\n",
      "batch: 663, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.36\n",
      "batch: 664, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.01\n",
      "batch: 665, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.09\n",
      "batch: 666, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.18\n",
      "batch: 667, batch train loss: 0.04, train acc: 98.86%, consuming tine: 6.77\n",
      "batch: 668, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.30\n",
      "batch: 669, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.29\n",
      "batch: 670, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.39\n",
      "batch: 671, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.30\n",
      "batch: 672, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.50\n",
      "batch: 673, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.28\n",
      "batch: 674, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.48\n",
      "batch: 675, batch train loss: 0.04, train acc: 98.86%, consuming tine: 5.29\n",
      "batch: 676, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.29\n",
      "batch: 677, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.22\n",
      "batch: 678, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.36\n",
      "batch: 679, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.19\n",
      "batch: 680, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.47\n",
      "batch: 681, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.38\n",
      "batch: 682, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.39\n",
      "batch: 683, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.30\n",
      "batch: 684, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.28\n",
      "batch: 685, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.39\n",
      "batch: 686, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.29\n",
      "batch: 687, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 688, batch train loss: 0.04, train acc: 98.87%, consuming tine: 5.37\n",
      "batch: 689, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.08\n",
      "batch: 690, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.39\n",
      "batch: 691, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.19\n",
      "batch: 692, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.39\n",
      "batch: 693, batch train loss: 0.03, train acc: 98.87%, consuming tine: 4.59\n",
      "batch: 694, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.09\n",
      "batch: 695, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.08\n",
      "batch: 696, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.30\n",
      "batch: 697, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.09\n",
      "batch: 698, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.09\n",
      "batch: 699, batch train loss: 0.03, train acc: 98.87%, consuming tine: 4.98\n",
      "batch: 700, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.00\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 4.83, valid acc: 35.10%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.04\n",
      "batch: 702, batch train loss: 0.03, train acc: 98.87%, consuming tine: 5.59\n",
      "batch: 703, batch train loss: 0.03, train acc: 98.87%, consuming tine: 4.90\n",
      "batch: 704, batch train loss: 0.03, train acc: 98.87%, consuming tine: 4.78\n",
      "Epoch 8, Loss: 0.03, Accuracy: 98.87%, Valid Loss: 4.83, Valid Accuracy: 35.10%\n",
      "batch: 1, batch train loss: 0.04, train acc: 98.34%, consuming tine: 5.15\n",
      "batch: 2, batch train loss: 0.04, train acc: 98.58%, consuming tine: 4.91\n",
      "batch: 3, batch train loss: 0.04, train acc: 98.73%, consuming tine: 4.72\n",
      "batch: 4, batch train loss: 0.03, train acc: 98.90%, consuming tine: 5.18\n",
      "batch: 5, batch train loss: 0.03, train acc: 98.91%, consuming tine: 5.06\n",
      "batch: 6, batch train loss: 0.03, train acc: 98.89%, consuming tine: 5.30\n",
      "batch: 7, batch train loss: 0.03, train acc: 98.84%, consuming tine: 5.18\n",
      "batch: 8, batch train loss: 0.03, train acc: 98.86%, consuming tine: 5.20\n",
      "batch: 9, batch train loss: 0.03, train acc: 98.91%, consuming tine: 5.30\n",
      "batch: 10, batch train loss: 0.03, train acc: 98.99%, consuming tine: 5.08\n",
      "batch: 11, batch train loss: 0.03, train acc: 99.01%, consuming tine: 5.18\n",
      "batch: 12, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.08\n",
      "batch: 13, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.19\n",
      "batch: 14, batch train loss: 0.03, train acc: 99.08%, consuming tine: 5.24\n",
      "batch: 15, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.24\n",
      "batch: 16, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.30\n",
      "batch: 17, batch train loss: 0.03, train acc: 99.08%, consuming tine: 4.90\n",
      "batch: 18, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.27\n",
      "batch: 19, batch train loss: 0.03, train acc: 99.06%, consuming tine: 4.81\n",
      "batch: 20, batch train loss: 0.03, train acc: 99.04%, consuming tine: 5.16\n",
      "batch: 21, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.26\n",
      "batch: 22, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.22\n",
      "batch: 23, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.09\n",
      "batch: 24, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.37\n",
      "batch: 25, batch train loss: 0.03, train acc: 99.05%, consuming tine: 5.20\n",
      "batch: 26, batch train loss: 0.03, train acc: 99.05%, consuming tine: 5.20\n",
      "batch: 27, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.21\n",
      "batch: 28, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.18\n",
      "batch: 29, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.36\n",
      "batch: 30, batch train loss: 0.03, train acc: 99.05%, consuming tine: 5.10\n",
      "batch: 31, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.17\n",
      "batch: 32, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.38\n",
      "batch: 33, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.19\n",
      "batch: 34, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.40\n",
      "batch: 35, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.28\n",
      "batch: 36, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.20\n",
      "batch: 37, batch train loss: 0.03, train acc: 99.06%, consuming tine: 5.39\n",
      "batch: 38, batch train loss: 0.03, train acc: 99.05%, consuming tine: 4.98\n",
      "batch: 39, batch train loss: 0.03, train acc: 99.05%, consuming tine: 5.39\n",
      "batch: 40, batch train loss: 0.03, train acc: 99.05%, consuming tine: 5.29\n",
      "batch: 41, batch train loss: 0.03, train acc: 99.06%, consuming tine: 4.80\n",
      "batch: 42, batch train loss: 0.03, train acc: 99.07%, consuming tine: 5.20\n",
      "batch: 43, batch train loss: 0.03, train acc: 99.08%, consuming tine: 5.19\n",
      "batch: 44, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.27\n",
      "batch: 45, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.19\n",
      "batch: 46, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.39\n",
      "batch: 47, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.27\n",
      "batch: 48, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.10\n",
      "batch: 49, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.18\n",
      "batch: 50, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.30\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 5.08, valid acc: 34.15%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.30\n",
      "batch: 52, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.10\n",
      "batch: 53, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.59\n",
      "batch: 54, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.47\n",
      "batch: 55, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.20\n",
      "batch: 56, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.59\n",
      "batch: 57, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.36\n",
      "batch: 58, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.12\n",
      "batch: 59, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.27\n",
      "batch: 60, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.59\n",
      "batch: 61, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.19\n",
      "batch: 62, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.10\n",
      "batch: 63, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.59\n",
      "batch: 64, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.08\n",
      "batch: 65, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.20\n",
      "batch: 66, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.57\n",
      "batch: 67, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.30\n",
      "batch: 68, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.19\n",
      "batch: 69, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.48\n",
      "batch: 70, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.20\n",
      "batch: 71, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.48\n",
      "batch: 72, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.59\n",
      "batch: 73, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.39\n",
      "batch: 74, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.20\n",
      "batch: 75, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.37\n",
      "batch: 76, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.09\n",
      "batch: 77, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.20\n",
      "batch: 78, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.18\n",
      "batch: 79, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.28\n",
      "batch: 80, batch train loss: 0.03, train acc: 99.16%, consuming tine: 4.89\n",
      "batch: 81, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.08\n",
      "batch: 82, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.18\n",
      "batch: 83, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.40\n",
      "batch: 84, batch train loss: 0.03, train acc: 99.15%, consuming tine: 4.99\n",
      "batch: 85, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.29\n",
      "batch: 86, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.49\n",
      "batch: 87, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 88, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.31\n",
      "batch: 89, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.28\n",
      "batch: 90, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.16\n",
      "batch: 91, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.30\n",
      "batch: 92, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.89\n",
      "batch: 93, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.21\n",
      "batch: 94, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.27\n",
      "batch: 95, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 96, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.20\n",
      "batch: 97, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 98, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.97\n",
      "batch: 99, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 100, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.29\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 5.12, valid acc: 34.74%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.30\n",
      "batch: 102, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.37\n",
      "batch: 103, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.91\n",
      "batch: 104, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.21\n",
      "batch: 105, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.17\n",
      "batch: 106, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.07\n",
      "batch: 107, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.42\n",
      "batch: 108, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.23\n",
      "batch: 109, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.22\n",
      "batch: 110, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.30\n",
      "batch: 111, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.19\n",
      "batch: 112, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.91\n",
      "batch: 113, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.46\n",
      "batch: 114, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.39\n",
      "batch: 115, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.28\n",
      "batch: 116, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.29\n",
      "batch: 117, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.20\n",
      "batch: 118, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.12\n",
      "batch: 119, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.07\n",
      "batch: 120, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.18\n",
      "batch: 121, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.19\n",
      "batch: 122, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.06\n",
      "batch: 123, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.14\n",
      "batch: 124, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.27\n",
      "batch: 125, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.40\n",
      "batch: 126, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.38\n",
      "batch: 127, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.11\n",
      "batch: 128, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.27\n",
      "batch: 129, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.19\n",
      "batch: 130, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.98\n",
      "batch: 131, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.29\n",
      "batch: 132, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.51\n",
      "batch: 133, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.98\n",
      "batch: 134, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 135, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.23\n",
      "batch: 136, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.25\n",
      "batch: 137, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.02\n",
      "batch: 138, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.37\n",
      "batch: 139, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.19\n",
      "batch: 140, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.58\n",
      "batch: 141, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.90\n",
      "batch: 142, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.59\n",
      "batch: 143, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.37\n",
      "batch: 144, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.37\n",
      "batch: 145, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 146, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.89\n",
      "batch: 147, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.98\n",
      "batch: 148, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.40\n",
      "batch: 149, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.08\n",
      "batch: 150, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 5.09, valid acc: 34.77%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.13\n",
      "batch: 152, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.18\n",
      "batch: 153, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.18\n",
      "batch: 154, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.35\n",
      "batch: 155, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.44\n",
      "batch: 156, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.08\n",
      "batch: 157, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.30\n",
      "batch: 158, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.47\n",
      "batch: 159, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.18\n",
      "batch: 160, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.37\n",
      "batch: 161, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.32\n",
      "batch: 162, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.09\n",
      "batch: 163, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.29\n",
      "batch: 164, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.98\n",
      "batch: 165, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.98\n",
      "batch: 166, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.48\n",
      "batch: 167, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.19\n",
      "batch: 168, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.10\n",
      "batch: 169, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.27\n",
      "batch: 170, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.19\n",
      "batch: 171, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.49\n",
      "batch: 172, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.39\n",
      "batch: 173, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.28\n",
      "batch: 174, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.49\n",
      "batch: 175, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.20\n",
      "batch: 176, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.98\n",
      "batch: 177, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.39\n",
      "batch: 178, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.28\n",
      "batch: 179, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.20\n",
      "batch: 180, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.29\n",
      "batch: 181, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.20\n",
      "batch: 182, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.10\n",
      "batch: 183, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.37\n",
      "batch: 184, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.89\n",
      "batch: 185, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.09\n",
      "batch: 186, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.39\n",
      "batch: 187, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.77\n",
      "batch: 188, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.42\n",
      "batch: 189, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.47\n",
      "batch: 190, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.40\n",
      "batch: 191, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 192, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.09\n",
      "batch: 193, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.99\n",
      "batch: 194, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.49\n",
      "batch: 195, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.88\n",
      "batch: 196, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.30\n",
      "batch: 197, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.17\n",
      "batch: 198, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.90\n",
      "batch: 199, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.29\n",
      "batch: 200, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.99\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 5.10, valid acc: 34.81%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.38\n",
      "batch: 202, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.17\n",
      "batch: 203, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.99\n",
      "batch: 204, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.19\n",
      "batch: 205, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.40\n",
      "batch: 206, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.27\n",
      "batch: 207, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.39\n",
      "batch: 208, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.20\n",
      "batch: 209, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.98\n",
      "batch: 210, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.11\n",
      "batch: 211, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.36\n",
      "batch: 212, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.19\n",
      "batch: 213, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.39\n",
      "batch: 214, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.28\n",
      "batch: 215, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.19\n",
      "batch: 216, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.98\n",
      "batch: 217, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.39\n",
      "batch: 218, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.39\n",
      "batch: 219, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.95\n",
      "batch: 220, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.41\n",
      "batch: 221, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.29\n",
      "batch: 222, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.08\n",
      "batch: 223, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 224, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.59\n",
      "batch: 225, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.49\n",
      "batch: 226, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.99\n",
      "batch: 227, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.10\n",
      "batch: 228, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.18\n",
      "batch: 229, batch train loss: 0.03, train acc: 99.15%, consuming tine: 4.87\n",
      "batch: 230, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.51\n",
      "batch: 231, batch train loss: 0.03, train acc: 99.14%, consuming tine: 4.98\n",
      "batch: 232, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.21\n",
      "batch: 233, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.16\n",
      "batch: 234, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.00\n",
      "batch: 235, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.00\n",
      "batch: 236, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.20\n",
      "batch: 237, batch train loss: 0.03, train acc: 99.14%, consuming tine: 4.86\n",
      "batch: 238, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.29\n",
      "batch: 239, batch train loss: 0.03, train acc: 99.14%, consuming tine: 4.99\n",
      "batch: 240, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.48\n",
      "batch: 241, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.30\n",
      "batch: 242, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.08\n",
      "batch: 243, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.18\n",
      "batch: 244, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.09\n",
      "batch: 245, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.30\n",
      "batch: 246, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.08\n",
      "batch: 247, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.28\n",
      "batch: 248, batch train loss: 0.03, train acc: 99.13%, consuming tine: 4.99\n",
      "batch: 249, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.10\n",
      "batch: 250, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.38\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 5.09, valid acc: 34.76%\n",
      "##################################################\n",
      "batch: 251, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.27\n",
      "batch: 252, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.20\n",
      "batch: 253, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.29\n",
      "batch: 254, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.38\n",
      "batch: 255, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.28\n",
      "batch: 256, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.08\n",
      "batch: 257, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.18\n",
      "batch: 258, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.40\n",
      "batch: 259, batch train loss: 0.03, train acc: 99.13%, consuming tine: 4.88\n",
      "batch: 260, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.19\n",
      "batch: 261, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.48\n",
      "batch: 262, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.40\n",
      "batch: 263, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.18\n",
      "batch: 264, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.72\n",
      "batch: 265, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.37\n",
      "batch: 266, batch train loss: 0.03, train acc: 99.13%, consuming tine: 4.87\n",
      "batch: 267, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.01\n",
      "batch: 268, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.38\n",
      "batch: 269, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.18\n",
      "batch: 270, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.10\n",
      "batch: 271, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.19\n",
      "batch: 272, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.21\n",
      "batch: 273, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.09\n",
      "batch: 274, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.56\n",
      "batch: 275, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.30\n",
      "batch: 276, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.19\n",
      "batch: 277, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.34\n",
      "batch: 278, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.12\n",
      "batch: 279, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.12\n",
      "batch: 280, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.36\n",
      "batch: 281, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.50\n",
      "batch: 282, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.28\n",
      "batch: 283, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.18\n",
      "batch: 284, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.10\n",
      "batch: 285, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.27\n",
      "batch: 286, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.29\n",
      "batch: 287, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.37\n",
      "batch: 288, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.02\n",
      "batch: 289, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.18\n",
      "batch: 290, batch train loss: 0.03, train acc: 99.11%, consuming tine: 4.82\n",
      "batch: 291, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.23\n",
      "batch: 292, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.29\n",
      "batch: 293, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.19\n",
      "batch: 294, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.28\n",
      "batch: 295, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 296, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.27\n",
      "batch: 297, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.39\n",
      "batch: 298, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.29\n",
      "batch: 299, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.59\n",
      "batch: 300, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.29\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 5.06, valid acc: 34.69%\n",
      "##################################################\n",
      "batch: 301, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.27\n",
      "batch: 302, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.48\n",
      "batch: 303, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.30\n",
      "batch: 304, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.19\n",
      "batch: 305, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.28\n",
      "batch: 306, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.50\n",
      "batch: 307, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.28\n",
      "batch: 308, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.50\n",
      "batch: 309, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.19\n",
      "batch: 310, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.28\n",
      "batch: 311, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.29\n",
      "batch: 312, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.39\n",
      "batch: 313, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.19\n",
      "batch: 314, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.18\n",
      "batch: 315, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.19\n",
      "batch: 316, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.39\n",
      "batch: 317, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.29\n",
      "batch: 318, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.39\n",
      "batch: 319, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.10\n",
      "batch: 320, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.28\n",
      "batch: 321, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.45\n",
      "batch: 322, batch train loss: 0.03, train acc: 99.09%, consuming tine: 4.95\n",
      "batch: 323, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.31\n",
      "batch: 324, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.44\n",
      "batch: 325, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.08\n",
      "batch: 326, batch train loss: 0.03, train acc: 99.09%, consuming tine: 4.71\n",
      "batch: 327, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.56\n",
      "batch: 328, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.20\n",
      "batch: 329, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.38\n",
      "batch: 330, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.00\n",
      "batch: 331, batch train loss: 0.03, train acc: 99.09%, consuming tine: 4.98\n",
      "batch: 332, batch train loss: 0.03, train acc: 99.09%, consuming tine: 5.39\n",
      "batch: 333, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.32\n",
      "batch: 334, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.25\n",
      "batch: 335, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.45\n",
      "batch: 336, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.03\n",
      "batch: 337, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.38\n",
      "batch: 338, batch train loss: 0.03, train acc: 99.10%, consuming tine: 4.99\n",
      "batch: 339, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.30\n",
      "batch: 340, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.37\n",
      "batch: 341, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.38\n",
      "batch: 342, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.20\n",
      "batch: 343, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.60\n",
      "batch: 344, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.18\n",
      "batch: 345, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.01\n",
      "batch: 346, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.05\n",
      "batch: 347, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.20\n",
      "batch: 348, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.31\n",
      "batch: 349, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.31\n",
      "batch: 350, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.24\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 5.07, valid acc: 34.70%\n",
      "##################################################\n",
      "batch: 351, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.40\n",
      "batch: 352, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.30\n",
      "batch: 353, batch train loss: 0.03, train acc: 99.10%, consuming tine: 4.98\n",
      "batch: 354, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.31\n",
      "batch: 355, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.59\n",
      "batch: 356, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.37\n",
      "batch: 357, batch train loss: 0.03, train acc: 99.10%, consuming tine: 5.30\n",
      "batch: 358, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.36\n",
      "batch: 359, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.32\n",
      "batch: 360, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.47\n",
      "batch: 361, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.19\n",
      "batch: 362, batch train loss: 0.03, train acc: 99.11%, consuming tine: 4.89\n",
      "batch: 363, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.50\n",
      "batch: 364, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.37\n",
      "batch: 365, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.19\n",
      "batch: 366, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.09\n",
      "batch: 367, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.18\n",
      "batch: 368, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.18\n",
      "batch: 369, batch train loss: 0.03, train acc: 99.11%, consuming tine: 5.31\n",
      "batch: 370, batch train loss: 0.03, train acc: 99.11%, consuming tine: 4.99\n",
      "batch: 371, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.28\n",
      "batch: 372, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.28\n",
      "batch: 373, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.28\n",
      "batch: 374, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.09\n",
      "batch: 375, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.49\n",
      "batch: 376, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.17\n",
      "batch: 377, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.31\n",
      "batch: 378, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.57\n",
      "batch: 379, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.08\n",
      "batch: 380, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.40\n",
      "batch: 381, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.38\n",
      "batch: 382, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.20\n",
      "batch: 383, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.29\n",
      "batch: 384, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.37\n",
      "batch: 385, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.20\n",
      "batch: 386, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.42\n",
      "batch: 387, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.36\n",
      "batch: 388, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.40\n",
      "batch: 389, batch train loss: 0.03, train acc: 99.12%, consuming tine: 5.18\n",
      "batch: 390, batch train loss: 0.03, train acc: 99.12%, consuming tine: 4.99\n",
      "batch: 391, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.09\n",
      "batch: 392, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.48\n",
      "batch: 393, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.39\n",
      "batch: 394, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.28\n",
      "batch: 395, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.55\n",
      "batch: 396, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.12\n",
      "batch: 397, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.19\n",
      "batch: 398, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.20\n",
      "batch: 399, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 400, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.10\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 5.07, valid acc: 34.77%\n",
      "##################################################\n",
      "batch: 401, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.22\n",
      "batch: 402, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.41\n",
      "batch: 403, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.17\n",
      "batch: 404, batch train loss: 0.03, train acc: 99.13%, consuming tine: 4.88\n",
      "batch: 405, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.39\n",
      "batch: 406, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.31\n",
      "batch: 407, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.17\n",
      "batch: 408, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.38\n",
      "batch: 409, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.30\n",
      "batch: 410, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.35\n",
      "batch: 411, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.83\n",
      "batch: 412, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.17\n",
      "batch: 413, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.11\n",
      "batch: 414, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.36\n",
      "batch: 415, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.30\n",
      "batch: 416, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.08\n",
      "batch: 417, batch train loss: 0.03, train acc: 99.13%, consuming tine: 4.82\n",
      "batch: 418, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.16\n",
      "batch: 419, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.41\n",
      "batch: 420, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.28\n",
      "batch: 421, batch train loss: 0.03, train acc: 99.13%, consuming tine: 5.08\n",
      "batch: 422, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.39\n",
      "batch: 423, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.19\n",
      "batch: 424, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.30\n",
      "batch: 425, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.58\n",
      "batch: 426, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.80\n",
      "batch: 427, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.08\n",
      "batch: 428, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.08\n",
      "batch: 429, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.11\n",
      "batch: 430, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.08\n",
      "batch: 431, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.68\n",
      "batch: 432, batch train loss: 0.03, train acc: 99.14%, consuming tine: 4.98\n",
      "batch: 433, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.37\n",
      "batch: 434, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.35\n",
      "batch: 435, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.12\n",
      "batch: 436, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.39\n",
      "batch: 437, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.38\n",
      "batch: 438, batch train loss: 0.03, train acc: 99.14%, consuming tine: 5.40\n",
      "batch: 439, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.18\n",
      "batch: 440, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.37\n",
      "batch: 441, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.18\n",
      "batch: 442, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 443, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 444, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 445, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 446, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.17\n",
      "batch: 447, batch train loss: 0.03, train acc: 99.15%, consuming tine: 4.81\n",
      "batch: 448, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.17\n",
      "batch: 449, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 450, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.28\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 5.10, valid acc: 34.74%\n",
      "##################################################\n",
      "batch: 451, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.17\n",
      "batch: 452, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.30\n",
      "batch: 453, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 454, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.29\n",
      "batch: 455, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.20\n",
      "batch: 456, batch train loss: 0.03, train acc: 99.15%, consuming tine: 4.99\n",
      "batch: 457, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.38\n",
      "batch: 458, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.47\n",
      "batch: 459, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.23\n",
      "batch: 460, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.46\n",
      "batch: 461, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.09\n",
      "batch: 462, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.18\n",
      "batch: 463, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.50\n",
      "batch: 464, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.23\n",
      "batch: 465, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.04\n",
      "batch: 466, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.70\n",
      "batch: 467, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.30\n",
      "batch: 468, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.40\n",
      "batch: 469, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.34\n",
      "batch: 470, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.41\n",
      "batch: 471, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.48\n",
      "batch: 472, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.18\n",
      "batch: 473, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.08\n",
      "batch: 474, batch train loss: 0.03, train acc: 99.16%, consuming tine: 4.91\n",
      "batch: 475, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.27\n",
      "batch: 476, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.59\n",
      "batch: 477, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.48\n",
      "batch: 478, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.10\n",
      "batch: 479, batch train loss: 0.03, train acc: 99.16%, consuming tine: 4.99\n",
      "batch: 480, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.38\n",
      "batch: 481, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.20\n",
      "batch: 482, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.29\n",
      "batch: 483, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.15\n",
      "batch: 484, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.61\n",
      "batch: 485, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.28\n",
      "batch: 486, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.09\n",
      "batch: 487, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.41\n",
      "batch: 488, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.08\n",
      "batch: 489, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.89\n",
      "batch: 490, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.39\n",
      "batch: 491, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.39\n",
      "batch: 492, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.69\n",
      "batch: 493, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.49\n",
      "batch: 494, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.40\n",
      "batch: 495, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.17\n",
      "batch: 496, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.19\n",
      "batch: 497, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.20\n",
      "batch: 498, batch train loss: 0.03, train acc: 99.17%, consuming tine: 4.80\n",
      "batch: 499, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.27\n",
      "batch: 500, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.30\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 5.11, valid acc: 34.78%\n",
      "##################################################\n",
      "batch: 501, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 502, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.29\n",
      "batch: 503, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.07\n",
      "batch: 504, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.08\n",
      "batch: 505, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.30\n",
      "batch: 506, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.12\n",
      "batch: 507, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.14\n",
      "batch: 508, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.09\n",
      "batch: 509, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.30\n",
      "batch: 510, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.39\n",
      "batch: 511, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.32\n",
      "batch: 512, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.27\n",
      "batch: 513, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.08\n",
      "batch: 514, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.10\n",
      "batch: 515, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.19\n",
      "batch: 516, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.19\n",
      "batch: 517, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.27\n",
      "batch: 518, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.41\n",
      "batch: 519, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.36\n",
      "batch: 520, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.72\n",
      "batch: 521, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.26\n",
      "batch: 522, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.29\n",
      "batch: 523, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.38\n",
      "batch: 524, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.10\n",
      "batch: 525, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 526, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.29\n",
      "batch: 527, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 528, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.40\n",
      "batch: 529, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.68\n",
      "batch: 530, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.19\n",
      "batch: 531, batch train loss: 0.03, train acc: 99.18%, consuming tine: 4.98\n",
      "batch: 532, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 533, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.21\n",
      "batch: 534, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 535, batch train loss: 0.03, train acc: 99.18%, consuming tine: 5.09\n",
      "batch: 536, batch train loss: 0.03, train acc: 99.19%, consuming tine: 4.98\n",
      "batch: 537, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.30\n",
      "batch: 538, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.38\n",
      "batch: 539, batch train loss: 0.03, train acc: 99.19%, consuming tine: 4.99\n",
      "batch: 540, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.58\n",
      "batch: 541, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.29\n",
      "batch: 542, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.20\n",
      "batch: 543, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.26\n",
      "batch: 544, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.12\n",
      "batch: 545, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.58\n",
      "batch: 546, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.08\n",
      "batch: 547, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.29\n",
      "batch: 548, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.04\n",
      "batch: 549, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.18\n",
      "batch: 550, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.17\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 5.14, valid acc: 34.77%\n",
      "##################################################\n",
      "batch: 551, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.34\n",
      "batch: 552, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.06\n",
      "batch: 553, batch train loss: 0.03, train acc: 99.19%, consuming tine: 4.88\n",
      "batch: 554, batch train loss: 0.03, train acc: 99.19%, consuming tine: 5.49\n",
      "batch: 555, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.06\n",
      "batch: 556, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.44\n",
      "batch: 557, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.06\n",
      "batch: 558, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.08\n",
      "batch: 559, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.39\n",
      "batch: 560, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.27\n",
      "batch: 561, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.29\n",
      "batch: 562, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.09\n",
      "batch: 563, batch train loss: 0.03, train acc: 99.20%, consuming tine: 5.31\n",
      "batch: 564, batch train loss: 0.03, train acc: 99.20%, consuming tine: 4.73\n",
      "batch: 565, batch train loss: 0.03, train acc: 99.20%, consuming tine: 4.89\n",
      "batch: 566, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.40\n",
      "batch: 567, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.87\n",
      "batch: 568, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.60\n",
      "batch: 569, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.76\n",
      "batch: 570, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.01\n",
      "batch: 571, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.08\n",
      "batch: 572, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.27\n",
      "batch: 573, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.11\n",
      "batch: 574, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.37\n",
      "batch: 575, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.98\n",
      "batch: 576, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.19\n",
      "batch: 577, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.38\n",
      "batch: 578, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.89\n",
      "batch: 579, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.08\n",
      "batch: 580, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.11\n",
      "batch: 581, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.98\n",
      "batch: 582, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.70\n",
      "batch: 583, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.09\n",
      "batch: 584, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.18\n",
      "batch: 585, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.41\n",
      "batch: 586, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.16\n",
      "batch: 587, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.30\n",
      "batch: 588, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.08\n",
      "batch: 589, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.00\n",
      "batch: 590, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.29\n",
      "batch: 591, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.38\n",
      "batch: 592, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.09\n",
      "batch: 593, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.56\n",
      "batch: 594, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.03\n",
      "batch: 595, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.28\n",
      "batch: 596, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.21\n",
      "batch: 597, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.36\n",
      "batch: 598, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.32\n",
      "batch: 599, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.47\n",
      "batch: 600, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.18\n",
      "##################################################\n",
      "batch: 600, batch valid loss: 5.15, valid acc: 34.77%\n",
      "##################################################\n",
      "batch: 601, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.01\n",
      "batch: 602, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.17\n",
      "batch: 603, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.39\n",
      "batch: 604, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.19\n",
      "batch: 605, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 606, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.87\n",
      "batch: 607, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.98\n",
      "batch: 608, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.10\n",
      "batch: 609, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.38\n",
      "batch: 610, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.89\n",
      "batch: 611, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.48\n",
      "batch: 612, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.09\n",
      "batch: 613, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.31\n",
      "batch: 614, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.87\n",
      "batch: 615, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.99\n",
      "batch: 616, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.29\n",
      "batch: 617, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.28\n",
      "batch: 618, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.19\n",
      "batch: 619, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.30\n",
      "batch: 620, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.10\n",
      "batch: 621, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.26\n",
      "batch: 622, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.48\n",
      "batch: 623, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.21\n",
      "batch: 624, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.74\n",
      "batch: 625, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.53\n",
      "batch: 626, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.99\n",
      "batch: 627, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.48\n",
      "batch: 628, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.32\n",
      "batch: 629, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.65\n",
      "batch: 630, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.03\n",
      "batch: 631, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.56\n",
      "batch: 632, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.46\n",
      "batch: 633, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.10\n",
      "batch: 634, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.28\n",
      "batch: 635, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.24\n",
      "batch: 636, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.24\n",
      "batch: 637, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.18\n",
      "batch: 638, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.58\n",
      "batch: 639, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.30\n",
      "batch: 640, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.29\n",
      "batch: 641, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.20\n",
      "batch: 642, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.38\n",
      "batch: 643, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.70\n",
      "batch: 644, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.87\n",
      "batch: 645, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.31\n",
      "batch: 646, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.26\n",
      "batch: 647, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.18\n",
      "batch: 648, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.19\n",
      "batch: 649, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.24\n",
      "batch: 650, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.84\n",
      "##################################################\n",
      "batch: 650, batch valid loss: 5.18, valid acc: 34.74%\n",
      "##################################################\n",
      "batch: 651, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.39\n",
      "batch: 652, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.37\n",
      "batch: 653, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.20\n",
      "batch: 654, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.38\n",
      "batch: 655, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.18\n",
      "batch: 656, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.39\n",
      "batch: 657, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.47\n",
      "batch: 658, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.04\n",
      "batch: 659, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.97\n",
      "batch: 660, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.50\n",
      "batch: 661, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.86\n",
      "batch: 662, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.41\n",
      "batch: 663, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.27\n",
      "batch: 664, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.30\n",
      "batch: 665, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.28\n",
      "batch: 666, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.10\n",
      "batch: 667, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.37\n",
      "batch: 668, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.31\n",
      "batch: 669, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.38\n",
      "batch: 670, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.17\n",
      "batch: 671, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.40\n",
      "batch: 672, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.09\n",
      "batch: 673, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.28\n",
      "batch: 674, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.39\n",
      "batch: 675, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.00\n",
      "batch: 676, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.20\n",
      "batch: 677, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.36\n",
      "batch: 678, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.40\n",
      "batch: 679, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.38\n",
      "batch: 680, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.21\n",
      "batch: 681, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.19\n",
      "batch: 682, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.36\n",
      "batch: 683, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.49\n",
      "batch: 684, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.19\n",
      "batch: 685, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.39\n",
      "batch: 686, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.11\n",
      "batch: 687, batch train loss: 0.02, train acc: 99.23%, consuming tine: 4.99\n",
      "batch: 688, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.17\n",
      "batch: 689, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.31\n",
      "batch: 690, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.57\n",
      "batch: 691, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.11\n",
      "batch: 692, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.47\n",
      "batch: 693, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.21\n",
      "batch: 694, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.27\n",
      "batch: 695, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.28\n",
      "batch: 696, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.98\n",
      "batch: 697, batch train loss: 0.02, train acc: 99.23%, consuming tine: 5.31\n",
      "batch: 698, batch train loss: 0.02, train acc: 99.23%, consuming tine: 5.29\n",
      "batch: 699, batch train loss: 0.02, train acc: 99.23%, consuming tine: 4.87\n",
      "batch: 700, batch train loss: 0.02, train acc: 99.23%, consuming tine: 5.30\n",
      "##################################################\n",
      "batch: 700, batch valid loss: 5.19, valid acc: 34.76%\n",
      "##################################################\n",
      "batch: 701, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.49\n",
      "batch: 702, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.07\n",
      "batch: 703, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.06\n",
      "batch: 704, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.82\n",
      "Epoch 9, Loss: 0.02, Accuracy: 99.22%, Valid Loss: 5.19, Valid Accuracy: 34.76%\n",
      "batch: 1, batch train loss: 0.01, train acc: 99.32%, consuming tine: 5.38\n",
      "batch: 2, batch train loss: 0.02, train acc: 99.37%, consuming tine: 5.30\n",
      "batch: 3, batch train loss: 0.02, train acc: 99.35%, consuming tine: 4.99\n",
      "batch: 4, batch train loss: 0.02, train acc: 99.37%, consuming tine: 5.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5, batch train loss: 0.02, train acc: 99.34%, consuming tine: 5.81\n",
      "batch: 6, batch train loss: 0.02, train acc: 99.35%, consuming tine: 5.19\n",
      "batch: 7, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.10\n",
      "batch: 8, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.46\n",
      "batch: 9, batch train loss: 0.03, train acc: 99.16%, consuming tine: 5.10\n",
      "batch: 10, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.30\n",
      "batch: 11, batch train loss: 0.03, train acc: 99.17%, consuming tine: 5.21\n",
      "batch: 12, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.27\n",
      "batch: 13, batch train loss: 0.02, train acc: 99.17%, consuming tine: 5.29\n",
      "batch: 14, batch train loss: 0.02, train acc: 99.16%, consuming tine: 5.28\n",
      "batch: 15, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 16, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.00\n",
      "batch: 17, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.38\n",
      "batch: 18, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.01\n",
      "batch: 19, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.61\n",
      "batch: 20, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.37\n",
      "batch: 21, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.08\n",
      "batch: 22, batch train loss: 0.02, train acc: 99.18%, consuming tine: 4.89\n",
      "batch: 23, batch train loss: 0.02, train acc: 99.16%, consuming tine: 4.71\n",
      "batch: 24, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.37\n",
      "batch: 25, batch train loss: 0.03, train acc: 99.15%, consuming tine: 5.17\n",
      "batch: 26, batch train loss: 0.02, train acc: 99.16%, consuming tine: 5.39\n",
      "batch: 27, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.18\n",
      "batch: 28, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.10\n",
      "batch: 29, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.39\n",
      "batch: 30, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.28\n",
      "batch: 31, batch train loss: 0.02, train acc: 99.16%, consuming tine: 5.22\n",
      "batch: 32, batch train loss: 0.02, train acc: 99.16%, consuming tine: 5.58\n",
      "batch: 33, batch train loss: 0.02, train acc: 99.16%, consuming tine: 5.17\n",
      "batch: 34, batch train loss: 0.02, train acc: 99.17%, consuming tine: 5.39\n",
      "batch: 35, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.08\n",
      "batch: 36, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.11\n",
      "batch: 37, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.18\n",
      "batch: 38, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.20\n",
      "batch: 39, batch train loss: 0.02, train acc: 99.18%, consuming tine: 5.25\n",
      "batch: 40, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.33\n",
      "batch: 41, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.26\n",
      "batch: 42, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.90\n",
      "batch: 43, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.10\n",
      "batch: 44, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.96\n",
      "batch: 45, batch train loss: 0.02, train acc: 99.19%, consuming tine: 4.92\n",
      "batch: 46, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.35\n",
      "batch: 47, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.19\n",
      "batch: 48, batch train loss: 0.02, train acc: 99.19%, consuming tine: 5.72\n",
      "batch: 49, batch train loss: 0.02, train acc: 99.20%, consuming tine: 4.77\n",
      "batch: 50, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.08\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 5.32, valid acc: 34.01%\n",
      "##################################################\n",
      "batch: 51, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.61\n",
      "batch: 52, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.59\n",
      "batch: 53, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.87\n",
      "batch: 54, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.17\n",
      "batch: 55, batch train loss: 0.02, train acc: 99.20%, consuming tine: 5.28\n",
      "batch: 56, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.13\n",
      "batch: 57, batch train loss: 0.02, train acc: 99.21%, consuming tine: 4.96\n",
      "batch: 58, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.08\n",
      "batch: 59, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.28\n",
      "batch: 60, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.00\n",
      "batch: 61, batch train loss: 0.02, train acc: 99.21%, consuming tine: 5.39\n",
      "batch: 62, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.28\n",
      "batch: 63, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.29\n",
      "batch: 64, batch train loss: 0.02, train acc: 99.22%, consuming tine: 5.58\n",
      "batch: 65, batch train loss: 0.02, train acc: 99.22%, consuming tine: 4.98\n",
      "batch: 66, batch train loss: 0.02, train acc: 99.23%, consuming tine: 5.50\n",
      "batch: 67, batch train loss: 0.02, train acc: 99.23%, consuming tine: 5.07\n",
      "batch: 68, batch train loss: 0.02, train acc: 99.24%, consuming tine: 4.98\n",
      "batch: 69, batch train loss: 0.02, train acc: 99.24%, consuming tine: 5.40\n",
      "batch: 70, batch train loss: 0.02, train acc: 99.25%, consuming tine: 5.40\n",
      "batch: 71, batch train loss: 0.02, train acc: 99.25%, consuming tine: 4.88\n",
      "batch: 72, batch train loss: 0.02, train acc: 99.25%, consuming tine: 5.41\n",
      "batch: 73, batch train loss: 0.02, train acc: 99.25%, consuming tine: 4.98\n",
      "batch: 74, batch train loss: 0.02, train acc: 99.25%, consuming tine: 5.18\n",
      "batch: 75, batch train loss: 0.02, train acc: 99.26%, consuming tine: 5.48\n",
      "batch: 76, batch train loss: 0.02, train acc: 99.26%, consuming tine: 5.18\n",
      "batch: 77, batch train loss: 0.02, train acc: 99.26%, consuming tine: 4.69\n",
      "batch: 78, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.51\n",
      "batch: 79, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.07\n",
      "batch: 80, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.69\n",
      "batch: 81, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.79\n",
      "batch: 82, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.07\n",
      "batch: 83, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.00\n",
      "batch: 84, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.01\n",
      "batch: 85, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.79\n",
      "batch: 86, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.01\n",
      "batch: 87, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.86\n",
      "batch: 88, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.97\n",
      "batch: 89, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.79\n",
      "batch: 90, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.59\n",
      "batch: 91, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.79\n",
      "batch: 92, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.60\n",
      "batch: 93, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.07\n",
      "batch: 94, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.78\n",
      "batch: 95, batch train loss: 0.02, train acc: 99.27%, consuming tine: 5.00\n",
      "batch: 96, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.88\n",
      "batch: 97, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.71\n",
      "batch: 98, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.87\n",
      "batch: 99, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.67\n",
      "batch: 100, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.70\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 5.31, valid acc: 34.61%\n",
      "##################################################\n",
      "batch: 101, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.48\n",
      "batch: 102, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.79\n",
      "batch: 103, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.69\n",
      "batch: 104, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.28\n",
      "batch: 105, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.80\n",
      "batch: 106, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.79\n",
      "batch: 107, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.98\n",
      "batch: 108, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.48\n",
      "batch: 109, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.97\n",
      "batch: 110, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 111, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.79\n",
      "batch: 112, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.76\n",
      "batch: 113, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.84\n",
      "batch: 114, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.75\n",
      "batch: 115, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.77\n",
      "batch: 116, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.50\n",
      "batch: 117, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.06\n",
      "batch: 118, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.02\n",
      "batch: 119, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.38\n",
      "batch: 120, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.09\n",
      "batch: 121, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.59\n",
      "batch: 122, batch train loss: 0.02, train acc: 99.27%, consuming tine: 4.98\n",
      "batch: 123, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.49\n",
      "batch: 124, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.60\n",
      "batch: 125, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.78\n",
      "batch: 126, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.69\n",
      "batch: 127, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.89\n",
      "batch: 128, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.01\n",
      "batch: 129, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.48\n",
      "batch: 130, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.87\n",
      "batch: 131, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.79\n",
      "batch: 132, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.17\n",
      "batch: 133, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.63\n",
      "batch: 134, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.85\n",
      "batch: 135, batch train loss: 0.02, train acc: 99.28%, consuming tine: 5.08\n",
      "batch: 136, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.41\n",
      "batch: 137, batch train loss: 0.02, train acc: 99.28%, consuming tine: 4.49\n",
      "batch: 138, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.57\n",
      "batch: 139, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.82\n",
      "batch: 140, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.57\n",
      "batch: 141, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.69\n",
      "batch: 142, batch train loss: 0.02, train acc: 99.29%, consuming tine: 5.00\n",
      "batch: 143, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.78\n",
      "batch: 144, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.58\n",
      "batch: 145, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.59\n",
      "batch: 146, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.71\n",
      "batch: 147, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.66\n",
      "batch: 148, batch train loss: 0.02, train acc: 99.29%, consuming tine: 4.70\n",
      "batch: 149, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.67\n",
      "batch: 150, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.81\n",
      "##################################################\n",
      "batch: 150, batch valid loss: 5.31, valid acc: 34.55%\n",
      "##################################################\n",
      "batch: 151, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.76\n",
      "batch: 152, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.59\n",
      "batch: 153, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.55\n",
      "batch: 154, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.52\n",
      "batch: 155, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.78\n",
      "batch: 156, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.77\n",
      "batch: 157, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.70\n",
      "batch: 158, batch train loss: 0.02, train acc: 99.30%, consuming tine: 5.10\n",
      "batch: 159, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.47\n",
      "batch: 160, batch train loss: 0.02, train acc: 99.30%, consuming tine: 5.09\n",
      "batch: 161, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.90\n",
      "batch: 162, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.17\n",
      "batch: 163, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.64\n",
      "batch: 164, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.84\n",
      "batch: 165, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.79\n",
      "batch: 166, batch train loss: 0.02, train acc: 99.30%, consuming tine: 5.07\n",
      "batch: 167, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.60\n",
      "batch: 168, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.78\n",
      "batch: 169, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.48\n",
      "batch: 170, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.49\n",
      "batch: 171, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.99\n",
      "batch: 172, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.59\n",
      "batch: 173, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.88\n",
      "batch: 174, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.81\n",
      "batch: 175, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.90\n",
      "batch: 176, batch train loss: 0.02, train acc: 99.30%, consuming tine: 5.16\n",
      "batch: 177, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.79\n",
      "batch: 178, batch train loss: 0.02, train acc: 99.30%, consuming tine: 5.08\n",
      "batch: 179, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.70\n",
      "batch: 180, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.98\n",
      "batch: 181, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.88\n",
      "batch: 182, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.88\n",
      "batch: 183, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.94\n",
      "batch: 184, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.44\n",
      "batch: 185, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.48\n",
      "batch: 186, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.69\n",
      "batch: 187, batch train loss: 0.02, train acc: 99.30%, consuming tine: 4.59\n",
      "batch: 188, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.50\n",
      "batch: 189, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.67\n",
      "batch: 190, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.39\n",
      "batch: 191, batch train loss: 0.02, train acc: 99.31%, consuming tine: 5.08\n",
      "batch: 192, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.39\n",
      "batch: 193, batch train loss: 0.02, train acc: 99.31%, consuming tine: 4.69\n",
      "batch: 194, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.69\n",
      "batch: 195, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.61\n",
      "batch: 196, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.87\n",
      "batch: 197, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.58\n",
      "batch: 198, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.70\n",
      "batch: 199, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.18\n",
      "batch: 200, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.61\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 5.34, valid acc: 34.43%\n",
      "##################################################\n",
      "batch: 201, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.08\n",
      "batch: 202, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.37\n",
      "batch: 203, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.91\n",
      "batch: 204, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.70\n",
      "batch: 205, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.97\n",
      "batch: 206, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.00\n",
      "batch: 207, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.48\n",
      "batch: 208, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.17\n",
      "batch: 209, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.40\n",
      "batch: 210, batch train loss: 0.02, train acc: 99.33%, consuming tine: 4.90\n",
      "batch: 211, batch train loss: 0.02, train acc: 99.33%, consuming tine: 4.98\n",
      "batch: 212, batch train loss: 0.02, train acc: 99.33%, consuming tine: 5.50\n",
      "batch: 213, batch train loss: 0.02, train acc: 99.33%, consuming tine: 4.69\n",
      "batch: 214, batch train loss: 0.02, train acc: 99.33%, consuming tine: 4.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 215, batch train loss: 0.02, train acc: 99.33%, consuming tine: 5.21\n",
      "batch: 216, batch train loss: 0.02, train acc: 99.33%, consuming tine: 5.17\n",
      "batch: 217, batch train loss: 0.02, train acc: 99.33%, consuming tine: 5.19\n",
      "batch: 218, batch train loss: 0.02, train acc: 99.33%, consuming tine: 5.20\n",
      "batch: 219, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.40\n",
      "batch: 220, batch train loss: 0.02, train acc: 99.32%, consuming tine: 4.97\n",
      "batch: 221, batch train loss: 0.02, train acc: 99.32%, consuming tine: 5.70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bcab47b9fa8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mCNT\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         print('batch: {:d}, batch train loss: {:.2f}, train acc: {:.2f}%, consuming tine: {:.2f}'.\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "\n",
    "    CNT = 0\n",
    "    for features, labels in train_ds:\n",
    "        begin = time.time()\n",
    "        train_step(features, labels)\n",
    "        CNT += 1\n",
    "        print('batch: {:d}, batch train loss: {:.2f}, train acc: {:.2f}%, consuming tine: {:.2f}'.\n",
    "              format(CNT, train_loss.result(), train_accuracy.result()*100, time.time()-begin))\n",
    "\n",
    "        if CNT % 50 == 0: \n",
    "            for val_features, val_labels in valid_ds:\n",
    "                valid_step(val_features, val_labels)\n",
    "            print('##################################################')\n",
    "            print('batch: {:d}, batch valid loss: {:.2f}, valid acc: {:.2f}%'.format(CNT, valid_loss.result(), valid_accuracy.result()*100)) \n",
    "            print('##################################################')\n",
    "            \n",
    "    template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}%, Valid Loss: {:.2f}, Valid Accuracy: {:.2f}%'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         valid_loss.result(),\n",
    "                         valid_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition-py36",
   "language": "python",
   "name": "competition-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
