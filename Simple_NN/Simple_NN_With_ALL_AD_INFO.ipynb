{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter\n",
    "###################\n",
    "SAVE_PATH = '/home/huangzc/competition/tencent/model_ckpt/simple_nn/model_all.ckpt'\n",
    "EMB_SIZE = 50\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "TARGET = 'age'\n",
    "COLS_NAME = ['creative_id', 'advertiser_id', 'ad_id', 'product_id', 'product_category', 'industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data\n",
    "### TARGET DF\n",
    "tr_user_df = pd.read_pickle(TRAIN_DIR+USER_PATH)\n",
    "tr_user_df = tr_user_df.groupby(['user_id']).agg({'age': 'first', 'gender': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ad_df = pd.read_pickle(TRAIN_DIR+AD_PATH)\n",
    "ts_ad_df = pd.read_pickle(TEST_DIR+AD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:03<00:00, 40.64s/it]\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "for col in tqdm(COLS_NAME):\n",
    "    train_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    test_df.append(pd.read_pickle(TRAIN_DIR+CLK_PATH_DICT[col]))\n",
    "    \n",
    "train_df.append(tr_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_df(creative_df, advertiser_df, ad_df, product_id_df, product_cat_df, industry_df, user_df=None):\n",
    "    if user_df is  None:\n",
    "        user_df = creative_df[['user_id']]\n",
    "        user_df[TARGET] = np.nan\n",
    "    assert user_df['user_id'].values.tolist() == creative_df['user_id'].values.tolist() \\\n",
    "    == ad_df['user_id'].values.tolist() == product_id_df['user_id'].values.tolist() \\\n",
    "    == product_cat_df['user_id'].values.tolist() == industry_df['user_id'].values.tolist() \\\n",
    "    == advertiser_df['user_id'].values.tolist()\n",
    "\n",
    "    del advertiser_df['user_id'], ad_df['user_id'], product_id_df['user_id'], product_cat_df['user_id'], industry_df['user_id']\n",
    "    \n",
    "    grid_df = pd.concat([creative_df, advertiser_df, \n",
    "                         ad_df, product_id_df, \n",
    "                         product_cat_df, industry_df,\n",
    "                         user_df[[TARGET]]], axis=1)\n",
    "\n",
    "    return grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = get_grid_df(*train_df)\n",
    "grid_df_test = get_grid_df(*test_df)\n",
    "grid_df[TARGET] = grid_df[TARGET] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(grid_df, test_size=0.2, random_state=2020)\n",
    "test = grid_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_feature():  \n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    x_test = []\n",
    "    \n",
    "    sentence_size = max(int(grid_df[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.99)), \n",
    "                        int(grid_df_test[COLS_NAME[0]].map(lambda x: len(x)).quantile(0.99)))\n",
    "    print('choose sentences max len: %d' % (sentence_size))\n",
    "    print(\"Pad sequences (samples x time)\")      \n",
    "    for col in tqdm(COLS_NAME):\n",
    "        x_train.append(sequence.pad_sequences(train[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_val.append(sequence.pad_sequences(val[col],\n",
    "                                             maxlen=sentence_size, \n",
    "                                             padding='post', \n",
    "                                             truncating='post',\n",
    "                                             dtype='int64',\n",
    "                                             value=0\n",
    "                                             ))\n",
    "        x_test.append(sequence.pad_sequences(test[col], \n",
    "                                            maxlen=sentence_size, \n",
    "                                            padding='post',\n",
    "                                            truncating='post',\n",
    "                                            dtype='int64',\n",
    "                                            value=0\n",
    "                                           ))\n",
    "    print('feature count: train->%d, valid->%d, test->%d' %(len(x_train), len(x_val), len(x_test)))\n",
    "    return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose sentences max len: 156\n",
      "Pad sequences (samples x time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:08<00:00, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature count: train->6, valid->6, test->6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, x_test = pad_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get tf dataset\n",
    "def get_train_ds(x, y): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.repeat(EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "def get_test_ds(x, ): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_train_ds(tuple(x_train), train[TARGET].values)\n",
    "valid_ds = get_train_ds(tuple(x_val), val[TARGET].values)\n",
    "test_ds = get_test_ds(tuple(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_id\n",
      "advertiser_id\n",
      "ad_id\n",
      "product_id\n",
      "product_category\n",
      "industry\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = []\n",
    "temp = pd.concat([tr_ad_df, ts_ad_df], axis=0)\n",
    "for col in COLS_NAME:\n",
    "    print(col)\n",
    "    vocab_sizes.append(max(temp[col].unique().tolist()) + 1) ### padding 0 need add 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.load('/home/huangzc/competition/tencent/data/train_preliminary/gensim_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct Model\n",
    "#################################\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, EMB_SIZE, weights=[weights])\n",
    "        self.embeddings = []\n",
    "        for s in vocab_sizes:\n",
    "            self.embeddings.append(tf.keras.layers.Embedding(s, EMB_SIZE))\n",
    "        self.pool1D = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embs = []\n",
    "        for emb, inp in zip(self.embeddings, inputs):\n",
    "            x = emb(inp)            \n",
    "            x = self.pool1D(x)\n",
    "            embs.append(x)\n",
    "        x = self.concat(embs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f07e25529e8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Restore the weights\n",
    "model.load_weights(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss & Metric\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(features, labels):\n",
    "    predictions = model(features)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(v_loss)\n",
    "    valid_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1, batch train loss: 1.28, train acc: 50.78%\n",
      "batch: 2, batch train loss: 1.68, train acc: 39.50%\n",
      "batch: 3, batch train loss: 1.55, train acc: 42.97%\n",
      "batch: 4, batch train loss: 1.61, train acc: 40.01%\n",
      "batch: 5, batch train loss: 1.62, train acc: 38.61%\n",
      "batch: 6, batch train loss: 1.59, train acc: 38.88%\n",
      "batch: 7, batch train loss: 1.56, train acc: 38.95%\n",
      "batch: 8, batch train loss: 1.56, train acc: 38.73%\n",
      "batch: 9, batch train loss: 1.56, train acc: 38.14%\n",
      "batch: 10, batch train loss: 1.55, train acc: 38.00%\n",
      "batch: 11, batch train loss: 1.52, train acc: 38.99%\n",
      "batch: 12, batch train loss: 1.51, train acc: 39.15%\n",
      "batch: 13, batch train loss: 1.51, train acc: 39.09%\n",
      "batch: 14, batch train loss: 1.51, train acc: 39.36%\n",
      "batch: 15, batch train loss: 1.50, train acc: 39.79%\n",
      "batch: 16, batch train loss: 1.49, train acc: 40.30%\n",
      "batch: 17, batch train loss: 1.48, train acc: 40.52%\n",
      "batch: 18, batch train loss: 1.47, train acc: 40.56%\n",
      "batch: 19, batch train loss: 1.46, train acc: 40.70%\n",
      "batch: 20, batch train loss: 1.46, train acc: 40.94%\n",
      "batch: 21, batch train loss: 1.45, train acc: 41.24%\n",
      "batch: 22, batch train loss: 1.45, train acc: 41.56%\n",
      "batch: 23, batch train loss: 1.44, train acc: 41.63%\n",
      "batch: 24, batch train loss: 1.44, train acc: 41.81%\n",
      "batch: 25, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 26, batch train loss: 1.43, train acc: 42.27%\n",
      "batch: 27, batch train loss: 1.42, train acc: 42.56%\n",
      "batch: 28, batch train loss: 1.42, train acc: 42.70%\n",
      "batch: 29, batch train loss: 1.42, train acc: 42.91%\n",
      "batch: 30, batch train loss: 1.42, train acc: 43.08%\n",
      "batch: 31, batch train loss: 1.41, train acc: 43.33%\n",
      "batch: 32, batch train loss: 1.41, train acc: 43.47%\n",
      "batch: 33, batch train loss: 1.41, train acc: 43.54%\n",
      "batch: 34, batch train loss: 1.40, train acc: 43.59%\n",
      "batch: 35, batch train loss: 1.40, train acc: 43.78%\n",
      "batch: 36, batch train loss: 1.40, train acc: 43.83%\n",
      "batch: 37, batch train loss: 1.40, train acc: 43.92%\n",
      "batch: 38, batch train loss: 1.40, train acc: 43.91%\n",
      "batch: 39, batch train loss: 1.40, train acc: 43.94%\n",
      "batch: 40, batch train loss: 1.40, train acc: 44.02%\n",
      "batch: 41, batch train loss: 1.39, train acc: 44.06%\n",
      "batch: 42, batch train loss: 1.39, train acc: 44.04%\n",
      "batch: 43, batch train loss: 1.39, train acc: 44.05%\n",
      "batch: 44, batch train loss: 1.39, train acc: 44.02%\n",
      "batch: 45, batch train loss: 1.40, train acc: 43.95%\n",
      "batch: 46, batch train loss: 1.40, train acc: 43.89%\n",
      "batch: 47, batch train loss: 1.40, train acc: 43.80%\n",
      "batch: 48, batch train loss: 1.40, train acc: 43.73%\n",
      "batch: 49, batch train loss: 1.40, train acc: 43.64%\n",
      "batch: 50, batch train loss: 1.40, train acc: 43.58%\n",
      "##################################################\n",
      "batch: 50, batch valid loss: 1.47, valid acc: 40.09%\n",
      "batch: 51, batch train loss: 1.40, train acc: 43.49%\n",
      "batch: 52, batch train loss: 1.41, train acc: 43.43%\n",
      "batch: 53, batch train loss: 1.41, train acc: 43.36%\n",
      "batch: 54, batch train loss: 1.41, train acc: 43.32%\n",
      "batch: 55, batch train loss: 1.41, train acc: 43.25%\n",
      "batch: 56, batch train loss: 1.41, train acc: 43.18%\n",
      "batch: 57, batch train loss: 1.41, train acc: 43.14%\n",
      "batch: 58, batch train loss: 1.41, train acc: 43.11%\n",
      "batch: 59, batch train loss: 1.41, train acc: 43.07%\n",
      "batch: 60, batch train loss: 1.41, train acc: 43.02%\n",
      "batch: 61, batch train loss: 1.41, train acc: 42.99%\n",
      "batch: 62, batch train loss: 1.41, train acc: 42.98%\n",
      "batch: 63, batch train loss: 1.41, train acc: 42.97%\n",
      "batch: 64, batch train loss: 1.41, train acc: 42.99%\n",
      "batch: 65, batch train loss: 1.42, train acc: 42.93%\n",
      "batch: 66, batch train loss: 1.42, train acc: 42.88%\n",
      "batch: 67, batch train loss: 1.42, train acc: 42.86%\n",
      "batch: 68, batch train loss: 1.42, train acc: 42.84%\n",
      "batch: 69, batch train loss: 1.42, train acc: 42.77%\n",
      "batch: 70, batch train loss: 1.42, train acc: 42.74%\n",
      "batch: 71, batch train loss: 1.42, train acc: 42.71%\n",
      "batch: 72, batch train loss: 1.42, train acc: 42.72%\n",
      "batch: 73, batch train loss: 1.42, train acc: 42.68%\n",
      "batch: 74, batch train loss: 1.42, train acc: 42.66%\n",
      "batch: 75, batch train loss: 1.42, train acc: 42.65%\n",
      "batch: 76, batch train loss: 1.42, train acc: 42.62%\n",
      "batch: 77, batch train loss: 1.42, train acc: 42.64%\n",
      "batch: 78, batch train loss: 1.42, train acc: 42.61%\n",
      "batch: 79, batch train loss: 1.42, train acc: 42.57%\n",
      "batch: 80, batch train loss: 1.42, train acc: 42.52%\n",
      "batch: 81, batch train loss: 1.42, train acc: 42.48%\n",
      "batch: 82, batch train loss: 1.42, train acc: 42.43%\n",
      "batch: 83, batch train loss: 1.42, train acc: 42.40%\n",
      "batch: 84, batch train loss: 1.42, train acc: 42.35%\n",
      "batch: 85, batch train loss: 1.42, train acc: 42.34%\n",
      "batch: 86, batch train loss: 1.42, train acc: 42.31%\n",
      "batch: 87, batch train loss: 1.42, train acc: 42.29%\n",
      "batch: 88, batch train loss: 1.42, train acc: 42.25%\n",
      "batch: 89, batch train loss: 1.42, train acc: 42.23%\n",
      "batch: 90, batch train loss: 1.42, train acc: 42.21%\n",
      "batch: 91, batch train loss: 1.42, train acc: 42.20%\n",
      "batch: 92, batch train loss: 1.42, train acc: 42.22%\n",
      "batch: 93, batch train loss: 1.43, train acc: 42.20%\n",
      "batch: 94, batch train loss: 1.43, train acc: 42.18%\n",
      "batch: 95, batch train loss: 1.43, train acc: 42.17%\n",
      "batch: 96, batch train loss: 1.43, train acc: 42.16%\n",
      "batch: 97, batch train loss: 1.43, train acc: 42.14%\n",
      "batch: 98, batch train loss: 1.43, train acc: 42.16%\n",
      "batch: 99, batch train loss: 1.43, train acc: 42.14%\n",
      "batch: 100, batch train loss: 1.43, train acc: 42.15%\n",
      "##################################################\n",
      "batch: 100, batch valid loss: 1.46, valid acc: 40.17%\n",
      "batch: 101, batch train loss: 1.43, train acc: 42.12%\n",
      "batch: 102, batch train loss: 1.43, train acc: 42.13%\n",
      "batch: 103, batch train loss: 1.43, train acc: 42.10%\n",
      "batch: 104, batch train loss: 1.43, train acc: 42.08%\n",
      "batch: 105, batch train loss: 1.43, train acc: 42.07%\n",
      "batch: 106, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 107, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 108, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 109, batch train loss: 1.43, train acc: 42.03%\n",
      "batch: 110, batch train loss: 1.43, train acc: 42.03%\n",
      "batch: 111, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 112, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 113, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 114, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 115, batch train loss: 1.43, train acc: 42.03%\n",
      "batch: 116, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 117, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 118, batch train loss: 1.43, train acc: 42.05%\n",
      "batch: 119, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 120, batch train loss: 1.43, train acc: 42.05%\n",
      "batch: 121, batch train loss: 1.43, train acc: 42.06%\n",
      "batch: 122, batch train loss: 1.43, train acc: 42.06%\n",
      "batch: 123, batch train loss: 1.43, train acc: 42.05%\n",
      "batch: 124, batch train loss: 1.43, train acc: 42.04%\n",
      "batch: 125, batch train loss: 1.43, train acc: 42.03%\n",
      "batch: 126, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 127, batch train loss: 1.43, train acc: 42.03%\n",
      "batch: 128, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 129, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 130, batch train loss: 1.43, train acc: 41.99%\n",
      "batch: 131, batch train loss: 1.43, train acc: 42.00%\n",
      "batch: 132, batch train loss: 1.43, train acc: 41.99%\n",
      "batch: 133, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 134, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 135, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 136, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 137, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 138, batch train loss: 1.43, train acc: 41.95%\n",
      "batch: 139, batch train loss: 1.43, train acc: 41.96%\n",
      "batch: 140, batch train loss: 1.43, train acc: 41.95%\n",
      "batch: 141, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 142, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 143, batch train loss: 1.43, train acc: 41.93%\n",
      "batch: 144, batch train loss: 1.43, train acc: 41.93%\n",
      "batch: 145, batch train loss: 1.43, train acc: 41.95%\n",
      "batch: 146, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 147, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 148, batch train loss: 1.43, train acc: 41.95%\n",
      "batch: 149, batch train loss: 1.43, train acc: 41.93%\n",
      "batch: 150, batch train loss: 1.43, train acc: 41.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "batch: 150, batch valid loss: 1.46, valid acc: 40.39%\n",
      "batch: 151, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 152, batch train loss: 1.43, train acc: 41.93%\n",
      "batch: 153, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 154, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 155, batch train loss: 1.43, train acc: 41.94%\n",
      "batch: 156, batch train loss: 1.43, train acc: 41.93%\n",
      "batch: 157, batch train loss: 1.43, train acc: 41.95%\n",
      "batch: 158, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 159, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 160, batch train loss: 1.43, train acc: 41.96%\n",
      "batch: 161, batch train loss: 1.43, train acc: 41.97%\n",
      "batch: 162, batch train loss: 1.43, train acc: 41.98%\n",
      "batch: 163, batch train loss: 1.43, train acc: 41.98%\n",
      "batch: 164, batch train loss: 1.43, train acc: 41.99%\n",
      "batch: 165, batch train loss: 1.43, train acc: 42.00%\n",
      "batch: 166, batch train loss: 1.43, train acc: 42.00%\n",
      "batch: 167, batch train loss: 1.43, train acc: 42.01%\n",
      "batch: 168, batch train loss: 1.43, train acc: 42.01%\n",
      "batch: 169, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 170, batch train loss: 1.43, train acc: 42.02%\n",
      "batch: 171, batch train loss: 1.42, train acc: 42.03%\n",
      "batch: 172, batch train loss: 1.42, train acc: 42.04%\n",
      "batch: 173, batch train loss: 1.42, train acc: 42.04%\n",
      "batch: 174, batch train loss: 1.42, train acc: 42.04%\n",
      "batch: 175, batch train loss: 1.42, train acc: 42.05%\n",
      "batch: 176, batch train loss: 1.42, train acc: 42.06%\n",
      "batch: 177, batch train loss: 1.42, train acc: 42.06%\n",
      "batch: 178, batch train loss: 1.42, train acc: 42.05%\n",
      "batch: 179, batch train loss: 1.42, train acc: 42.07%\n",
      "batch: 180, batch train loss: 1.42, train acc: 42.09%\n",
      "batch: 181, batch train loss: 1.42, train acc: 42.10%\n",
      "batch: 182, batch train loss: 1.42, train acc: 42.11%\n",
      "batch: 183, batch train loss: 1.42, train acc: 42.13%\n",
      "batch: 184, batch train loss: 1.42, train acc: 42.14%\n",
      "batch: 185, batch train loss: 1.42, train acc: 42.15%\n",
      "batch: 186, batch train loss: 1.42, train acc: 42.14%\n",
      "batch: 187, batch train loss: 1.42, train acc: 42.16%\n",
      "batch: 188, batch train loss: 1.42, train acc: 42.16%\n",
      "batch: 189, batch train loss: 1.42, train acc: 42.16%\n",
      "batch: 190, batch train loss: 1.42, train acc: 42.16%\n",
      "batch: 191, batch train loss: 1.42, train acc: 42.16%\n",
      "batch: 192, batch train loss: 1.42, train acc: 42.17%\n",
      "batch: 193, batch train loss: 1.42, train acc: 42.19%\n",
      "batch: 194, batch train loss: 1.42, train acc: 42.20%\n",
      "batch: 195, batch train loss: 1.42, train acc: 42.20%\n",
      "batch: 196, batch train loss: 1.42, train acc: 42.21%\n",
      "batch: 197, batch train loss: 1.42, train acc: 42.21%\n",
      "batch: 198, batch train loss: 1.42, train acc: 42.23%\n",
      "batch: 199, batch train loss: 1.42, train acc: 42.23%\n",
      "batch: 200, batch train loss: 1.42, train acc: 42.24%\n",
      "##################################################\n",
      "batch: 200, batch valid loss: 1.45, valid acc: 40.52%\n",
      "batch: 201, batch train loss: 1.42, train acc: 42.26%\n",
      "batch: 202, batch train loss: 1.42, train acc: 42.26%\n",
      "batch: 203, batch train loss: 1.42, train acc: 42.27%\n",
      "batch: 204, batch train loss: 1.42, train acc: 42.28%\n",
      "batch: 205, batch train loss: 1.42, train acc: 42.30%\n",
      "batch: 206, batch train loss: 1.42, train acc: 42.31%\n",
      "batch: 207, batch train loss: 1.42, train acc: 42.31%\n",
      "batch: 208, batch train loss: 1.42, train acc: 42.31%\n",
      "batch: 209, batch train loss: 1.42, train acc: 42.31%\n",
      "batch: 210, batch train loss: 1.42, train acc: 42.32%\n",
      "batch: 211, batch train loss: 1.42, train acc: 42.33%\n",
      "batch: 212, batch train loss: 1.42, train acc: 42.34%\n",
      "batch: 213, batch train loss: 1.42, train acc: 42.34%\n",
      "batch: 214, batch train loss: 1.41, train acc: 42.35%\n",
      "batch: 215, batch train loss: 1.41, train acc: 42.36%\n",
      "batch: 216, batch train loss: 1.41, train acc: 42.37%\n",
      "batch: 217, batch train loss: 1.41, train acc: 42.38%\n",
      "batch: 218, batch train loss: 1.41, train acc: 42.40%\n",
      "batch: 219, batch train loss: 1.41, train acc: 42.40%\n",
      "batch: 220, batch train loss: 1.41, train acc: 42.41%\n",
      "batch: 221, batch train loss: 1.41, train acc: 42.42%\n",
      "batch: 222, batch train loss: 1.41, train acc: 42.44%\n",
      "batch: 223, batch train loss: 1.41, train acc: 42.46%\n",
      "batch: 224, batch train loss: 1.41, train acc: 42.47%\n",
      "batch: 225, batch train loss: 1.41, train acc: 42.48%\n",
      "batch: 226, batch train loss: 1.41, train acc: 42.49%\n",
      "batch: 227, batch train loss: 1.41, train acc: 42.50%\n",
      "batch: 228, batch train loss: 1.41, train acc: 42.50%\n",
      "batch: 229, batch train loss: 1.41, train acc: 42.50%\n",
      "batch: 230, batch train loss: 1.41, train acc: 42.51%\n",
      "batch: 231, batch train loss: 1.41, train acc: 42.52%\n",
      "batch: 232, batch train loss: 1.41, train acc: 42.53%\n",
      "batch: 233, batch train loss: 1.41, train acc: 42.55%\n",
      "batch: 234, batch train loss: 1.41, train acc: 42.56%\n",
      "batch: 235, batch train loss: 1.41, train acc: 42.57%\n",
      "batch: 236, batch train loss: 1.41, train acc: 42.59%\n",
      "batch: 237, batch train loss: 1.41, train acc: 42.60%\n",
      "batch: 238, batch train loss: 1.41, train acc: 42.61%\n",
      "batch: 239, batch train loss: 1.41, train acc: 42.62%\n",
      "batch: 240, batch train loss: 1.41, train acc: 42.64%\n",
      "batch: 241, batch train loss: 1.41, train acc: 42.65%\n",
      "batch: 242, batch train loss: 1.41, train acc: 42.65%\n",
      "batch: 243, batch train loss: 1.41, train acc: 42.66%\n",
      "batch: 244, batch train loss: 1.41, train acc: 42.66%\n",
      "batch: 245, batch train loss: 1.41, train acc: 42.66%\n",
      "batch: 246, batch train loss: 1.41, train acc: 42.67%\n",
      "batch: 247, batch train loss: 1.41, train acc: 42.68%\n",
      "batch: 248, batch train loss: 1.40, train acc: 42.69%\n",
      "batch: 249, batch train loss: 1.40, train acc: 42.70%\n",
      "batch: 250, batch train loss: 1.40, train acc: 42.71%\n",
      "##################################################\n",
      "batch: 250, batch valid loss: 1.45, valid acc: 40.63%\n",
      "batch: 251, batch train loss: 1.40, train acc: 42.72%\n",
      "batch: 252, batch train loss: 1.40, train acc: 42.71%\n",
      "batch: 253, batch train loss: 1.40, train acc: 42.72%\n",
      "batch: 254, batch train loss: 1.40, train acc: 42.72%\n",
      "batch: 255, batch train loss: 1.40, train acc: 42.73%\n",
      "batch: 256, batch train loss: 1.40, train acc: 42.74%\n",
      "batch: 257, batch train loss: 1.40, train acc: 42.75%\n",
      "batch: 258, batch train loss: 1.40, train acc: 42.76%\n",
      "batch: 259, batch train loss: 1.40, train acc: 42.78%\n",
      "batch: 260, batch train loss: 1.40, train acc: 42.80%\n",
      "batch: 261, batch train loss: 1.40, train acc: 42.81%\n",
      "batch: 262, batch train loss: 1.40, train acc: 42.82%\n",
      "batch: 263, batch train loss: 1.40, train acc: 42.83%\n",
      "batch: 264, batch train loss: 1.40, train acc: 42.83%\n",
      "batch: 265, batch train loss: 1.40, train acc: 42.85%\n",
      "batch: 266, batch train loss: 1.40, train acc: 42.87%\n",
      "batch: 267, batch train loss: 1.40, train acc: 42.87%\n",
      "batch: 268, batch train loss: 1.40, train acc: 42.88%\n",
      "batch: 269, batch train loss: 1.40, train acc: 42.90%\n",
      "batch: 270, batch train loss: 1.40, train acc: 42.91%\n",
      "batch: 271, batch train loss: 1.40, train acc: 42.92%\n",
      "batch: 272, batch train loss: 1.40, train acc: 42.93%\n",
      "batch: 273, batch train loss: 1.40, train acc: 42.94%\n",
      "batch: 274, batch train loss: 1.40, train acc: 42.94%\n",
      "batch: 275, batch train loss: 1.40, train acc: 42.96%\n",
      "batch: 276, batch train loss: 1.40, train acc: 42.97%\n",
      "batch: 277, batch train loss: 1.40, train acc: 42.98%\n",
      "batch: 278, batch train loss: 1.40, train acc: 42.99%\n",
      "batch: 279, batch train loss: 1.40, train acc: 43.01%\n",
      "batch: 280, batch train loss: 1.40, train acc: 43.02%\n",
      "batch: 281, batch train loss: 1.40, train acc: 43.04%\n",
      "batch: 282, batch train loss: 1.40, train acc: 43.05%\n",
      "batch: 283, batch train loss: 1.40, train acc: 43.07%\n",
      "batch: 284, batch train loss: 1.39, train acc: 43.08%\n",
      "batch: 285, batch train loss: 1.39, train acc: 43.08%\n",
      "batch: 286, batch train loss: 1.39, train acc: 43.10%\n",
      "batch: 287, batch train loss: 1.39, train acc: 43.12%\n",
      "batch: 288, batch train loss: 1.39, train acc: 43.12%\n",
      "batch: 289, batch train loss: 1.39, train acc: 43.13%\n",
      "batch: 290, batch train loss: 1.39, train acc: 43.14%\n",
      "batch: 291, batch train loss: 1.39, train acc: 43.15%\n",
      "batch: 292, batch train loss: 1.39, train acc: 43.16%\n",
      "batch: 293, batch train loss: 1.39, train acc: 43.17%\n",
      "batch: 294, batch train loss: 1.39, train acc: 43.19%\n",
      "batch: 295, batch train loss: 1.39, train acc: 43.20%\n",
      "batch: 296, batch train loss: 1.39, train acc: 43.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 297, batch train loss: 1.39, train acc: 43.22%\n",
      "batch: 298, batch train loss: 1.39, train acc: 43.23%\n",
      "batch: 299, batch train loss: 1.39, train acc: 43.25%\n",
      "batch: 300, batch train loss: 1.39, train acc: 43.26%\n",
      "##################################################\n",
      "batch: 300, batch valid loss: 1.45, valid acc: 40.68%\n",
      "batch: 301, batch train loss: 1.39, train acc: 43.27%\n",
      "batch: 302, batch train loss: 1.39, train acc: 43.27%\n",
      "batch: 303, batch train loss: 1.39, train acc: 43.27%\n",
      "batch: 304, batch train loss: 1.39, train acc: 43.28%\n",
      "batch: 305, batch train loss: 1.39, train acc: 43.29%\n",
      "batch: 306, batch train loss: 1.39, train acc: 43.30%\n",
      "batch: 307, batch train loss: 1.39, train acc: 43.30%\n",
      "batch: 308, batch train loss: 1.39, train acc: 43.32%\n",
      "batch: 309, batch train loss: 1.39, train acc: 43.32%\n",
      "batch: 310, batch train loss: 1.39, train acc: 43.33%\n",
      "batch: 311, batch train loss: 1.39, train acc: 43.34%\n",
      "batch: 312, batch train loss: 1.39, train acc: 43.35%\n",
      "batch: 313, batch train loss: 1.39, train acc: 43.36%\n",
      "batch: 314, batch train loss: 1.39, train acc: 43.37%\n",
      "batch: 315, batch train loss: 1.39, train acc: 43.39%\n",
      "batch: 316, batch train loss: 1.39, train acc: 43.40%\n",
      "batch: 317, batch train loss: 1.39, train acc: 43.41%\n",
      "batch: 318, batch train loss: 1.39, train acc: 43.43%\n",
      "batch: 319, batch train loss: 1.39, train acc: 43.44%\n",
      "batch: 320, batch train loss: 1.39, train acc: 43.45%\n",
      "batch: 321, batch train loss: 1.39, train acc: 43.47%\n",
      "batch: 322, batch train loss: 1.39, train acc: 43.48%\n",
      "batch: 323, batch train loss: 1.38, train acc: 43.49%\n",
      "batch: 324, batch train loss: 1.38, train acc: 43.50%\n",
      "batch: 325, batch train loss: 1.38, train acc: 43.51%\n",
      "batch: 326, batch train loss: 1.38, train acc: 43.52%\n",
      "batch: 327, batch train loss: 1.38, train acc: 43.54%\n",
      "batch: 328, batch train loss: 1.38, train acc: 43.55%\n",
      "batch: 329, batch train loss: 1.38, train acc: 43.55%\n",
      "batch: 330, batch train loss: 1.38, train acc: 43.56%\n",
      "batch: 331, batch train loss: 1.38, train acc: 43.57%\n",
      "batch: 332, batch train loss: 1.38, train acc: 43.58%\n",
      "batch: 333, batch train loss: 1.38, train acc: 43.58%\n",
      "batch: 334, batch train loss: 1.38, train acc: 43.59%\n",
      "batch: 335, batch train loss: 1.38, train acc: 43.60%\n",
      "batch: 336, batch train loss: 1.38, train acc: 43.61%\n",
      "batch: 337, batch train loss: 1.38, train acc: 43.63%\n",
      "batch: 338, batch train loss: 1.38, train acc: 43.64%\n",
      "batch: 339, batch train loss: 1.38, train acc: 43.66%\n",
      "batch: 340, batch train loss: 1.38, train acc: 43.67%\n",
      "batch: 341, batch train loss: 1.38, train acc: 43.68%\n",
      "batch: 342, batch train loss: 1.38, train acc: 43.70%\n",
      "batch: 343, batch train loss: 1.38, train acc: 43.71%\n",
      "batch: 344, batch train loss: 1.38, train acc: 43.72%\n",
      "batch: 345, batch train loss: 1.38, train acc: 43.72%\n",
      "batch: 346, batch train loss: 1.38, train acc: 43.73%\n",
      "batch: 347, batch train loss: 1.38, train acc: 43.74%\n",
      "batch: 348, batch train loss: 1.38, train acc: 43.75%\n",
      "batch: 349, batch train loss: 1.38, train acc: 43.76%\n",
      "batch: 350, batch train loss: 1.38, train acc: 43.77%\n",
      "##################################################\n",
      "batch: 350, batch valid loss: 1.45, valid acc: 40.72%\n",
      "batch: 351, batch train loss: 1.38, train acc: 43.79%\n",
      "batch: 352, batch train loss: 1.38, train acc: 43.80%\n",
      "batch: 353, batch train loss: 1.38, train acc: 43.82%\n",
      "batch: 354, batch train loss: 1.38, train acc: 43.83%\n",
      "batch: 355, batch train loss: 1.38, train acc: 43.84%\n",
      "batch: 356, batch train loss: 1.38, train acc: 43.84%\n",
      "batch: 357, batch train loss: 1.38, train acc: 43.85%\n",
      "batch: 358, batch train loss: 1.38, train acc: 43.85%\n",
      "batch: 359, batch train loss: 1.38, train acc: 43.87%\n",
      "batch: 360, batch train loss: 1.38, train acc: 43.87%\n",
      "batch: 361, batch train loss: 1.37, train acc: 43.88%\n",
      "batch: 362, batch train loss: 1.37, train acc: 43.89%\n",
      "batch: 363, batch train loss: 1.37, train acc: 43.90%\n",
      "batch: 364, batch train loss: 1.37, train acc: 43.91%\n",
      "batch: 365, batch train loss: 1.37, train acc: 43.91%\n",
      "batch: 366, batch train loss: 1.37, train acc: 43.92%\n",
      "batch: 367, batch train loss: 1.37, train acc: 43.94%\n",
      "batch: 368, batch train loss: 1.37, train acc: 43.95%\n",
      "batch: 369, batch train loss: 1.37, train acc: 43.96%\n",
      "batch: 370, batch train loss: 1.37, train acc: 43.97%\n",
      "batch: 371, batch train loss: 1.37, train acc: 43.98%\n",
      "batch: 372, batch train loss: 1.37, train acc: 43.99%\n",
      "batch: 373, batch train loss: 1.37, train acc: 44.00%\n",
      "batch: 374, batch train loss: 1.37, train acc: 44.02%\n",
      "batch: 375, batch train loss: 1.37, train acc: 44.03%\n",
      "batch: 376, batch train loss: 1.37, train acc: 44.04%\n",
      "batch: 377, batch train loss: 1.37, train acc: 44.05%\n",
      "batch: 378, batch train loss: 1.37, train acc: 44.06%\n",
      "batch: 379, batch train loss: 1.37, train acc: 44.07%\n",
      "batch: 380, batch train loss: 1.37, train acc: 44.09%\n",
      "batch: 381, batch train loss: 1.37, train acc: 44.10%\n",
      "batch: 382, batch train loss: 1.37, train acc: 44.11%\n",
      "batch: 383, batch train loss: 1.37, train acc: 44.12%\n",
      "batch: 384, batch train loss: 1.37, train acc: 44.12%\n",
      "batch: 385, batch train loss: 1.37, train acc: 44.13%\n",
      "batch: 386, batch train loss: 1.37, train acc: 44.14%\n",
      "batch: 387, batch train loss: 1.37, train acc: 44.15%\n",
      "batch: 388, batch train loss: 1.37, train acc: 44.16%\n",
      "batch: 389, batch train loss: 1.37, train acc: 44.17%\n",
      "batch: 390, batch train loss: 1.37, train acc: 44.17%\n",
      "batch: 391, batch train loss: 1.37, train acc: 44.18%\n",
      "batch: 392, batch train loss: 1.37, train acc: 44.19%\n",
      "batch: 393, batch train loss: 1.37, train acc: 44.20%\n",
      "batch: 394, batch train loss: 1.37, train acc: 44.22%\n",
      "batch: 395, batch train loss: 1.37, train acc: 44.22%\n",
      "batch: 396, batch train loss: 1.37, train acc: 44.23%\n",
      "batch: 397, batch train loss: 1.37, train acc: 44.24%\n",
      "batch: 398, batch train loss: 1.37, train acc: 44.25%\n",
      "batch: 399, batch train loss: 1.37, train acc: 44.26%\n",
      "batch: 400, batch train loss: 1.36, train acc: 44.27%\n",
      "##################################################\n",
      "batch: 400, batch valid loss: 1.45, valid acc: 40.69%\n",
      "batch: 401, batch train loss: 1.36, train acc: 44.28%\n",
      "batch: 402, batch train loss: 1.36, train acc: 44.28%\n",
      "batch: 403, batch train loss: 1.36, train acc: 44.29%\n",
      "batch: 404, batch train loss: 1.36, train acc: 44.30%\n",
      "batch: 405, batch train loss: 1.36, train acc: 44.31%\n",
      "batch: 406, batch train loss: 1.36, train acc: 44.33%\n",
      "batch: 407, batch train loss: 1.36, train acc: 44.34%\n",
      "batch: 408, batch train loss: 1.36, train acc: 44.35%\n",
      "batch: 409, batch train loss: 1.36, train acc: 44.36%\n",
      "batch: 410, batch train loss: 1.36, train acc: 44.37%\n",
      "batch: 411, batch train loss: 1.36, train acc: 44.38%\n",
      "batch: 412, batch train loss: 1.36, train acc: 44.39%\n",
      "batch: 413, batch train loss: 1.36, train acc: 44.40%\n",
      "batch: 414, batch train loss: 1.36, train acc: 44.41%\n",
      "batch: 415, batch train loss: 1.36, train acc: 44.43%\n",
      "batch: 416, batch train loss: 1.36, train acc: 44.44%\n",
      "batch: 417, batch train loss: 1.36, train acc: 44.44%\n",
      "batch: 418, batch train loss: 1.36, train acc: 44.46%\n",
      "batch: 419, batch train loss: 1.36, train acc: 44.47%\n",
      "batch: 420, batch train loss: 1.36, train acc: 44.48%\n",
      "batch: 421, batch train loss: 1.36, train acc: 44.49%\n",
      "batch: 422, batch train loss: 1.36, train acc: 44.49%\n",
      "batch: 423, batch train loss: 1.36, train acc: 44.50%\n",
      "batch: 424, batch train loss: 1.36, train acc: 44.52%\n",
      "batch: 425, batch train loss: 1.36, train acc: 44.53%\n",
      "batch: 426, batch train loss: 1.36, train acc: 44.53%\n",
      "batch: 427, batch train loss: 1.36, train acc: 44.54%\n",
      "batch: 428, batch train loss: 1.36, train acc: 44.55%\n",
      "batch: 429, batch train loss: 1.36, train acc: 44.56%\n",
      "batch: 430, batch train loss: 1.36, train acc: 44.57%\n",
      "batch: 431, batch train loss: 1.36, train acc: 44.58%\n",
      "batch: 432, batch train loss: 1.36, train acc: 44.59%\n",
      "batch: 433, batch train loss: 1.36, train acc: 44.59%\n",
      "batch: 434, batch train loss: 1.36, train acc: 44.61%\n",
      "batch: 435, batch train loss: 1.36, train acc: 44.61%\n",
      "batch: 436, batch train loss: 1.36, train acc: 44.62%\n",
      "batch: 437, batch train loss: 1.36, train acc: 44.64%\n",
      "batch: 438, batch train loss: 1.36, train acc: 44.64%\n",
      "batch: 439, batch train loss: 1.36, train acc: 44.66%\n",
      "batch: 440, batch train loss: 1.36, train acc: 44.67%\n",
      "batch: 441, batch train loss: 1.36, train acc: 44.68%\n",
      "batch: 442, batch train loss: 1.36, train acc: 44.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 443, batch train loss: 1.36, train acc: 44.70%\n",
      "batch: 444, batch train loss: 1.35, train acc: 44.71%\n",
      "batch: 445, batch train loss: 1.35, train acc: 44.73%\n",
      "batch: 446, batch train loss: 1.35, train acc: 44.74%\n",
      "batch: 447, batch train loss: 1.35, train acc: 44.75%\n",
      "batch: 448, batch train loss: 1.35, train acc: 44.77%\n",
      "batch: 449, batch train loss: 1.35, train acc: 44.78%\n",
      "batch: 450, batch train loss: 1.35, train acc: 44.78%\n",
      "##################################################\n",
      "batch: 450, batch valid loss: 1.45, valid acc: 40.73%\n",
      "batch: 451, batch train loss: 1.35, train acc: 44.79%\n",
      "batch: 452, batch train loss: 1.35, train acc: 44.80%\n",
      "batch: 453, batch train loss: 1.35, train acc: 44.81%\n",
      "batch: 454, batch train loss: 1.35, train acc: 44.82%\n",
      "batch: 455, batch train loss: 1.35, train acc: 44.83%\n",
      "batch: 456, batch train loss: 1.35, train acc: 44.84%\n",
      "batch: 457, batch train loss: 1.35, train acc: 44.85%\n",
      "batch: 458, batch train loss: 1.35, train acc: 44.86%\n",
      "batch: 459, batch train loss: 1.35, train acc: 44.87%\n",
      "batch: 460, batch train loss: 1.35, train acc: 44.88%\n",
      "batch: 461, batch train loss: 1.35, train acc: 44.89%\n",
      "batch: 462, batch train loss: 1.35, train acc: 44.90%\n",
      "batch: 463, batch train loss: 1.35, train acc: 44.91%\n",
      "batch: 464, batch train loss: 1.35, train acc: 44.93%\n",
      "batch: 465, batch train loss: 1.35, train acc: 44.94%\n",
      "batch: 466, batch train loss: 1.35, train acc: 44.95%\n",
      "batch: 467, batch train loss: 1.35, train acc: 44.95%\n",
      "batch: 468, batch train loss: 1.35, train acc: 44.96%\n",
      "batch: 469, batch train loss: 1.35, train acc: 44.97%\n",
      "batch: 470, batch train loss: 1.35, train acc: 44.99%\n",
      "batch: 471, batch train loss: 1.35, train acc: 45.00%\n",
      "batch: 472, batch train loss: 1.35, train acc: 45.00%\n",
      "batch: 473, batch train loss: 1.35, train acc: 45.01%\n",
      "batch: 474, batch train loss: 1.35, train acc: 45.02%\n",
      "batch: 475, batch train loss: 1.35, train acc: 45.04%\n",
      "batch: 476, batch train loss: 1.35, train acc: 45.05%\n",
      "batch: 477, batch train loss: 1.35, train acc: 45.06%\n",
      "batch: 478, batch train loss: 1.35, train acc: 45.07%\n",
      "batch: 479, batch train loss: 1.35, train acc: 45.09%\n",
      "batch: 480, batch train loss: 1.35, train acc: 45.10%\n",
      "batch: 481, batch train loss: 1.35, train acc: 45.12%\n",
      "batch: 482, batch train loss: 1.35, train acc: 45.13%\n",
      "batch: 483, batch train loss: 1.35, train acc: 45.14%\n",
      "batch: 484, batch train loss: 1.35, train acc: 45.15%\n",
      "batch: 485, batch train loss: 1.35, train acc: 45.17%\n",
      "batch: 486, batch train loss: 1.34, train acc: 45.18%\n",
      "batch: 487, batch train loss: 1.34, train acc: 45.19%\n",
      "batch: 488, batch train loss: 1.34, train acc: 45.20%\n",
      "batch: 489, batch train loss: 1.34, train acc: 45.21%\n",
      "batch: 490, batch train loss: 1.34, train acc: 45.23%\n",
      "batch: 491, batch train loss: 1.34, train acc: 45.23%\n",
      "batch: 492, batch train loss: 1.34, train acc: 45.24%\n",
      "batch: 493, batch train loss: 1.34, train acc: 45.25%\n",
      "batch: 494, batch train loss: 1.34, train acc: 45.27%\n",
      "batch: 495, batch train loss: 1.34, train acc: 45.28%\n",
      "batch: 496, batch train loss: 1.34, train acc: 45.28%\n",
      "batch: 497, batch train loss: 1.34, train acc: 45.29%\n",
      "batch: 498, batch train loss: 1.34, train acc: 45.31%\n",
      "batch: 499, batch train loss: 1.34, train acc: 45.32%\n",
      "batch: 500, batch train loss: 1.34, train acc: 45.33%\n",
      "##################################################\n",
      "batch: 500, batch valid loss: 1.46, valid acc: 40.77%\n",
      "batch: 501, batch train loss: 1.34, train acc: 45.34%\n",
      "batch: 502, batch train loss: 1.34, train acc: 45.35%\n",
      "batch: 503, batch train loss: 1.34, train acc: 45.36%\n",
      "batch: 504, batch train loss: 1.34, train acc: 45.36%\n",
      "batch: 505, batch train loss: 1.34, train acc: 45.37%\n",
      "batch: 506, batch train loss: 1.34, train acc: 45.38%\n",
      "batch: 507, batch train loss: 1.34, train acc: 45.39%\n",
      "batch: 508, batch train loss: 1.34, train acc: 45.40%\n",
      "batch: 509, batch train loss: 1.34, train acc: 45.41%\n",
      "batch: 510, batch train loss: 1.34, train acc: 45.42%\n",
      "batch: 511, batch train loss: 1.34, train acc: 45.43%\n",
      "batch: 512, batch train loss: 1.34, train acc: 45.45%\n",
      "batch: 513, batch train loss: 1.34, train acc: 45.46%\n",
      "batch: 514, batch train loss: 1.34, train acc: 45.47%\n",
      "batch: 515, batch train loss: 1.34, train acc: 45.47%\n",
      "batch: 516, batch train loss: 1.34, train acc: 45.48%\n",
      "batch: 517, batch train loss: 1.34, train acc: 45.49%\n",
      "batch: 518, batch train loss: 1.34, train acc: 45.50%\n",
      "batch: 519, batch train loss: 1.34, train acc: 45.52%\n",
      "batch: 520, batch train loss: 1.34, train acc: 45.52%\n",
      "batch: 521, batch train loss: 1.34, train acc: 45.53%\n",
      "batch: 522, batch train loss: 1.34, train acc: 45.54%\n",
      "batch: 523, batch train loss: 1.34, train acc: 45.55%\n",
      "batch: 524, batch train loss: 1.34, train acc: 45.56%\n",
      "batch: 525, batch train loss: 1.34, train acc: 45.57%\n",
      "batch: 526, batch train loss: 1.34, train acc: 45.58%\n",
      "batch: 527, batch train loss: 1.34, train acc: 45.59%\n",
      "batch: 528, batch train loss: 1.34, train acc: 45.60%\n",
      "batch: 529, batch train loss: 1.34, train acc: 45.61%\n",
      "batch: 530, batch train loss: 1.34, train acc: 45.62%\n",
      "batch: 531, batch train loss: 1.34, train acc: 45.63%\n",
      "batch: 532, batch train loss: 1.34, train acc: 45.64%\n",
      "batch: 533, batch train loss: 1.34, train acc: 45.65%\n",
      "batch: 534, batch train loss: 1.34, train acc: 45.66%\n",
      "batch: 535, batch train loss: 1.33, train acc: 45.66%\n",
      "batch: 536, batch train loss: 1.33, train acc: 45.67%\n",
      "batch: 537, batch train loss: 1.33, train acc: 45.69%\n",
      "batch: 538, batch train loss: 1.33, train acc: 45.69%\n",
      "batch: 539, batch train loss: 1.33, train acc: 45.70%\n",
      "batch: 540, batch train loss: 1.33, train acc: 45.71%\n",
      "batch: 541, batch train loss: 1.33, train acc: 45.72%\n",
      "batch: 542, batch train loss: 1.33, train acc: 45.73%\n",
      "batch: 543, batch train loss: 1.33, train acc: 45.74%\n",
      "batch: 544, batch train loss: 1.33, train acc: 45.75%\n",
      "batch: 545, batch train loss: 1.33, train acc: 45.76%\n",
      "batch: 546, batch train loss: 1.33, train acc: 45.77%\n",
      "batch: 547, batch train loss: 1.33, train acc: 45.78%\n",
      "batch: 548, batch train loss: 1.33, train acc: 45.78%\n",
      "batch: 549, batch train loss: 1.33, train acc: 45.79%\n",
      "batch: 550, batch train loss: 1.33, train acc: 45.80%\n",
      "##################################################\n",
      "batch: 550, batch valid loss: 1.46, valid acc: 40.77%\n",
      "batch: 551, batch train loss: 1.33, train acc: 45.81%\n",
      "batch: 552, batch train loss: 1.33, train acc: 45.82%\n",
      "batch: 553, batch train loss: 1.33, train acc: 45.83%\n",
      "batch: 554, batch train loss: 1.33, train acc: 45.84%\n",
      "batch: 555, batch train loss: 1.33, train acc: 45.85%\n",
      "batch: 556, batch train loss: 1.33, train acc: 45.85%\n",
      "batch: 557, batch train loss: 1.33, train acc: 45.86%\n",
      "batch: 558, batch train loss: 1.33, train acc: 45.87%\n",
      "batch: 559, batch train loss: 1.33, train acc: 45.88%\n",
      "batch: 560, batch train loss: 1.33, train acc: 45.88%\n",
      "batch: 561, batch train loss: 1.33, train acc: 45.89%\n",
      "batch: 562, batch train loss: 1.33, train acc: 45.90%\n",
      "batch: 563, batch train loss: 1.33, train acc: 45.91%\n",
      "batch: 564, batch train loss: 1.33, train acc: 45.92%\n",
      "batch: 565, batch train loss: 1.33, train acc: 45.93%\n",
      "batch: 566, batch train loss: 1.33, train acc: 45.94%\n",
      "batch: 567, batch train loss: 1.33, train acc: 45.94%\n",
      "batch: 568, batch train loss: 1.33, train acc: 45.95%\n",
      "batch: 569, batch train loss: 1.33, train acc: 45.96%\n",
      "batch: 570, batch train loss: 1.33, train acc: 45.97%\n",
      "batch: 571, batch train loss: 1.33, train acc: 45.98%\n",
      "batch: 572, batch train loss: 1.33, train acc: 45.99%\n",
      "batch: 573, batch train loss: 1.33, train acc: 46.00%\n",
      "batch: 574, batch train loss: 1.33, train acc: 46.01%\n",
      "batch: 575, batch train loss: 1.33, train acc: 46.02%\n",
      "batch: 576, batch train loss: 1.33, train acc: 46.03%\n",
      "batch: 577, batch train loss: 1.33, train acc: 46.04%\n",
      "batch: 578, batch train loss: 1.33, train acc: 46.05%\n",
      "batch: 579, batch train loss: 1.33, train acc: 46.05%\n",
      "batch: 580, batch train loss: 1.33, train acc: 46.06%\n",
      "batch: 581, batch train loss: 1.33, train acc: 46.07%\n",
      "batch: 582, batch train loss: 1.33, train acc: 46.08%\n",
      "batch: 583, batch train loss: 1.33, train acc: 46.09%\n",
      "batch: 584, batch train loss: 1.33, train acc: 46.09%\n",
      "batch: 585, batch train loss: 1.33, train acc: 46.11%\n",
      "batch: 586, batch train loss: 1.33, train acc: 46.11%\n",
      "batch: 587, batch train loss: 1.33, train acc: 46.12%\n",
      "batch: 588, batch train loss: 1.33, train acc: 46.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 589, batch train loss: 1.32, train acc: 46.14%\n",
      "batch: 590, batch train loss: 1.32, train acc: 46.14%\n",
      "batch: 591, batch train loss: 1.32, train acc: 46.15%\n",
      "batch: 592, batch train loss: 1.32, train acc: 46.16%\n",
      "batch: 593, batch train loss: 1.32, train acc: 46.17%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6dc8a9edd25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mCNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mCNT\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch: {:d}, batch train loss: {:.2f}, train acc: {:.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/competition-py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "\n",
    "    CNT = 0\n",
    "    for features, labels in train_ds:\n",
    "        train_step(features, labels)\n",
    "        CNT += 1\n",
    "        print('batch: {:d}, batch train loss: {:.2f}, train acc: {:.2f}%'.format(CNT, train_loss.result(), train_accuracy.result()*100))  \n",
    "\n",
    "        if CNT % 50 == 0: \n",
    "            for val_features, val_labels in valid_ds:\n",
    "                valid_step(val_features, val_labels)\n",
    "            print('##################################################')\n",
    "            print('batch: {:d}, batch valid loss: {:.2f}, valid acc: {:.2f}%'.format(CNT, valid_loss.result(), valid_accuracy.result()*100)) \n",
    "            print('##################################################')\n",
    "            \n",
    "    template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}%, Valid Loss: {:.2f}, Valid Accuracy: {:.2f}%'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         valid_loss.result(),\n",
    "                         valid_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存权重\n",
    "model.save_weights(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition-py36",
   "language": "python",
   "name": "competition-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
